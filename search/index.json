[{"content":"Shardingsphere（5.0.0.beta）源码学习-元数据上下文 上下文对象 上下文对象作为读取并解析配置, 承载数据的核心；\nspring有ApplicationContext,netty有HandlerContext\n向其他中间件一样，在ShardingSphere也十分重要; 后续几乎所有功能比如数据分片、数据加密、SQL 改写，各类扩展都依赖上下文对象存储的数据\n标准元数据上下文 StandardMetaDataContexts 核心存储了ShardingSphereMetaData元数据集合，作为重点后面分析\n@Getter public final class StandardMetaDataContexts implements MetaDataContexts { // 元数据集合  private final Map\u0026lt;String, ShardingSphereMetaData\u0026gt; metaDataMap; private final ShardingSphereRuleMetaData globalRuleMetaData; // 执行引擎  private final ExecutorEngine executorEngine; //优化引擎上下文工厂  private final OptimizeContextFactory optimizeContextFactory; private final ConfigurationProperties props; // 状态上下文  private final StateContext stateContext; ... } 治理元数据上下文 StandardMetaDataContexts 其实也是用的标准的StandardMetaDataContexts，治理模块shardingsphere-governance使用该上下文，通过配置中心读取规则配置，\n注入到StandardMetaDataContexts中，GovernanceFacade是配置中心的门面模式，目前支持了zookeeper和etcd,其实还可以通过RegistryCenterRepository的spi实现其他的配置中心，比如nacos, apollo,consul等。\npublic final class GovernanceMetaDataContexts implements MetaDataContexts { //治理： 配置中心的门面模式  private final GovernanceFacade governanceFacade; //还是使用StandardMetaDataContexts 装饰器模式  private volatile StandardMetaDataContexts metaDataContexts; private final ShardingSphereLock lock; ... } 初始化上下文对象 ShardingSphere中通过\norg.apache.shardingsphere.infra.context.metadata.MetaDataContextsBuilder构造器模式，构造\norg.apache.shardingsphere.infra.context.metadata.MetaDataContexts\npublic ShardingSphereDataSource(final Map\u0026lt;String, DataSource\u0026gt; dataSourceMap, final Collection\u0026lt;RuleConfiguration\u0026gt; configurations, final Properties props) throws SQLException { // 构建器构造 元数据上下文  metaDataContexts = new MetaDataContextsBuilder( Collections.singletonMap(DefaultSchema.LOGIC_NAME, dataSourceMap), Collections.singletonMap(DefaultSchema.LOGIC_NAME, configurations), props).build(); // xa事务类型  String xaTransactionMangerType = metaDataContexts.getProps().getValue(ConfigurationPropertyKey.XA_TRANSACTION_MANAGER_TYPE); // 创建事务上下文TransactionContexts//后面学习中细讲，本文不涉及  transactionContexts = createTransactionContexts(metaDataContexts.getDefaultMetaData().getResource().getDatabaseType(), dataSourceMap, xaTransactionMangerType); } MetaDataContextsBuilder.build()构建MetaDataContexts org.apache.shardingsphere.infra.context.metadata.MetaDataContextsBuilder.build方法\n/** * Build meta data contexts. * * @exception SQLException SQL exception * @return meta data contexts */ public StandardMetaDataContexts build() throws SQLException { Map\u0026lt;String, ShardingSphereMetaData\u0026gt; mataDataMap = new HashMap\u0026lt;\u0026gt;(schemaRuleConfigs.size(), 1); for (String each : schemaRuleConfigs.keySet()) { // buildMetaData构造核心的元数据对象ShardingSphereMetaData，加入到集合中  mataDataMap.put(each, buildMetaData(each)); } // 返回标准元数据上下文  return new StandardMetaDataContexts(mataDataMap, buildGlobalSchemaMetaData(mataDataMap), executorEngine, props); } ShardingSphere元数据 ShardingSphere元数据存储以下数据：\n 1数据源： 元数据资源，\n2 规则配置\n3 数据库元数据信息，表，字段，索引\n // @RequiredArgsConstructor @Getter public final class ShardingSphereMetaData { private final String name; // 数据源 元数据资源  private final ShardingSphereResource resource; // 配置规则元数据： 原始配置RuleConfiguration集合 =》 配置解析后的ShardingSphereRule集合  private final ShardingSphereRuleMetaData ruleMetaData; // 数据库表元数据： 字段元数据及索引元数据  private final ShardingSphereSchema schema; /** * Judge whether is completed. * * @return is completed or not */ public boolean isComplete() { return !ruleMetaData.getRules().isEmpty() \u0026amp;\u0026amp; !resource.getDataSources().isEmpty(); } } 再看如何构建ShardingSphere元数据，buildMetaData方法 private ShardingSphereMetaData buildMetaData(final String schemaName) throws SQLException { Map\u0026lt;String, DataSource\u0026gt; dataSourceMap = dataSources.get(schemaName); Collection\u0026lt;RuleConfiguration\u0026gt; ruleConfigs = schemaRuleConfigs.get(schemaName); //确定数据库类型  DatabaseType databaseType = DatabaseTypeRecognizer.getDatabaseType(dataSourceMap.values()); //a) 通过用户配置的RuleConfiguration 解析出 ShardingSphereRule  Collection\u0026lt;ShardingSphereRule\u0026gt; rules = ShardingSphereRulesBuilder.buildSchemaRules(schemaName, ruleConfigs, databaseType, dataSourceMap); // 构造规则元数据对象ShardingSphereRuleMetaData  ShardingSphereRuleMetaData ruleMetaData = new ShardingSphereRuleMetaData(ruleConfigs, rules); //构造ShardingSphere元数据对象ShardingSphereMetaData  //b) 构建数据源的元数据对象 c) 构建数据库的元数据  return new ShardingSphereMetaData(schemaName, buildResource(databaseType, dataSourceMap), ruleMetaData, buildSchema(databaseType, dataSourceMap, rules)); } a) ShardingSphere规则元数据 @RequiredArgsConstructor @Getter public final class ShardingSphereRuleMetaData { // 用户配置的规则对象集合  private final Collection\u0026lt;RuleConfiguration\u0026gt; configurations; // 解析后的规则集合  private final Collection\u0026lt;ShardingSphereRule\u0026gt; rules; } RuleConfiguration直接对应了用户的yaml, properties等配置\nShardingSphereRule\nbuildSchemaRules()\npublic static Collection\u0026lt;ShardingSphereRule\u0026gt; buildSchemaRules(final String schemaName, final Collection\u0026lt;RuleConfiguration\u0026gt; schemaRuleConfigurations, final DatabaseType databaseType, final Map\u0026lt;String, DataSource\u0026gt; dataSourceMap) { Map\u0026lt;RuleConfiguration, SchemaRuleBuilder\u0026gt; builders = OrderedSPIRegistry.getRegisteredServices(schemaRuleConfigurations, SchemaRuleBuilder.class); appendDefaultKernelSchemaRuleConfigurationBuilder(builders); // SchemaRuleBuilder. build() 构建ShardingSphereRule  return builders.entrySet().stream().map(entry -\u0026gt; entry.getValue().build(schemaName, dataSourceMap, databaseType, entry.getKey())).collect(Collectors.toList()); } SchemaRuleBuilder作为一个规则解析扩展点\nb) 再看buildResource，即构建数据源的元数据对象 private ShardingSphereResource buildResource(final DatabaseType databaseType, final Map\u0026lt;String, DataSource\u0026gt; dataSourceMap) throws SQLException { //构建数据源的元数据对象  DataSourcesMetaData dataSourceMetas = new DataSourcesMetaData(databaseType, getDatabaseAccessConfigurationMap(dataSourceMap)); //缓存下数据源的各类属性 链接，用户名，驱动，各类版本等等的原始信息  CachedDatabaseMetaData cachedDatabaseMetaData = createCachedDatabaseMetaData(dataSourceMap).orElse(null); return new ShardingSphereResource(dataSourceMap, dataSourceMetas, cachedDatabaseMetaData, databaseType); } c) 再看如何构建数据库元数据, buildSchema()方法 加载数据库的元数据，但是各个方言版本的元数据不太一样，例如mysql的在information_schema中，针对对不同方言，可以通过DialectTableMetaDataLoader的spi扩展定制各类方言版本的数据库元数据加载。\n 数据库元数据  public final class ShardingSphereSchema { // 表元数据  private final Map\u0026lt;String, TableMetaData\u0026gt; tables; ... }  数据库表元数据  // 表元数据 public final class TableMetaData { // 组合 列元数据  private final Map\u0026lt;String, ColumnMetaData\u0026gt; columns; // 组合 索引元数据  private final Map\u0026lt;String, IndexMetaData\u0026gt; indexes; ... } 列元数据\n@RequiredArgsConstructor @Getter @EqualsAndHashCode @ToString public final class ColumnMetaData { // 列名  private final String name; // 数据类型  private final int dataType; // 是否主键  private final boolean primaryKey; // 是否自动生成  private final boolean generated; // 是否大小敏感  private final boolean caseSensitive; } public final class IndexMetaData { // 索引字段名  private final String name; } buildSchema()方法\nprivate ShardingSphereSchema buildSchema(final DatabaseType databaseType, final Map\u0026lt;String, DataSource\u0026gt; dataSourceMap, final Collection\u0026lt;ShardingSphereRule\u0026gt; rules) throws SQLException { return SchemaBuilder.build(new SchemaBuilderMaterials(databaseType, dataSourceMap, rules, props)); } public static ShardingSphereSchema build(final SchemaBuilderMaterials materials) throws SQLException { ShardingSphereSchema result = new ShardingSphereSchema(); // 有规则配置的表的处理 可以通过 RuleBasedTableMetaDataBuilder 的spi扩展解析  addRuleConfiguredTables(materials, result); // 通过找对对应方言数据库的加载器DialectTableMetaDataLoader加载  appendRemainTables(materials, result); return result; } 总结 整个加载过程中使用不少设计模式，构造器模式，工厂模式，装饰器，组合等等，spi扩展点也预留不少，仔细研读收获颇丰。\n目前的解读只是从大的主线来理解，当然中间还有很多细节需要后续补充。\ndebug过程中容易走丢，实时记录下路线是个不错的方式：\n","date":"2021-09-05T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/shardingsphere5.0.0.beta%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8A%E4%B8%8B%E6%96%87/","title":"shardingsphere（5.0.0.beta）元数据上下文"},{"content":"Shardingsphere（5.0.0.beta）源码学习-总览 shardingsphere作为极为优秀的开源分布式数据库解决方案，通过阅读源码可以学到很多软件设计与开发的知识。\n本次我继续按照之前读源码的方式从整体到细节，带着问题读源码的方式记录这次深入学习 shardingsphere的过程。\n源码版本  5.0.0.beta 官方文档 源码地址 https://github.com/apache/shardingsphere/tree/5.0.0-beta  项目结构 先大概理解各个模块的主要功能点\n   一级目录 说明     examples 各种使用例子   shardingsphere-agent 监控, 对接apm,链路追踪   shardingsphere-db-protocol 数据库协议   shardingsphere-distribution 相关打包发步用   shardingsphere-distsql-parser distsql新功能:ShardingSphere 特有的内置 SQL 语言，提供了标准 SQL 之外的增量功能操作能力。   shardingsphere-features 常用功能shardingsphere-db-discovery 基于MGR主从切换的功能shardingsphere-encrypt 加解密shardingsphere-readwrite-splitting 读写分离 **重点**shardingsphere-shadow 影子库shardingsphere-sharding 分库分表 **重点**   shardingsphere-governance 数据治理：结合注册中心，提供给前端页面使用   shardingsphere-infra 引擎内核：shardingsphere-infra-authority proxy的权限控制shardingsphere-infra-binder sql解析后的结果绑定封装SQLStatement封装为各类上下文contextshardingsphere-infra-common 重要的实体类及工具 的元数据metadata,SPI,yaml工具，rule接口等shardingsphere-infra-context 上下文相关shardingsphere-infra-datetime 时间服务shardingsphere-infra-executor 执行器引擎 **重点**shardingsphere-infra-merge 归并引擎**重点**shardingsphere-infra-optimize 优化引擎**重点**shardingsphere-infra-parser 解析引擎**重点**shardingsphere-infra-rewrite 改写引擎**重点**shardingsphere-infra-route 路由引擎**重点**   shardingsphere-jdbc jdbc核心功能：增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。装饰器模式，对原生的DataSource,Connection,Statement(PrepareStatement),ResultSet进行包装，   shardingsphere-proxy 透明化的数据库代理提供封装了数据库二进制协议的服务端版本，用于完成对异构语言的支持   shardingsphere-scaling 数据迁移相关:弹性伸缩   shardingsphere-sql-parser sql解析器：antlr4 词法语法解析出SqlStatement,提供各类数据库的方言实现。SQL 解析作为分库分表类产品的核心，其性能和兼容性是最重要的衡量指标   shardingsphere-test 测试引擎   shardingsphere-transaction 事务：整合现有的成熟事务方案，本地事务、两阶段事务（XA）和柔性事务（Seata AT 事务）提供统一的分布式事务接口    对于重点核心内容有个大致认识，后面再单独分模块分析。\n代码分析 shardingsphere-jdbc是用的最多的接入方式，以 shardingsphere-jdbc的insert一条数据为例，先过一遍流程。\nINSERT INTO t_order (user_id, address_id, status) VALUES (?, ?, ?)\n分析入口 使用org.apache.shardingsphere.example.sharding.raw.jdbc.YamlConfigurationExampleMain 的分库分表示例，插入数据。\npublic final class YamlConfigurationExampleMain { private static ShardingType shardingType = ShardingType.SHARDING_DATABASES_AND_TABLES; public static void main(final String[] args) throws SQLException, IOException { // 初始化得到的DataSource为ShardingSphereDataSource  DataSource dataSource = YamlDataSourceFactory.newInstance(shardingType); ExampleExecuteTemplate.run(getExampleService(dataSource)); } ... } YamlDataSourceFactory.newInstance(shardingType)调用ShardingSphereDataSourceFactory.createDataSource得到ShardingSphereDataSource，可以看到ShardingSphereDataSource是对JDBC规范DataSource的实现。\n同样，后续用的Connection、Statement、PrepareStatement都有对应的ShardingConnection、ShardingStatment、ShardingPreparedStatement的实现。\npublic final class ShardingSphereDataSourceFactory { ... public static DataSource createDataSource(final Map\u0026lt;String, DataSource\u0026gt; dataSourceMap, final Collection\u0026lt;RuleConfiguration\u0026gt; configurations, final Properties props) throws SQLException { return new ShardingSphereDataSource(dataSourceMap, configurations, props); } ... } jdbc shardingsphere-jdbc下shardingsphere-jdbc-core中定义了jdbc规范的ShardingSphere实现\n  AbstractXXXAdapter对jdbc规范接口做一次适配\n  都继承了父类AbstractUnsupportedOperationXxx : 各个数据库厂家对jdbc规范没有完整实现，ShardingSphere对这些没实现方法统一在\nAbstractUnsupportedOperationXxx 中抛出不支持的异常，指明用户不可以使用。\n  org.apache.shardingsphere.driver.jdbc.core.datasource.ShardingSphereDataSource\npublic final class ShardingSphereDataSource extends AbstractUnsupportedOperationDataSource implements AutoCloseable { private final MetaDataContexts metaDataContexts; private final TransactionContexts transactionContexts; ... @Override public ShardingSphereConnection getConnection() { return new ShardingSphereConnection(getDataSourceMap(), metaDataContexts, transactionContexts, TransactionTypeHolder.get()); } @Override public ShardingSphereConnection getConnection(final String username, final String password) { return getConnection(); } ... } org.apache.shardingsphere.driver.jdbc.core.connection.ShardingSphereConnection\npublic final class ShardingSphereConnection extends AbstractConnectionAdapter implements ExecutorJDBCManager { // 数据源map  private final Map\u0026lt;String, DataSource\u0026gt; dataSourceMap; private final MetaDataContexts metaDataContexts; //事务类型 LOCAL, XA, BASE;  private final TransactionType transactionType; private final ShardingTransactionManager shardingTransactionManager; ... @Override public PreparedStatement prepareStatement(final String sql, final int resultSetType, final int resultSetConcurrency, final int resultSetHoldability) throws SQLException { return new ShardingSpherePreparedStatement(this, sql, resultSetType, resultSetConcurrency, resultSetHoldability); } ... @Override public Statement createStatement(final int resultSetType, final int resultSetConcurrency, final int resultSetHoldability) { return new ShardingSphereStatement(this, resultSetType, resultSetConcurrency, resultSetHoldability); } ... } org.apache.shardingsphere.driver.jdbc.core.statement.ShardingSpherePreparedStatement\npublic final class ShardingSpherePreparedStatement extends AbstractPreparedStatementAdapter { ... @Override public ResultSet executeQuery() throws SQLException { ResultSet result; try { clearPrevious(); executionContext = createExecutionContext(); List\u0026lt;QueryResult\u0026gt; queryResults = executeQuery0(); // 执行的查询结构通过归并引擎归并  MergedResult mergedResult = mergeQuery(queryResults); result = new ShardingSphereResultSet(getResultSetsForShardingSphereResultSet(), mergedResult, this, executionContext); } finally { clearBatch(); } currentResultSet = result; return result; } @Override public int executeUpdate() throws SQLException { ... } @Override public boolean execute() throws SQLException { ... } // 调用归并引擎  private MergedResult mergeQuery(final List\u0026lt;QueryResult\u0026gt; queryResults) throws SQLException { ShardingSphereMetaData metaData = metaDataContexts.getDefaultMetaData(); MergeEngine mergeEngine = new MergeEngine( metaDataContexts.getDefaultMetaData().getResource().getDatabaseType(), metaData.getSchema(), metaDataContexts.getProps(), metaData.getRuleMetaData().getRules()); return mergeEngine.merge(queryResults, executionContext.getSqlStatementContext()); } ... } 执行流程 再接着调试栈来看\n执行流程为\n下面简单分析：\npublic final class ShardingSpherePreparedStatement extends AbstractPreparedStatementAdapter { private ShardingSpherePreparedStatement(final ShardingSphereConnection connection, final String sql, final int resultSetType, final int resultSetConcurrency, final int resultSetHoldability, final boolean returnGeneratedKeys) throws SQLException { .... // 各种初始化省略  // 敲黑板： 1， 使用解析器引擎解析sql语句，得到结果sqlStatement。 SqlStatement封装了sql解析后各类AST节点（DDL,DML,DCL...）后面细讲  ShardingSphereSQLParserEngine sqlParserEngine = new ShardingSphereSQLParserEngine(DatabaseTypeRegistry.getTrunkDatabaseTypeName(metaDataContexts.getDefaultMetaData().getResource().getDatabaseType())); sqlStatement = sqlParserEngine.parse(sql, true); ... // sql执行器 RawExecutor  rawExecutor = new RawExecutor(metaDataContexts.getExecutorEngine(), connection.isHoldTransaction(), metaDataContexts.getProps()); .... // 各种初始化省略  } ... @Override public int executeUpdate() throws SQLException { try { clearPrevious(); // 创建执行的上下文，创建过程中完成了路由的解析，sQL改写真实sql,  executionContext = createExecutionContext(); if (metaDataContexts.getDefaultMetaData().getRuleMetaData().getRules().stream().anyMatch(each -\u0026gt; each instanceof RawExecutionRule)) { Collection\u0026lt;ExecuteResult\u0026gt; executeResults = rawExecutor.execute(createRawExecutionGroupContext(), executionContext.getSqlStatementContext(), new RawSQLExecutorCallback()); accumulate(executeResults); } // 下面会调用执行  ExecutionGroupContext\u0026lt;JDBCExecutionUnit\u0026gt; executionGroupContext = createExecutionGroupContext(); cacheStatements(executionGroupContext.getInputGroups()); // DriverJDBCExecutor会调用执行引擎执行 driverJDBCExecutor.executeUpdate下面会简单分析  return driverJDBCExecutor.executeUpdate(executionGroupContext, executionContext.getSqlStatementContext(), executionContext.getRouteContext().getRouteUnits(), createExecuteUpdateCallback()); } finally { clearBatch(); } } ... // 创建执行的上下文  private ExecutionContext createExecutionContext() { // 创建逻辑SQL  LogicSQL logicSQL = createLogicSQL(); // SQLCheckEngine检查SQL的是否合法  SQLCheckEngine.check(logicSQL.getSqlStatementContext().getSqlStatement(), logicSQL.getParameters(), metaDataContexts.getDefaultMetaData().getRuleMetaData().getRules(), DefaultSchema.LOGIC_NAME, metaDataContexts.getMetaDataMap(), null); //内核处理器生成执行上下文  ExecutionContext result = kernelProcessor.generateExecutionContext(logicSQL, metaDataContexts.getDefaultMetaData(), metaDataContexts.getProps()); findGeneratedKey(result).ifPresent(generatedKey -\u0026gt; generatedValues.addAll(generatedKey.getGeneratedValues())); return result; } // 创建逻辑SQL  private LogicSQL createLogicSQL() { List\u0026lt;Object\u0026gt; parameters = new ArrayList\u0026lt;\u0026gt;(getParameters()); ShardingSphereSchema schema = metaDataContexts.getDefaultMetaData().getSchema(); SQLStatementContext\u0026lt;?\u0026gt; sqlStatementContext = SQLStatementContextFactory.newInstance(schema, parameters, sqlStatement); return new LogicSQL(sqlStatementContext, sql, parameters); } ... } 再看内核处理器如何生成执行上下文\norg.apache.shardingsphere.infra.context.kernel.KernelProcessor\n/** * Kernel processor. 内核处理器 */ public final class KernelProcessor { /** * Generate execution context. * 创建执行上下文 * @param logicSQL logic SQL * @param metaData ShardingSphere meta data * @param props configuration properties * @return execution context */ public ExecutionContext generateExecutionContext(final LogicSQL logicSQL, final ShardingSphereMetaData metaData, final ConfigurationProperties props) { //2 使用路由引擎创建路由  RouteContext routeContext = route(logicSQL, metaData, props); //3 使用改写引擎改写出真实执行的sql  SQLRewriteResult rewriteResult = rewrite(logicSQL, metaData, props, routeContext); //4 创建执行上下文  ExecutionContext result = createExecutionContext(logicSQL, metaData, routeContext, rewriteResult); // 日志  logSQL(logicSQL, props, result); return result; } // 使用路由引擎创建路由 SQLRouteEngine(...).route(..)  private RouteContext route(final LogicSQL logicSQL, final ShardingSphereMetaData metaData, final ConfigurationProperties props) { return new SQLRouteEngine(metaData.getRuleMetaData().getRules(), props).route(logicSQL, metaData); } // SQLRewriteEntry改写引擎  private SQLRewriteResult rewrite(final LogicSQL logicSQL, final ShardingSphereMetaData metaData, final ConfigurationProperties props, final RouteContext routeContext) { return new SQLRewriteEntry( metaData.getSchema(), props, metaData.getRuleMetaData().getRules()).rewrite(logicSQL.getSql(), logicSQL.getParameters(), logicSQL.getSqlStatementContext(), routeContext); } // 创建执行上下文  private ExecutionContext createExecutionContext(final LogicSQL logicSQL, final ShardingSphereMetaData metaData, final RouteContext routeContext, final SQLRewriteResult rewriteResult) { return new ExecutionContext(logicSQL.getSqlStatementContext(), ExecutionContextBuilder.build(metaData, rewriteResult, logicSQL.getSqlStatementContext()), routeContext); } private void logSQL(final LogicSQL logicSQL, final ConfigurationProperties props, final ExecutionContext executionContext) { if (props.\u0026lt;Boolean\u0026gt;getValue(ConfigurationPropertyKey.SQL_SHOW)) { SQLLogger.logSQL(logicSQL, props.\u0026lt;Boolean\u0026gt;getValue(ConfigurationPropertyKey.SQL_SIMPLE), executionContext); } } } 解析引擎 路由引擎 @RequiredArgsConstructor public final class SQLRouteEngine { private final Collection\u0026lt;ShardingSphereRule\u0026gt; rules; private final ConfigurationProperties props; /** * Route SQL. * * @param logicSQL logic SQL * @param metaData ShardingSphere meta data * @return route context */ public RouteContext route(final LogicSQL logicSQL, final ShardingSphereMetaData metaData) { SQLRouteExecutor executor = isNeedAllSchemas(logicSQL.getSqlStatementContext().getSqlStatement()) ? new AllSQLRouteExecutor() : new PartialSQLRouteExecutor(rules, props); // 进行路由计算，生成路由结果上下文RouteContext  return executor.route(logicSQL, metaData); } // TODO use dynamic config to judge UnconfiguredSchema  private boolean isNeedAllSchemas(final SQLStatement sqlStatement) { return sqlStatement instanceof MySQLShowTablesStatement; } } SQL改写  加密的SQL改写\n影子库SQL改写\n分片的SQL改写\n 执行引擎  将路由和改写完成之后的真实 SQL 安全且高效发送到底层数据源执行。\n public final class DriverJDBCExecutor { ... /** * Execute update. * * @param executionGroupContext execution group context * @param sqlStatementContext SQL statement context * @param routeUnits route units * @param callback JDBC executor callback 回调 * @return effected records count * @throws SQLException SQL exception */ public int executeUpdate(final ExecutionGroupContext\u0026lt;JDBCExecutionUnit\u0026gt; executionGroupContext, final SQLStatementContext\u0026lt;?\u0026gt; sqlStatementContext, final Collection\u0026lt;RouteUnit\u0026gt; routeUnits, final JDBCExecutorCallback\u0026lt;Integer\u0026gt; callback) throws SQLException { try { // 执行引擎初始化  ExecuteProcessEngine.initialize(sqlStatementContext, executionGroupContext, metaDataContexts.getProps()); List\u0026lt;Integer\u0026gt; results = jdbcLockEngine.execute(executionGroupContext, sqlStatementContext, routeUnits, callback); int result = isNeedAccumulate(metaDataContexts.getDefaultMetaData().getRuleMetaData().getRules(), sqlStatementContext) ? accumulate(results) : results.get(0); ExecuteProcessEngine.finish(executionGroupContext.getExecutionID()); return result; } finally { ExecuteProcessEngine.clean(); } } ... } 归并引擎 对于查询类，有结果会使用归并引擎合并结果 MergeEngine(..).merge(..)\n 将从各个数据节点获取的多数据结果集，组合成为一个结果集并正确的返回至请求客户端，称为结果归并。\n org.apache.shardingsphere.infra.merge.MergeEngine\npublic final class MergeEngine { static { ShardingSphereServiceLoader.register(ResultProcessEngine.class); } private final DatabaseType databaseType; private final ShardingSphereSchema schema; private final ConfigurationProperties props; @SuppressWarnings(\u0026#34;rawtypes\u0026#34;) private final Map\u0026lt;ShardingSphereRule, ResultProcessEngine\u0026gt; engines; public MergeEngine(final DatabaseType databaseType, final ShardingSphereSchema schema, final ConfigurationProperties props, final Collection\u0026lt;ShardingSphereRule\u0026gt; rules) { this.databaseType = databaseType; this.schema = schema; this.props = props; engines = OrderedSPIRegistry.getRegisteredServices(rules, ResultProcessEngine.class); } /** * Merge. * * @param queryResults query results * @param sqlStatementContext SQL statement context * @return merged result * @throws SQLException SQL exception */ public MergedResult merge(final List\u0026lt;QueryResult\u0026gt; queryResults, final SQLStatementContext\u0026lt;?\u0026gt; sqlStatementContext) throws SQLException { //生成合并结果集  Optional\u0026lt;MergedResult\u0026gt; mergedResult = executeMerge(queryResults, sqlStatementContext); //对合并结果集装饰处理  Optional\u0026lt;MergedResult\u0026gt; result = mergedResult.isPresent() ? Optional.of(decorate(mergedResult.get(), sqlStatementContext)) : decorate(queryResults.get(0), sqlStatementContext); return result.orElseGet(() -\u0026gt; new TransparentMergedResult(queryResults.get(0))); } @SuppressWarnings({\u0026#34;unchecked\u0026#34;, \u0026#34;rawtypes\u0026#34;}) private Optional\u0026lt;MergedResult\u0026gt; executeMerge(final List\u0026lt;QueryResult\u0026gt; queryResults, final SQLStatementContext\u0026lt;?\u0026gt; sqlStatementContext) throws SQLException { for (Entry\u0026lt;ShardingSphereRule, ResultProcessEngine\u0026gt; entry : engines.entrySet()) { if (entry.getValue() instanceof ResultMergerEngine) { ResultMerger resultMerger = ((ResultMergerEngine) entry.getValue()).newInstance(databaseType, entry.getKey(), props, sqlStatementContext); // ResultMerger进行合并  return Optional.of(resultMerger.merge(queryResults, sqlStatementContext, schema)); } } return Optional.empty(); } @SuppressWarnings({\u0026#34;unchecked\u0026#34;, \u0026#34;rawtypes\u0026#34;}) private MergedResult decorate(final MergedResult mergedResult, final SQLStatementContext\u0026lt;?\u0026gt; sqlStatementContext) throws SQLException { MergedResult result = null; for (Entry\u0026lt;ShardingSphereRule, ResultProcessEngine\u0026gt; entry : engines.entrySet()) { if (entry.getValue() instanceof ResultDecoratorEngine) { ResultDecorator resultDecorator = ((ResultDecoratorEngine) entry.getValue()).newInstance(databaseType, schema, entry.getKey(), props, sqlStatementContext); result = null == result ? resultDecorator.decorate(mergedResult, sqlStatementContext, entry.getKey()) : resultDecorator.decorate(result, sqlStatementContext, entry.getKey()); } } return null == result ? mergedResult : result; } @SuppressWarnings({\u0026#34;unchecked\u0026#34;, \u0026#34;rawtypes\u0026#34;}) private Optional\u0026lt;MergedResult\u0026gt; decorate(final QueryResult queryResult, final SQLStatementContext\u0026lt;?\u0026gt; sqlStatementContext) throws SQLException { MergedResult result = null; for (Entry\u0026lt;ShardingSphereRule, ResultProcessEngine\u0026gt; entry : engines.entrySet()) { if (entry.getValue() instanceof ResultDecoratorEngine) { ResultDecorator resultDecorator = ((ResultDecoratorEngine) entry.getValue()).newInstance(databaseType, schema, entry.getKey(), props, sqlStatementContext); result = null == result ? resultDecorator.decorate(queryResult, sqlStatementContext, entry.getKey()) : resultDecorator.decorate(result, sqlStatementContext, entry.getKey()); } } return Optional.ofNullable(result); } } ResultMerger\n 以上对整个流程中各个关键节点进行简单分析，后续对每个节点做详细学习  ","date":"2021-09-02T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/shardingsphere5.0.0.beta%E6%BA%90%E7%A0%81%E6%80%BB%E8%A7%88/","title":"shardingsphere（5.0.0.beta）源码总览"},{"content":"分布式任务调度Hodor源码分析（整体结构） 简介 Hodor是一个高性能的分布式任务调度框架。\ngithub地址(https://github.com/tincopper/hodor)\n架构图  因无任何文档，以下架构图纯阅读源码个人理解后手工所画：\n 代码目录及分析   hodor-admin\n待开发：配合前端页面做任务展示\n  hodor-client\n客户端，用户app通过该客户端将任务信息提交到hodor-server，供其调度\n集成了nettyserver服务，接收来自hodor-server的任务执行请求\n  hodor-client-demo\n用户app示例：集成了hodor-client\n  hodor-common\n通用库\n 环形队列 观察者模式（事件发布监听）模型 Excutor：多线程封装 Extension: SPI扩展方式封装 负载均衡 存储：本地缓存/mysql/h2 异常 日志 Distributor高性能队列    hodor-core\n简单的spring mybatis 工程：对任务/任务执行记录等数据入库（mysql）\n  hodor-extension\n  hodor-model\n实体\n  hodor-register\n注册中心封装，目前只实现了zookeeper\n hodor-register-api Hodor-register-zookeeper    hodor-remoting\nnetty http客户端\nnetty http服务端\n  hodor-scheduler\n任务定时的封装\n目前只实现了quartz\nquartz当到达\n hodor-scheduler-api hodor-scheduler-quartz    hodor-server\n1，hodor-server集成了注册中心客户端，监听server服务的变更；\n2， hodor-server注册到注册中心，并选举一个leader\n3， hodor-server通过集成hodor-remoting的nettyserver接收hodor-client发送过来的注册定时任务的命令\n4， hodor-server通过注册的server服务，合理分配任务\n5，记录任务到db，并记录日志，再向hodor-scheduler-quartz注册任务\n6，hodor-scheduler-quartz当某个任务到达执行时刻，触发 hodor-server的任务执行器去下发任务\n7， hodor-server通过集成hodor-remoting的nettyclient向hodor-client下发任务执行命令\n  ","date":"2021-09-01T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6hodor/","title":"分布式任务调度Hodor"},{"content":"docusaurus构建website 环境   Node.js version \u0026gt;= 12.13.0 以上(node -v 查看)，使用了国际化i18n，则Node.js version \u0026gt;=14以上\n  Yarn version \u0026gt;= 1.5 ( yarn \u0026ndash;version查看). mac下可以使用n管理node版本\n  Title logo等文案，首页展示 待讨论\n菜单调整   Nav : 文档 社区 新闻 博客 links 国际化切换 搜索\n  Documentation Community News Blog Links\n  Footer:\n  首页   下载按钮 文档按钮 star按钮修改\n  样式修改\n  国际化语言 yarn write-translations --locale zh 参考https://docusaurus.io/zh-CN/docs/cli#docusaurus-write-translations-sitedir\n中英文两个版本的文件名称保持一致。文档中没有指定sidebar_position时，默认按文件名称在菜单栏排序\n看中文效果\nyarn run start -- --locale zh 多版本 yarn run docusaurus docs:version 2.3.0 yarn run docusaurus docs:version 2.4.0 历史版本的国际化参考\nhttps://docusaurus.io/zh-CN/docs/api/plugins/@docusaurus/plugin-content-docs#i18n\n归档目录\n归档文件翻译目录\nhugo文档迁移注意事项   约定大于配置：很多目录都是约定好的。\n建议快速通读一遍文档，遇到编译问题，能快速定位到具体章节去查看。\n  keywords必须为数组,原来写的字符串\nkeywords: [\u0026ldquo;Apache shenyu\u0026rdquo;]\n  不支持md文件里的html标签 比如   sidebar_position: 1 指定文档在菜单栏的顺序第一位。\n  md文件中title就是菜单栏显示的名称，无法显示不一样的菜单名和文档标题名。\n  md中类似的标签，在发布时生效。\n本地想看效果，可以yarn build.然后将静态文件通过nginx代理访问。\n  插画使用: https://www.iconfont.cn/illustrations/detail?spm=a313x.7781069.1998910419.d9df05512\u0026amp;cid=24712\nhttps://www.iconfont.cn/collections/detail?spm=a313x.7781069.1998910419.d9df05512\u0026amp;cid=34403\n参考 https://docusaurus.io/zh-CN/docs\n","date":"2021-08-22T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/docusaurus%E6%9E%84%E5%BB%BAwebsite/","title":"docusaurus构建website"},{"content":"开放平台appKey appSecret设计 1，手动注册客户端，服务端返回appKey appSecret 客户端和服务端都保存ak as 2, 客户端第一次请求 通过appKey请求token 服务端通过appKey，appSecret，时间戳，用户的必要信息生成token，可以使用JWTToken.\n​ token具有有效期：Token是客户端访问服务端的凭证。\n3，客户端后续请求 参数为： Token + 当前时间戳 + 参数 +签名sign1\n签名sign1为 Token + 当前时间戳 + 参数+appSecret 按照一定签名算法比如 SHA256生成的签名字符串，为了保证请求中参数不被流量劫持篡改\n4，服务端收到请求进行校验  时间戳校验：请求时间和当前服务端时间大于一定范围，比如5分钟，拒绝执行 Token解析：token过期拒绝执行 签名校验： 通过token解析获取到appKey，再获取到appSecret， 使用与客户端相同的签名算法SHA256得到签名sign2, 如果sign1不等于sign2，拒绝执行  ","date":"2021-08-19T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E5%BC%80%E6%94%BE%E5%B9%B3%E5%8F%B0appkeyappsecret%E8%AE%BE%E8%AE%A1/","title":"开放平台appKey，appSecret设计"},{"content":"1. jmx_exporter 下载jmx_exporter ubuntu:/# mkdir -p /usr/local/prometheus/jmx_exporter ubuntu:/# cd /usr/local/prometheus/jmx_exporter ubuntu:/# wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.16.1/jmx_prometheus_javaagent-0.16.1.jar 配置文件jmx_exporter jmx_exporter.yml\n vim /usr/local/prometheus/jmx_exporter/jmx_exporter.yml --- rules: - pattern: \u0026quot;.*\u0026quot; java agent运行jmx_exporter 以java agent的方式启动你的一个java应用\njava -javaagent:/usr/local/prometheus/jmx_prometheus_javaagent-0.16.1.jar=3010:/usr/local/prometheus/jmx_exporter.yml -jar xxx-web-0.1-SNAPSHOT.jar 2. prometheus docker方式下载运行 # docker pull prom/prometheus #下载docker镜像 # mkdir -p /etc/prometheus # vim /etc/prometheus/prometheus.yml #配置 # docker run -d \\ -p 192.168.3.13:9090:9090 \\  -v /etc/prometheus:/etc/prometheus \\  prom/prometheus; prometheus中配置上步的jmx的metrics global:scrape_interval:15sscrape_timeout:10sevaluation_interval:15salerting:alertmanagers:- follow_redirects:truescheme:httptimeout:10sapi_version:v2static_configs:- targets:[]scrape_configs:- job_name:prometheushonor_timestamps:truescrape_interval:15sscrape_timeout:10smetrics_path:/metricsscheme:httpfollow_redirects:truestatic_configs:- targets:- 192.168.3.13:9090### 以下为jmx_exporter地址：需改为你实际的- job_name:\u0026#39;jmx\u0026#39;static_configs:scrape_interval:15s- targets:[\u0026#39;192.168.3.14:3010\u0026#39;]验证 访问http://192.168.3.13:9090 可看到读取的jmx_exporter的metrics\n3. Grafana连接Prometheus docker pull grafana/grafana docker run -d --name=grafana -p 3000:3000 grafana/grafana 访问http://192.168.3.13:3000 使用admin/admin即可登录\n配置prometheus数据源 配置dashboard 找一个合适的dashboard导入。如https://grafana.com/grafana/dashboards/3457\n效果 参考  prometheus安装 https://prometheus.io/docs/prometheus/latest/installation/   JMX Exporter 项目地址: https://github.com/prometheus/jmx_exporter JVM 监控面板: https://grafana.com/grafana/dashboards/3457 Grafana(https://grafana.com/)  ","date":"2021-08-15T14:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/prometheus%E7%9B%91%E6%8E%A7jvm/","title":"Prometheus监控JVM"},{"content":"# 清除nodejs的cache sudo npm cache clean -f # 由于您可能已经拥有node，最简单的安装方式n是npm： sudo npm install -g n # node所有版本 npm view node versions # 升级到最新版本 sudo n latest # 升级到稳定版本 sudo n stable # 升级到具体版本号 sudo n xx.xx ","date":"2021-07-21T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/mac%E4%B8%8Bnode%E5%8D%87%E7%BA%A7/","title":"mac下node升级"},{"content":"Disruptor通过以下设计来解决队列速度慢的问题：   环形数组结构\n  元素位置定位\n数组长度2^n， 位运算，加快定位的速度\n  无锁设计\nCas操作保证线程安全\n  参考 https://tech.meituan.com/2016/11/18/disruptor.html\nhttps://blog.csdn.net/liweisnake/article/details/9113119\n","date":"2021-07-20T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/disruptor%E9%AB%98%E6%80%A7%E8%83%BD%E9%98%9F%E5%88%97/","title":"Disruptor高性能队列"},{"content":"Envoy概述 Envoy 是以 C++ 开发的高性能代理;\n其内置服务发现、负载均衡、TLS终止、HTTP/2、GRPC代理、熔断器、健康检查，基于百分比流量拆分的灰度发布、故障注入等功能\n Downstream：下游主机，指连接到Envoy的主机，这些主机用来发送请求并接受响应。 Upstream：上游主机，指接收来自Envoy连接和请求的主机，并返回响应。 Listener：服务或程序的监听器， Envoy暴露一个或多个监听器监听下游主机的请求，当监听到请求时，通过Filter Chain把对请求的处理全部抽象为Filter， 例如ReadFilter、WriteFilter、HttpFilter等。 Cluster：服务提供集群，指Envoy连接的一组逻辑相同的上游主机。Envoy通过服务发现功能来发现集群内的成员，通过负载均衡功能将流量路由到集群的各个成员。 xDS：xDS中的x是一个代词，类似云计算里的XaaS可以指代IaaS、PaaS、SaaS等。DS为Discovery Service，即发现服务的意思。xDS包括CDS（cluster discovery service）、RDS（route discovery service）、EDS（endpoint discovery service）、ADS（aggregated discovery service），其中ADS称为聚合的发现服务，是对CDS、RDS、LDS、EDS服务的统一封装，解决CDS、RDS、LDS、EDS信息更新顺序依赖的问题，从而保证以一定的顺序同步各类配置信息。以上Endpoint、Cluster、Route的概念介绍如下：  Endpoint：一个具体的“应用实例”，类似于Kubernetes中的一个Pod； Cluster：可以理解“应用集群”，对应提供相同服务的一个或多个Endpoint， 类似Kubernetes中Service概念，即一个Service提供多个相同服务的Pod； Route：当我们做金丝雀发布部署时，同一个服务会有多个版本，这时需要Route规则规定请求如何路由到其中的某个版本上。    http://www.dockone.io/article/9116\n","date":"2021-07-20T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/envoy/","title":"Envoy"},{"content":"哈希摘要算法 好的哈希摘要算法需要具备\n  一是易变性，这是指算法的输入端发生了任何一点细微变动，都会引发雪崩效应,使得输出端的结果产生极大的变化\n常常被用来校验数据是否被篡改\n  二是不可逆性，摘要的过程是单向的，不可能从摘要的结果中逆向还原出输入值来\n  对称加密和非对称加密   对称加密 对称加密指的就是加密和解密使用同一个秘钥，所以叫做对称加密。对称加密只有一个秘钥，作为私钥。 常见的对称加密算法：DES，3DES，AES等等。\n  非对称加密 非对称加密指的是：加密和解密使用不同的秘钥，一把作为公开的公钥，另一把作为私钥。公钥加密的信息，只有私钥才能解密。私钥加密的信息，只有公钥才能解密。 常见的非对称加密算法：RSA，ECC\n  区别 对称加密算法相比非对称加密算法来说，加解密的效率要高得多。但是缺陷在于对于秘钥的管理上，以及在非安全信道中通讯时，密钥交换的安全性不能保障。所以在实际的网络环境中，会将两者混合使用.\n  对称加密传输大量数据\n非对称加密永远对称加密的密钥协商，传输。\n 例如针对C/S模型，\n服务端计算出一对秘钥pub/pri。将私钥保密，将公钥公开。 客户端请求服务端时，拿到服务端的公钥pub。 客户端通过AES计算出一个对称加密的秘钥X。 然后使用pub将X进行加密。 客户端将加密后的密文发送给服务端。服务端通过pri解密获得X。 然后两边的通讯内容就通过对称密钥X以对称加密算法来加解密。\n 三种密码学算法的对比    类型 特点 常见实现 主要用途 主要局限     哈希摘要 不可逆，即不能解密，所以并不是加密算法，只是一些场景把它当作加密算法使用。 易变性，输入发生 1 Bit 变动，就可能导致输出结果 50%的内容发生改变。 无论输入长度多少，输出长度固定（2 的 N 次幂）。 MD2/4/5/6、SHA0/1/256/512 摘要 无法解密   对称加密 加密是指加密和解密是一样的密钥。 设计难度相对较小，执行速度相对较块。 加密明文长度不受限制。 DES、AES、RC4、IDEA 加密 要解决如何把密钥安全地传递给解密者。   非对称加密 加密和解密使用的是不同的密钥。 明文长度不能超过公钥长度。 RSA、BCDSA、ElGamal 签名、传递密钥 性能与加密明文长度受限。    数字证书 解决公钥被劫持篡改的问题。由权威机构颁发保证\nHTTPS: HTTP over SSL/TLS TLS处于会话层\n客户端和服务端均通过握手过程协商出了许多信息，譬如一个只有双方才知道的随机产生的密钥(非对称加密算法比如RSA进行密钥协商)、传输过程中要采用的对称加密算法（比如AES128）、压缩算法等，此后该连接的通信将使用此密钥和加密算法进行加密、解密和压缩。\n保障所有信息都是第三方无法窃听（加密传输:对称加密算法）、无法篡改（一旦篡改通信算法会立刻发现）、无法冒充（证书验证身份）\n","date":"2021-06-21T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/","title":"加密算法"},{"content":"spi 数据同步 请求执行路径 服务调用 admin定时探活 ","date":"2021-06-19T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E5%9B%BE%E8%A7%A3shenyu/","title":"图解shenyu"},{"content":"背景 随着业务的发展，系统规模也会越来越大，各微服务间的调用关系也越来越错综复杂，每一个前端请求都会形成一条复杂的分布式服务调用链路，在每条链路中任何一个依赖服务出现延迟过高或错误的时候都会引起请求最后的失败。\n链路追踪原理 实现请求跟踪 当请求发送到分布式系统的入口端点时，只需要服务跟踪框架为该请求创建一个唯一的跟踪标识Trace ID，\n同时在分布式系统内部流转的时候，框架失踪保持该唯一标识，直到返回给请求方位置。\ntrace：服务追踪的追踪单元是从客户发起请求（request）抵达被追踪系统的边界开始，到被追踪系统向客户返回响应（response）为止的过程，称为一\n个“trace”\n统计各处理单元的时间延迟 当请求到达各个服务组件时，也是通过一个唯一标识Span ID来标记它的开始，具体过程以及结束。对每一个Span来说，它必须有开始和结束两个节点，通过记录开始Span和结束Span的时间戳，就能统计出该Span的时间延迟，除了时间戳记录之外，它还可以包含一些其他元数据，比如时间名称、请求信息等。\nUI可视化 APM技术组件 Zipkin+Sleuth Apache SkyWalking Cat Pinpoint 特点对比 ","date":"2021-06-12T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E6%9C%8D%E5%8A%A1%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/","title":"服务链路追踪"},{"content":"kubelet日志 journalctl -fu kubelet kubectl -h kubectl get namespaces kubectl get pods -A kubectl logs -f --tail=200 -l app=account -n bookstore-microservices 常用命令 #查看端口映射 kubectl get svc -n kube-system #查看 secret kubectl get secret -n kube-system #查看 token kubectl describe secret kubernetes-dashboard --namespace=kube-system #k8s 无法启动，查看日志，查找Failed journalctl -xefu kubelet #查看pod错误日志 kubectl logs kubernetes-dashboard-8556c848b7-4kpzd --namespace=kube-system #对资源进行配置 kubectl apply -f kubernetes-dashboard.yaml kubectl delete -f kubernetes-dashboard.ya YAML配置文件管理对象 对象管理： # 创建deployment资源 kubectl create -f nginx-deployment.yaml # 查看deployment kubectl get deploy # 查看ReplicaSet kubectl get rs # 查看pods所有标签 kubectl get pods --show-labels # 根据标签查看pods kubectl get pods -l app=nginx # 滚动更新镜像 kubectl set image deployment/nginx-deployment nginx=nginx:1.11 或者 kubectl edit deployment/nginx-deployment 或者 kubectl apply -f nginx-deployment.yaml # 实时观察发布状态： kubectl rollout status deployment/nginx-deployment # 查看deployment历史修订版本 kubectl rollout history deployment/nginx-deployment kubectl rollout history deployment/nginx-deployment --revision=3 # 回滚到以前版本 kubectl rollout undo deployment/nginx-deployment kubectl rollout undo deployment/nginx-deployment --to-revision=3 # 扩容deployment的Pod副本数量 kubectl scale deployment nginx-deployment --replicas=10 # 设置启动扩容/缩容 kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80 ","date":"2021-06-10T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/k8s-kubectl%E5%91%BD%E4%BB%A4/","title":"k8s Kubectl命令"},{"content":"单体架构(spring boot)   优点：所有代码都运行在同一个进程空间之内，所有模块、方法的调用都无须考虑网络分区、对象复制这些麻烦的事和性能损失。\n  缺点：损失了各个功能模块的自治、隔离能力；\n​\t由于隔离能力的缺失难以阻断错误传播、不便于动态更新程序以外，还面临难以技术异构的困难\n​\t可以 使用OSGi 这种运行时模块化框架，但是太复杂了。\n  SOA 架构（Service-Oriented Architecture） 面向服务的架构是一次具体地、系统性地成功解决分布式服务主要问题的架构模式。\nSOAP 协议被逐渐边缘化的本质原因：过于严格的规范定义带来过度的复杂性。而构建在 SOAP 基础之上的 ESB、BPM、SCA、SDO 等诸多上层建筑，进一步加剧了这种复杂性。\n微服务架构(spring cloud) 微服务是一种软件开发技术，是一种 SOA 的变体形式。\n升级背景：\n 制约软件质量与业务能力提升的最大因素是人而非硬件： 单体架构没有什么有效阻断错误传播的手段 技术异构的需求从可选渐渐成为必须：很多 Java 不擅长的事情 人工智能python 分布式协调工具 Etcd ,NSI C 编写的 Redis， \u0026hellip;  由于隔离能力的缺失，单体除了难以阻断错误传播、不便于动态更新程序以外，还面临难以技术异构的困难，每个模块的代码都通常需要使用一样的程序语言，乃至一样的编程框架去开发。\n随着软件架构演进，构筑可靠系统从“追求尽量不出错”，到正视“出错是必然”的观念转变，才是微服务架构得以挑战并逐步开始取代运作了数十年的单体架构的底气所在\n微服务时代充满着自由的气息，微服务时代充斥着迷茫的选择。\n微服务架构(Kubernetes) 升级背景：\n  微服务中的各种新技术名词，如配置中心、服务发现、网关、熔断、负载均衡等等带来的技术组件 Config、Eureka、Zuul、Hystrix、Ribbon、Feign 等\n占据了产品的大部分编译后的代码容量\n之前在应用层面而不是基础设施层面去解决这些分布式问题，完全是因为由硬件构成的基础设施，跟不上由软件构成的应用服务的灵活性的无奈之举\n  以 Docker Swarm、Apache Mesos 与 Kubernetes 为主要竞争者的“容器战争”终于有了明确的结果，Kubernetes 登基加冕\n容器动态构建出 DNS 服务器、服务负载均衡器等一系列虚拟化的基础设施，去代替原有的应用层面的技术组件\n  Microservice https://martinfowler.com/articles/microservices.html\n服务网格（Service Mesh） 升级背景：\n  基础设施是针对整个容器来管理的，粒度相对粗旷，只能到容器层面，对单个远程服务就很难有效管控。\n  服务的监控、认证、授权、安全、负载均衡等都有可能面临细化管理的需求\n譬如服务调用时的负载均衡，往往需要根据流量特征，调整负载均衡的层次、算法，等等，而 DNS 尽管能实现一定程度的负载均衡，但通常并不能满足这些额外的需求。\n  “边车代理模式”（Sidecar Proxy）   数据平面通信：这个代理除了实现正常的服务间通信外（称为数据平面通信）\n  控制平面通信：还接收来自控制器的指令（称为控制平面通信），根据控制平面中的配置，对数据平面通信的内容进行分析处理，以实现熔断、认证、度量、监控、负载均衡等各种附加功能。\n通过\n主要框架：Istio\n  上帝的归上帝，凯撒的归凯撒，业务与技术完全分离，远程与本地完全透明，也许这就是最好的时代了吧？\n无服务 更应该成为 无服务器\n包含两方面：\n 后端设施：指数据库、消息队列、日志、存储，等等这一类用于支撑业务逻辑运行，但本身无业务含义的技术组件，这些后端设施都运行在云中，无服务中称其为“后端即服务”（Backend as a Service，BaaS）。 函数： 指业务逻辑代码，这里函数的概念与粒度，都已经很接近于程序编码角度的函数了，其区别是无服务中的函数运行在云端，不必考虑算力问题，不必考虑容量规划（从技术角度可以不考虑，从计费的角度你的钱包够不够用还是要掂量一下的），无服务中称其为“函数即服务”（Function as a Service，FaaS）。  学习参考https://icyfenix.cn/architecture/architect-history/\n","date":"2021-06-10T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E6%9E%B6%E6%9E%84%E6%BC%94%E5%8F%98/","title":"架构演变"},{"content":"1.环境准备 安装 Kubernetes 最小需要 2 核处理器、2 GB 内存，且为 x86 架构（暂不支持 ARM 架构)\n本次实验操作系统：ubantu 20.04LTS\nKubernetes 并不在主流 Debian 系统自带的软件源中，所以要手工注册，然后才能使用apt-get安装\n# 添加GPG Key $ sudo curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - # 添加K8S软件源 $ sudo add-apt-repository \u0026#34;deb https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; 如果不能科学上网，可以使用阿里云的软件源地址\n# 添加GPG Key $ curl -fsSL http://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - # 添加K8S软件源 $ sudo add-apt-repository \u0026#34;deb http://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main\u0026#34; 添加源后需要更新\nsudo apt-get update 2. 安装 kubelet、kubectl、kubeadm 官网介绍：https://kubernetes.io/docs/reference/setup-tools/kubeadm/\n kubeadm: 引导启动 Kubernate 集群的命令行工具。 kubelet: 在群集中的所有计算机上运行的组件, 并用来执行如启动 Pods 和 Containers 等操作。 kubectl: 用于操作运行中的集群的命令行工具。  2.1关闭Swap 分区 kubeadm 初始化集群之前，需要关闭Swap 分区，首先基于安全性（如在官方文档中承诺的 Secret 只会在内存中读写，不会落盘）、利于保证节点同步一致性等原因，从 1.8 版开始，Kubernetes 就在它的文档中明确声明了它默认不支持Swap 分区，在未关闭 Swap 分区的机器中，集群将直接无法启动；\n其他参考\n 主要有两个方面原因：\n第一是因为性能问题，在生产环境我们经常会遇到容器性能突然降低的情况，查看原因后，大部分都是因为开启了swap导致的。swap看似解决了有限内存的问题，但这种通过时间换空间的做法也给性能带来了很大问题，尤其是在高并发场景中，很容易导致系统不稳定。\n第二是因为k8s定义的资源模型中，CPU和内存都是确定的可用资源，在调度的时候都会考虑在内。比如，设置了内存设置了limit 2G，就代表最大可用内存是2G，而引入swap（cgroup支持swap限制）后这个模型就变得复杂了，而且需要结合Qos，swap的使用完全是由操作系统根据水位自行调节的，并不直接受kubelet管理\n 一次性关闭\nsudo swapoff -a 永久关闭\n编辑器打开/etc/fstab，注释其中带有“swap”的行即可，\n# 先备份 $ yes | sudo cp /etc/fstab /etc/fstab_bak # 进行修改 $ sudo cat /etc/fstab_bak | grep -v swap \u0026gt; /etc/fstab 3 预拉取镜像 先查看kubeadm版本 v1.21.3\nroot@fengzhenbing-ubuntu:/home/fengzhenbing# kubeadm version kubeadm version: \u0026amp;version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;21\u0026#34;, GitVersion:\u0026#34;v1.21.3\u0026#34;, GitCommit:\u0026#34;ca643a4d1f7bfe34773c74f79527be4afd95bf39\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-07-15T21:03:28Z\u0026#34;, GoVersion:\u0026#34;go1.16.6\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 首先使用以下命令查询当前版本需要哪些镜像：\nfengzhenbing@fengzhenbing-ubuntu:/etc/docker$ kubeadm config images list --kubernetes-version v1.21.3 k8s.gcr.io/kube-apiserver:v1.21.3 k8s.gcr.io/kube-controller-manager:v1.21.3 k8s.gcr.io/kube-scheduler:v1.21.3 k8s.gcr.io/kube-proxy:v1.21.3 k8s.gcr.io/pause:3.4.1 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/coredns/coredns:v1.8.0 k8s.gcr.io 为google官方（Google Container Registry），对于不能科学上网的同学来说，可以使用阿里云加速\n3.1 配置阿里云加速地址 阿里云加速地址获取 https://cr.console.aliyun.com/cn-shanghai/instances/mirrors\nsudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://xxxx.mirror.aliyuncs.com\u0026#34;] } EOF sudo systemctl daemon-reload sudo systemctl restart docker 3.2 拉取镜像，修改tag 对于通过kubeadm config images list --kubernetes-version v1.21.3查找到需要下载的镜像名称及版本后，可以从DockerHub上找存有相同镜像的仓库来拉取。 我是用的k8simage的仓库，找到对应的v1.21.3版本的对应image\n下面为从k8simage拉取到的镜像\nfengzhenbing@fengzhenbing-ubuntu:/etc/docker$ sudo docker image list REPOSITORY TAG IMAGE ID CREATED SIZE k8simage/kube-apiserver v1.21.3 3d174f00aa39 9 days ago 126MB k8simage/kube-scheduler v1.21.3 6be0dc1302e3 9 days ago 50.6MB k8simage/kube-controller-manager v1.21.3 bc2bb319a703 9 days ago 120MB k8simage/kube-proxy v1.21.3 adb2816ea823 9 days ago 103MB k8simage/pause 3.4.1 0f8457a4c2ec 6 months ago 683kB k8simage/etcd 3.4.13-0 0369cf4303ff 11 months ago 253MB k8simage/coredns 1.7.0 bfe3a36ebd25 13 months ago 45.2MB 对每一镜像，1先拉取，2修改tag 3最后删除原来的。以kube-apiserver v1.21.3为例子\nsudo docker pull k8simage/kube-apiserver:v1.21.3 sudo docker tag k8simage/kube-apiserver:v1.21.3 k8s.gcr.io/kube-apiserver:v1.21.3 sudo docker rmi k8simage/kube-apiserver:v1.21.3 上面全部操作结束后 可看到都更新为kubeadm config images list --kubernetes-version v1.21.3所需要的镜像了\nroot@fengzhenbing-ubuntu:/home/fengzhenbing# sudo docker image list REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-apiserver v1.21.3 3d174f00aa39 9 days ago 126MB k8s.gcr.io/kube-scheduler v1.21.3 6be0dc1302e3 9 days ago 50.6MB k8s.gcr.io/kube-proxy v1.21.3 adb2816ea823 9 days ago 103MB k8s.gcr.io/kube-controller-manager v1.21.3 bc2bb319a703 9 days ago 120MB quay.io/coreos/flannel v0.14.0 8522d622299c 2 months ago 67.9MB k8s.gcr.io/pause 3.4.1 0f8457a4c2ec 6 months ago 683kB k8s.gcr.io/etcd 3.4.13-0 0369cf4303ff 11 months ago 253MB k8s.gcr.io/coredns/coredns v1.8.0 bfe3a36ebd25 13 months ago 45.2MB 4 初始化集群控制平面  确保 kubelet 是开机启动的  fengzhenbing@fengzhenbing-ubuntu:/etc/docker$ sudo systemctl start kubelet fengzhenbing@fengzhenbing-ubuntu:/etc/docker$ sudo systemctl enable kubelet  su 直接切换到 root 用户， 保证是root启动部署  kubeadm init \\ --pod-network-cidr=10.244.0.0/16 \\ --kubernetes-version v1.21.3 \\ --control-plane-endpoint 192.168.3.13  参数介绍   --kubernetes-version参数（要注意版本号与 kubelet 一致）的目的是与前面预拉取是一样的，避免额外的网络访问去查询版本号；如果能够科学上网，不需要加这个参数。\n--pod-network-cidr参数是给Flannel网络做网段划分使用的，着在稍后介绍完 CNI 网络插件时会去说明。\n--control-plane-endpoint参数是控制面的地址，强烈建议使用一个域名代替直接的IP地址来建立Kubernetes集群。因为CA证书直接与地址相关，Kubernetes中诸多配置（配置文件、ConfigMap资源）也直接存储了这个地址，一旦更换IP，要想要不重置集群，手工换起来异常麻烦。所以最好使用hostname（仅限单节点实验）或者dns name。\n 执行完，如下提示表明成功\n5 为当前用户生成 kubeconfig 使用 Kubernetes 前需要为当前用户先配置好 admin.conf 文件\nroot@fengzhenbing-ubuntu:~# mkdir -p $HOME/.kube root@fengzhenbing-ubuntu:~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config root@fengzhenbing-ubuntu:~# sudo chown $(id -u):$(id -g) $HOME/.kube/config 6 CNI插件 CNI 即“容器网络接口”\n部署 Kubernetes 时，我们可以有两种网络方案使得以后受管理的容器之间进行网络通讯：\n 使用 Kubernetes 的默认网络： 操作太复杂 使用 CNI 及其插件：  Flannel 插件为比较推荐的\n安装Flannel 插件 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml https://raw.githubusercontent.com 无法访问时，可以找到该yml文件上传到服务器，如下执行\nroot@fengzhenbing-ubuntu:/home/fengzhenbing# kubectl apply -f kube-flannel.yml Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ podsecuritypolicy.policy/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.apps/kube-flannel-ds created $ kubectl taint nodes --all node-role.kubernetes.io/master- 7 kubectl 命令自动补全功能 $ echo \u0026#39;source \u0026lt;(kubectl completion bash)\u0026#39; \u0026gt;\u0026gt; ~/.bashrc $ echo \u0026#39;source /usr/share/bash-completion/bash_completion\u0026#39; \u0026gt;\u0026gt; ~/.bashrc 8 加入其他 Node 节点到 Kubernetes 集群中 kubeadm init执行成功后的反馈内容中有提到：如上面的截图\nYou can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 192.168.3.13:6443 --token qxu02l.g995k2yhkxjh3k80 \\  --discovery-token-ca-cert-hash sha256:b9887f55b739bd89d3b0dbb038e693df9b5b7d9759902ffb2a250288ba1ffc25 \\  --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.3.13:6443 --token qxu02l.g995k2yhkxjh3k80 \\  --discovery-token-ca-cert-hash sha256:b9887f55b739bd89d3b0dbb038e693df9b5b7d9759902ffb2a250288ba1ffc25 Token 的有效时间为 24 小时，如果超时，使用以下命令重新获取：\n$ kubeadm token create --print-join-command 9相关问题 无法访问api端点如下\n对于实验测试非线上来讲，可以直接将system:anonymous加为用户\nkubectl create clusterrolebinding test:anonymous --clusterrole=cluster-admin --user=system:anonymous 对于正式环境，需要创建一个用户并授权\n ","date":"2021-06-08T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/kubeadm%E5%AE%89%E8%A3%85kubernetes/","title":"Kubeadm安装Kubernetes"},{"content":"1, ShutdownHook初识  在Java程序中可以通过添加关闭钩子，实现在程序退出时关闭资源、平滑退出的功能。 并且在以下几种场景将调用该钩子\n 程序正常退出 使用System.exit() 终端使用Ctrl+C触发的中断 系统关闭 使用Kill pid命令干掉进程   具体来讲Runtime.addShutdownHook 添加钩子到 ApplicationShutdownHooks中。\n// Runtime添加钩子（钩子具体来讲就是一个要执行的线程任务） public void addShutdownHook(Thread hook) { SecurityManager sm = System.getSecurityManager(); if (sm != null) { sm.checkPermission(new RuntimePermission(\u0026#34;shutdownHooks\u0026#34;)); } ApplicationShutdownHooks.add(hook); } // Runtime去除钩子  public boolean removeShutdownHook(Thread hook) { SecurityManager sm = System.getSecurityManager(); if (sm != null) { sm.checkPermission(new RuntimePermission(\u0026#34;shutdownHooks\u0026#34;)); } return ApplicationShutdownHooks.remove(hook); } 再看ApplicationShutdownHooks\nclass ApplicationShutdownHooks { /* The set of registered hooks */ private static IdentityHashMap\u0026lt;Thread, Thread\u0026gt; hooks; static { try { Shutdown.add(1 /* shutdown hook invocation order */, false /* not registered if shutdown in progress */, new Runnable() { public void run() { runHooks(); } } ); hooks = new IdentityHashMap\u0026lt;\u0026gt;(); } catch (IllegalStateException e) { // application shutdown hooks cannot be added if  // shutdown is in progress.  hooks = null; } } private ApplicationShutdownHooks() {} /* Add a new shutdown hook. Checks the shutdown state and the hook itself, * but does not do any security checks. */ static synchronized void add(Thread hook) { if(hooks == null) throw new IllegalStateException(\u0026#34;Shutdown in progress\u0026#34;); if (hook.isAlive()) throw new IllegalArgumentException(\u0026#34;Hook already running\u0026#34;); if (hooks.containsKey(hook)) throw new IllegalArgumentException(\u0026#34;Hook previously registered\u0026#34;); hooks.put(hook, hook); } /* Remove a previously-registered hook. Like the add method, this method * does not do any security checks. */ static synchronized boolean remove(Thread hook) { if(hooks == null) throw new IllegalStateException(\u0026#34;Shutdown in progress\u0026#34;); if (hook == null) throw new NullPointerException(); return hooks.remove(hook) != null; } /* Iterates over all application hooks creating a new thread for each * to run in. Hooks are run concurrently and this method waits for * them to finish. */ static void runHooks() { Collection\u0026lt;Thread\u0026gt; threads; synchronized(ApplicationShutdownHooks.class) { threads = hooks.keySet(); hooks = null; } for (Thread hook : threads) { hook.start(); } for (Thread hook : threads) { while (true) { try { hook.join(); break; } catch (InterruptedException ignored) { } } } } } 2, 自定义hooks 往往在应用程序里已经有了一些第三方注册好的hook, 当我们要对将自己自定义的hook放到其他hook之前执行，需要强制干预ApplicationShutdownHooks的hooks，可以通过反射来做\nString className = \u0026#34;java.lang.ApplicationShutdownHooks\u0026#34;; Class\u0026lt;?\u0026gt; clazz = Class.forName(className); Field field = clazz.getDeclaredField(\u0026#34;hooks\u0026#34;); field.setAccessible(true); // 先反射拿到其他的hook, IdentityHashMap\u0026lt;Thread, Thread\u0026gt; otherHookMap = (IdentityHashMap\u0026lt;Thread, Thread\u0026gt;) field.get(clazz); // 将otherHookMap 中 各个hook封装一个延时（比如3s）  // 再将自定义hook 加入 Runtime.getRuntime().addShutdownHook(new Thread(()-\u0026gt;{ System.out.println(\u0026#34;exec my hook\u0026#34;); })); 3, ShenYu中应用 ShenYu项目下线时，会将一些client服务，比如注册中心客户端，优雅停掉。\npublic class ShenyuClientShutdownHook { // 钩子名称  private static String hookNamePrefix = \u0026#34;ShenyuClientShutdownHook\u0026#34;; private static AtomicInteger hookId = new AtomicInteger(0); private static Properties props; private static AtomicBoolean delay = new AtomicBoolean(false); //等待加入延时，但是还没有加入的hooks  private static IdentityHashMap\u0026lt;Thread, Thread\u0026gt; delayHooks = new IdentityHashMap\u0026lt;\u0026gt;(); //已经做了延时的hook  private static IdentityHashMap\u0026lt;Thread, Thread\u0026gt; delayedHooks = new IdentityHashMap\u0026lt;\u0026gt;(); /** * Add shenyu client shutdown hook. * * @param result ShenyuClientRegisterRepository * @param props Properties */ public static void set(final ShenyuClientRegisterRepository result, final Properties props) { String name = hookNamePrefix + \u0026#34;-\u0026#34; + hookId.incrementAndGet(); Runtime.getRuntime().addShutdownHook(new Thread(() -\u0026gt; { result.close(); }, name)); log.info(\u0026#34;Add hook {}\u0026#34;, name); ShenyuClientShutdownHook.props = props; } /** * Delay other shutdown hooks.// 将其他的hooks做延时处理 */ public static void delayOtherHooks() { if (!delay.compareAndSet(false, true)) { return; } TakeoverOtherHooksThread thread = new TakeoverOtherHooksThread(); thread.start(); } /** * Delay other shutdown hooks thread. * 延时处理的线程， 1，反射拿到其他hooks *\t2，遍历hooks,通过线程包装原先的hook线程任务，并在其中加3s的延时 *\t3，将原先的hook线程任务重新加入到ApplicationShutdownHooks中 */ private static class TakeoverOtherHooksThread extends Thread { @SneakyThrows @Override public void run() { int shutdownWaitTime = Integer.parseInt(props.getProperty(\u0026#34;shutdownWaitTime\u0026#34;, \u0026#34;3000\u0026#34;)); int delayOtherHooksExecTime = Integer.parseInt(props.getProperty(\u0026#34;delayOtherHooksExecTime\u0026#34;, \u0026#34;2000\u0026#34;)); Class\u0026lt;?\u0026gt; clazz = Class.forName(props.getProperty(\u0026#34;applicationShutdownHooksClassName\u0026#34;, \u0026#34;java.lang.ApplicationShutdownHooks\u0026#34;)); Field field = clazz.getDeclaredField(props.getProperty(\u0026#34;applicationShutdownHooksFieldName\u0026#34;, \u0026#34;hooks\u0026#34;)); field.setAccessible(true); IdentityHashMap\u0026lt;Thread, Thread\u0026gt; hooks = (IdentityHashMap\u0026lt;Thread, Thread\u0026gt;) field.get(clazz); long s = System.currentTimeMillis(); while (System.currentTimeMillis() - s \u0026lt; delayOtherHooksExecTime) { for (Iterator\u0026lt;Thread\u0026gt; iterator = hooks.keySet().iterator(); iterator.hasNext();) { Thread hook = iterator.next(); if (hook.getName().startsWith(hookNamePrefix)) {// 为当前自己自定义的hook 就不做延时  continue; } if (delayHooks.containsKey(hook) || delayedHooks.containsKey(hook)) { continue; } Thread delayHook = new Thread(() -\u0026gt; {// 通过线程包装原先的hook线程任务，并在其中加3s的延时  log.info(\u0026#34;sleep {}ms\u0026#34;, shutdownWaitTime); try { TimeUnit.MILLISECONDS.sleep(shutdownWaitTime); } catch (InterruptedException ex) { ex.printStackTrace(); } hook.run(); }, hook.getName()); delayHooks.put(delayHook, delayHook);// 加入的待加入延时集合  iterator.remove();// 从原来的ApplicationShutdownHooks的hooks集合中去掉  } for (Iterator\u0026lt;Thread\u0026gt; iterator = delayHooks.keySet().iterator(); iterator.hasNext();) { Thread delayHook = iterator.next(); Runtime.getRuntime().addShutdownHook(delayHook);// 加入到ApplicationShutdownHooks的hooks集合中去掉  delayedHooks.put(delayHook, delayHook); iterator.remove(); log.info(\u0026#34;hook {} will sleep {}ms when it start\u0026#34;, delayHook.getName(), shutdownWaitTime); } TimeUnit.MILLISECONDS.sleep(100); } hookNamePrefix = null; hookId = null; props = null; delayHooks = null; delayedHooks = null; } } } ","date":"2021-05-19T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/%E4%BC%98%E9%9B%85%E5%81%9C%E6%9C%BA/","title":"优雅停机"},{"content":"jvm垃圾回收 对象存活判断算法 引用计数算法 问题： 可能存在两个对象相互引用，导致无法回收，内存泄漏\n根可达算法 维护引用的链表； 如果某个对象无法到达根节点，说明可以被回收\n垃圾回收算法  对停顿时间(延时)和吞吐量的权衡，没有最好，只有最适合当前业务场景的回收方式\n   针对 堆内存回收；\n  方法区\n java 8 前 永久区（也参与垃圾回收，一样的算法，省事了，但是有默认最大内存限制，容易oom） java 8 彻底抛弃了 永久区，叫元数据区，使用本地内存    分代收集理论 新生代\n老年代\n跨代引用： Remember set\n标记清除 产生内存碎片，内存分配复杂了。 可能需要类似硬盘的 “分区空闲分配链表” 等复杂方式解决\nCms搜集器在old 区回收时采用， 但是内存碎片达到一定量，会采取一次标记整理。（和稀泥的做法，结合两者，）\n标记复制 一般用于新生代回收: Serial ParNew 的新生代采用该算法\nscurvivorRadio Ēden survivor survivor 8:1:1\n对象存活率较高时，需要更多的复制操作，效率会降低\n标记整理 用于old区：\n相对于 标记清除， 标记后，需要移动：将存活的对象移动到内存区域的一端。\n  移动：增大的延迟，stw时间长些，但解决了内存碎片，内存分配复杂的问题，可以提高吞吐量。\n  不移动：降低了延迟，但内存碎片，内存分配复杂， 吞吐量有所下降。\n  垃圾回收器   对于新生代一般时使用标记复制算法\n  对于老年代一般使用标记整理算法 （CMS除外）\n  Serial收集器  串行  ParNew收集器  并行  Parallel Scavenge收集器 -XX:+UseParallelGC -XX:+UseParallelOldGC\n  调整方式\n 1.手动调整 -Xmn -Xms -XX:NewRatio=N 手动指定堆内存大小和代空间比例，一般要多次试验 2.自动参数调整 -XX:MaxGCPauseMillis=N 可接受最大停顿时间 -XX:GCTimeRatio=N 可接受GC时间占比（目标吞吐量） 吞吐量=1-1/(1+N)\n   并行\n  尽可能可以控制吞吐量 也叫 吞吐量优先收集器\n  -XX:+UseAdaptiveSizePolicy 默认开启的\nSerial Old收集器 用于old区：串行\nParallel Old收集器 用于old区：并行\nCMS收集器 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC\n  尽可能缩短垃圾回收时，用户线程的停顿时间\n  和ParNew收集器配合：\n  Garbage First (G1)收集器 内存分配与回收策略   对象优先在Edem区分配\n  大对象直接进入老年代\n  长期存活的对象进入老年代\n   每个对象有个年龄计数器 MinorGC一次，加1\n  -XX:MaxTenuringThreshold=X X默认是15， 15次后进入老年代\n     动态对象年龄判断\n-XX:TargetSurvivorRatio 目标存活率，默认为50%\nsurvivor中相同年龄的对象大小之和大于TargetSurvivorRatio后，大于等于该年龄的都晋升到老年代。\n  空间分配担保\n  相关应用 查看当前java版本使用的GC java -XX:+PrintCommandLineFlags -version\n[root@sd01 ~]# java -XX:+PrintCommandLineFlags -version -XX:InitialHeapSize=128167296 -XX:MaxHeapSize=2050676736 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC java version \u0026#34;1.8.0_131\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_131-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)  UseParallelGC 按道理时使用 Parallel Scavenge + SerialOld，但后来P araller old出现后，实际默认时用的Parallel Scavenge + Parallel Old\n  默认堆大小 什么是jdk1.8默认堆大小？ （MaxHeapSize）\n就是使用java -jar 并且没有手动指定-Xmx参数的启动的进程使用的堆的大小，就是用的默认jdk堆的大小。\n这个默认的堆大小是取决你服务器的物理内存，假如服务器内存大于1GB，则使用1/4的服务器物理内存作为jvm的堆内存大小。\neg：服务器内存4GB 则默认堆大小为1GB\njava -XX:+PrintFlagsInitial 配置试例 java -Xms1600m -Xmx1600m -Xmn800m -Xloggc:/xxx/logs/xxx_gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=20 -XX:GCLogFileSize=100M -Dprofile=prod -jar /xxx/xxx-1.0.0-SNAPSHOT.jar  -Xms: //堆内存初始化大小 -Xmx: //堆最大可用内存 -Xmn: //新生代堆最大可用内存 -XX:UseParallelGC //使用gc的类型\n-XX:SurvivorRadio//设置新生代中：Eden空间、SurvivorRadio From空间、SurvivorRadio To空间的占比 -XX:NewRatio //设置新生代空间和老年代空间的占比 -XX:+PrintGC //每次触发GC的时候打印相关日志 -XX:+PrintGCDetails //打印详细Gc日志\n-XX:+UseGCLogFileRotation\n-XX:NumberOfGCLogFiles\n-XX:GCLogFileSize\n ","date":"2021-03-14T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/jvm%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","title":"jvm垃圾回收"},{"content":"相关概念  计算存储分离   安装测试 # 下载 wget https://mirrors.bfsu.edu.cn/apache/pulsar/pulsar-2.7.1/apache-pulsar-2.7.1-bin.tar.gz tar xvfz apache-pulsar-2.7.1-bin.tar.gz cd apache-pulsar-2.7.1 # 单机启动 bin/pulsar standalone # 消费消息 bin/pulsar-client consume my-topic -s \u0026#34;first-subscription\u0026#34; # 生产消息 bin/pulsar-client produce my-topic --messages \u0026#34;hello-pulsar\u0026#34; 相关资料 官网快速启动\n","date":"2021-02-23T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/pulsar/","title":"pulsar"},{"content":"相关概念 二代mq, 纯java开发，和kafka无本质区别\n安装测试 # 下载 4.8.0 wget https://downloads.apache.org/rocketmq/4.8.0/rocketmq-all-4.8.0-bin-release.zip unzip rocketmq-all-4.8.0-bin-release.zip #运行命名服务， 替代kafka的zk nohup sh bin/mqnamesrv \u0026amp; #查看日志 tail -f ~/logs/rocketmqlogs/namesrv.log #运行broker nohup sh bin/mqbroker -n localhost:9876 \u0026amp; #查看日志 tail -f ~/logs/rocketmqlogs/broker.log # 发送消息 export NAMESRV_ADDR=localhost:9876 sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer #消费消息 sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer 相关资料 官网快速开始\n中文文档\n","date":"2021-02-20T09:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/rocketmq/","title":"rocketMQ"},{"content":"Rabbitmq相关概念 一代mq，erlang开发， 改进activemq\n Publisher 消息生产者, 返送消息时指定exchange 和routing key, 即可以将消息路由到匹配的queue中 Routing key Binding 通过routing key 将queue和exchange绑定 Exchange 工具人。。交易所。。代理  FanoutExchange: 将消息分发到所有的绑定队列，无routingkey的概念，发送时不指定routing key HeadersExchange ：通过添加属性key-value匹配 DirectExchange: 按照routingkey分发到指定队列 TopicExchange:多关键字匹配 正则   Consumer  Docker方式安装运行 docker pull rabbitmq:management docker run -itd --name rabbitmq-test -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin -p 15672:15672 -p 5672:5672 rabbitmq:management docker exec -it rabbitmq-test /bin/bash ","date":"2021-02-19T10:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/rabbitmq/","title":"rabbitMQ"},{"content":"康威定律：组织决定产品形态 第一定律 组织设计的产品/设计等价于这个组织的沟通结构。\n第二定律 时间再多一件事情也不可能做的完美，但总有时间做完一件事情\n第三定律 线型系统和线型组织架构间有潜在的异质同态特性\n第四定律 大的系统组织总是比小系统更倾向于分解\n","date":"2021-01-13T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E5%BA%B7%E5%A8%81%E5%AE%9A%E5%BE%8B/","title":"康威定律"},{"content":"Synchronized  Object.wait()：释放当前对象锁，并进入阻塞队列(wait set) Object.notify()：唤醒当前对象阻塞队列(wait set)里的任一线程（并不保证唤醒哪一个） Object.notifyAll()：唤醒当前对象阻塞队列(wait set)里的所有线程, 进到entry set 去竞争锁  为什么wait,notify和notifyAll要与synchronized一起使用？ Wait 只有通过synchronized拿到锁，才能进入wait set\nnotify notifyAll只有通过synchronized拿到锁，才能去唤醒 wait set 里线程 到entry set\nobject monitor 对象在内存中的存储 Markword 32位jvm 结构如下： 重量级锁即为 Synchronized 的锁\n锁升级 参考 https://mp.weixin.qq.com/s/2yxexZUr5MWdMZ02GCSwdA\n","date":"2020-12-15T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/synchronized%E9%94%81/","title":"Synchronized锁"},{"content":"安装 docker docker pull hazelcast/hazelcast docker run -e HZ_NETWORK_PUBLICADDRESS=192.168.3.14:5701 -p 5701:5701 hazelcast/hazelcast:$HAZELCAST_VERSION docker run -e HZ_NETWORK_PUBLICADDRESS=192.168.3.14:5702 -p 5702:5701 hazelcast/hazelcast:$HAZELCAST_VERSION docker run -p 8080:8080 hazelcast/management-center ","date":"2020-12-14T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/hazelcast/","title":"hazelcast"},{"content":"一.业务数据缓存* 经典用法。\n  通用数据缓存，string，int，list，map等。\n 验证码等    实时热数据，最新500条数据。\n 如热搜新闻。。    会话缓存，token缓存等。\n spring-session-data-redis sesion共享    二.业务数据处理   非严格一致性要求的数据\n  评论，点击，点赞等。\nset key 0 incr key // incr readcount::{帖子id} 每阅读一次 get key // get readcount::{帖子id} 获取阅读量     业务数据去重\n 订单处理的幂等校验等。 如订单id放到redis 的set中去重复， bitmap 等    业务数据排序\n 排名，排行榜等。 使用sortedset    三.全局一致计数 *   全局流控计数\n多个服务节点使用同一个redis的计数。\n  秒杀的库存计算\n和全局计数类似\n  抢红包\n和全局计数类似\n  全局ID生成\n例如：userId, 直接获取一段userId的最大值，缓存到本地服务慢慢累加，快到了userId的最大值时，再去获取一段，一个用户服务宕机了，也就一小段userId没有用到。 用数据库也可以。\nset userId 0 incr usrId //返回1 incrby userId 1000 //返回10001   四.高效统计计数   id去重，记录访问ip等\n全局bitmap操作\n  UV、PV等访问量==\u0026gt;非严格一致性要求\n  五.发布订阅与Stream   Pub-Sub 模拟队列\n127.0.0.1:6379\u0026gt; subscribe fzb Reading messages... (press Ctrl-C to quit) 1) \u0026quot;subscribe\u0026quot; 2) \u0026quot;fzb\u0026quot; 3) (integer) 1 1) \u0026quot;message\u0026quot; 2) \u0026quot;fzb\u0026quot; 3) \u0026quot;fff\u0026quot; 1) \u0026quot;message\u0026quot; 2) \u0026quot;fzb\u0026quot; 3) \u0026quot;ffff\u0026quot; 127.0.0.1:6379\u0026gt; publish fzb fff (integer) 1 127.0.0.1:6379\u0026gt; publish fzb ffff (integer) 1   Redis Stream 是 Redis 5.0 版本新增加的数据结构。 Redis Stream 主要用于消息队列(MQ，Message Queue)。\n具体可以参考 https://www.runoob.com/redis/redis-stream.html\n  六.分布式锁* 1、获取锁\u0026ndash;单个原子性操作\n SET dlock my_random_value NX PX 30000  127.0.0.1:6379\u0026gt; set myLock 1 NX PX 30000 OK 127.0.0.1:6379\u0026gt; set myLock 1 NX PX 30000 (nil) 2、释放锁\u0026ndash;lua脚本-保证原子性+单线程，从而具有事务性 . =\u0026gt; 因为内存操作是单线程的\nif redis.call(\u0026quot;get\u0026quot;,KEYS[1]) == ARGV[1] then return redis.call(\u0026quot;del\u0026quot;,KEYS[1]) else return 0 end  关键点:原子性、互斥、超时  更多细节：https://www.cnblogs.com/yunlongaimeng/p/10266690.html\n","date":"2020-12-13T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/redis%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/","title":"redis应用场景"},{"content":"redis线程 redis做为一个进程，一直是多线程的。处理io和处理内存的是不同线程。\n  io线程\n  redis6之前(2020.05)：io处理是单线程\n  redis6：io处理多线程，采用nio模型 =\u0026gt; 主要的性能提升点\n    内存处理线程\n 单线程 =\u0026gt;高性能核心，不用考虑线程调度    压测redis-benchmark  环境mac 4核8g  mokernetdeMac-mini:redis-6.0.9 mokernet$ ./bin/redis-benchmark -n 100000 -c 32 -t SET,GET,INCR,HSET,LPUSH,MSET -q SET: 121065.38 requests per second GET: 118764.84 requests per second INCR: 117508.81 requests per second LPUSH: 123001.23 requests per second HSET: 123915.74 requests per second MSET (10 keys): 96711.80 requests per second redis的5种基本数据结构 https://redis.io/commands\n1.字符串(string) 简单来说就是三种:int、string、byte[]\nRedis中字符串类型的value最多可以容纳的数据长度是512M\nset/get/setnx/getset/del/exists/append incr/decr/incrby/decrby 2.散列(hash) Redis中的Hash类型可以看成具有String key 和String value的map容器。\nhset/hget/hmset/hmget/hgetall/hdel/hincrby hexists/hlen/hkeys/hvals  hmset 相对于hset可一次设置多个键值对 hmget 相对于hget可一次获取多个键的值  3.列表(list) java的LinkedList\n在Redis中，List类型是按照插入顺序排序的字符串链表。和数据结构中的普通链表 一 样，我们可以在其头部(Left)和尾部(Right)添加新的元素。在插入时，如果该键并不存 在，Redis将为该键创建一个新的链表。与此相反，如果链表中所有的元素均被移除， 那么该键也将会被从数据库中删除。\nlpush/rpush/lrange/lpop/rpop 4.集合(set) java的set，不重复的list\n在redis中，可以将Set类型看作是没有排序的字符集合，和List类型一样，我们也可以 在该类型的数值上执行添加、删除和判断某一元素是否存在等操作。这些操作的时间复 杂度为O(1),即常量时间内完成依次操作。\n和List类型不同的是，Set集合中不允许出现重复的元素。\nsadd/srem/smembers/sismember 类比java中set的add, remove, all, contains, spop key [count] 随机返回集合中一个或多个 移除 SRANDMEMBER key [count] 返回集合中一个或多个随机数，不移除, 抽奖 sdiff/sinter/sunion 集合求差集，求交集，求并集 5.有序集合(sorted set) 按权重排序\nsortedset在set基础上给每个元素加了个分数score。\nredis 正是通过分数来为集合的成员进行从小到大的排序。sortedset中分数是可以重复的。\nzadd key score member score2 member2... : 将成员以及该成员的分数存放到sortedset中 zscore key member : 返回指定成员的分数 zcard key : 获取集合中成员数量 zrem key member [member...] : 移除集合中指定的成员，可以指定多个成员 zrange key start end [withscores] : 获取集合中脚注为start-end的成员，[withscores]参数表明返回的成员 包含其分数 zrevrange key start stop [withscores] : 按照分数从大到小的顺序返回索引从start到stop之间的所有元素 (包含两端的元素) zremrangebyrank key start stop : 按照排名范围删除元素 redis高级数据结构 1.Bitmaps bitmaps不是一个真实的数据结构。而是String类型上的一组面向bit操作的集合。由于 strings是二进制安全的blob，并且它们的最大长度是512m，所以bitmaps能最大设置 2^32个不同的bit。\nsetbit/getbit/bitop/bitcount/bitpos 可用作集中式的冥等去重 数据压缩在字节位上，极大的节约了空间\n2.HyperLogLog Redis 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。\n在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。\n但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。\npfadd 添加 pfcount 获得基数值 pfmerge 合并多个key 127.0.0.1:6379\u0026gt; pfadd pf1 1 3 1 4 1 5 (integer) 1 127.0.0.1:6379\u0026gt; pfcount pf1 (integer) 4 127.0.0.1:6379\u0026gt; pfadd pf2 3 4 5 6 7 (integer) 1 127.0.0.1:6379\u0026gt; pfcount pf2 (integer) 5 127.0.0.1:6379\u0026gt; pfmerge pf3 pf1 pf2 OK 127.0.0.1:6379\u0026gt; pfcount pf3 (integer) 6 应用场景说明：\n 基数不大，数据量不大就用不上，会有点大材小用浪费空间 有局限性，就是只能统计基数数量，而没办法去知道具体的内容是什么 和bitmap相比，属于两种特定统计情况，简单来说，HyperLogLog 去重比 bitmap 方便很多 一般可以bitmap和hyperloglog配合使用，bitmap标识哪些用户活跃，hyperloglog计数  一般使用：\n 统计注册 IP 数 统计每日访问 IP 数 统计页面实时 UV 数 统计在线用户数 统计用户每天搜索不同词条的个数  3.GEO geoadd/geohash/geopos/geodist/georadius/georadiusbymember Redis的GEO特性在 Redis3.2版本中推出，这个功能可以将用户给定的地理位置(经 度和纬度)信息储存起来，并对这些信息进行操作。\n4.Redis 中的布隆过滤器 Redis v4.0 之后有了 Module（模块/插件）,RedisBloom 作为 Redis 布隆过滤器的 Module，地址：https://github.com/RedisBloom/RedisBloom\nRedis Lua   类比openrestry = nginx + lua jit\n  类似于数据库的存储过程，mongodb的js脚本\n  redis内存操作的单线程 使得一段lua脚本之心具有：原子性，操作不会被打断，保证了事务\n  直接执行 eval \u0026ldquo;return\u0026rsquo;hello java'\u0026rdquo; 0 eval \u0026ldquo;redis.call(\u0026lsquo;set\u0026rsquo;,KEYS[1],ARGV[1])\u0026rdquo; 1 lua-key lua-value\n127.0.0.1:6379\u0026gt; eval \u0026quot;return'hello java'\u0026quot; 0 \u0026quot;hello java\u0026quot; 127.0.0.1:6379\u0026gt; eval \u0026quot;redis.call('set',KEYS[1],ARGV[1])\u0026quot; 1 ff zz (nil) 127.0.0.1:6379\u0026gt; get ff \u0026quot;zz\u0026quot;   预编译 script load script脚本片段 返回一个SHA-1签名 shastring evalsha shastring keynum [key1 key2 key3 \u0026hellip;] [param1 param2 param3 \u0026hellip;]\n127.0.0.1:6379\u0026gt; script load \u0026quot;redis.call('set',KEYS[1],ARGV[1])\u0026quot; \u0026quot;7cfb4342127e7ab3d63ac05e0d3615fd50b45b06\u0026quot; 127.0.0.1:6379\u0026gt; evalsha 7cfb4342127e7ab3d63ac05e0d3615fd50b45b06 1 fff zzz (nil) 127.0.0.1:6379\u0026gt; get fff \u0026quot;zzz\u0026quot;     redis pipeline 使用管道一次执行多个命令，多个命令的结果一起返回，减少每个命令来回建立链接，来回响应的时间。\nhttps://redis.io/topics/pipelining\nredis事务   开启事务:multi\n  命令入队\n  执行事务:exec\n  撤销事务:discard\n  Watch 监控事务： watch 一个key，发生变化则事务失败\n  unwatch 取消监听\n  127.0.0.1:6379\u0026gt; multi OK 127.0.0.1:6379\u0026gt; set ff ff QUEUED 127.0.0.1:6379\u0026gt; set fff fff QUEUED 127.0.0.1:6379\u0026gt; exec 1) OK 2) OK redis管道 redis备份恢复机制 RDB方式   快照恢复，类似mysql的frm.等数据：备份当前瞬间 Redis 在内存中的数据记录\n  save后在数据目录生成dump.rdb\n  bgsave 异步执行备份\n  恢复：将备份文件 (dump.rdb) 移动到 redis 数据目录并启动服务即可\n  redis.conf中\n#Redis默认配置文件中提供了三个备份条件： ##指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 save 900 1 save 300 10 save 60 10000 AOF方式  追加文件（Append-Only File，AOF） 类比mysql的binlog 配置为 always，其含义为当 Redis 执行 命令的时候，则同时同步到 AOF 文件，这样会使得 Redis 同步刷新 AOF 文件，造成 缓慢。而采用 evarysec 则代表每秒同步一次命令到 AOF 文件  appendfilename \u0026quot;appendonly.aof\u0026quot; # appendfsync always appendfsync everysec # appendfsync no......  恢复：自动加载  redis性能优化 slowlog get 10 几个缓存问题 缓存穿透  现象：key对应的数据在数据源并不存在，每次针对此key的请求从缓存获取不到，请求都会到数据源，从而可能压垮数据源。比如用一个不存在的用户id获取用户信息，不论缓存还是数据库都没有，若黑客利用此漏洞进行攻击可能压垮数据库 解决：  采用布隆过滤器：将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力，详见https://www.cnblogs.com/rinack/p/9712477.html 直接缓存空结果：如果一个查询返回的数据为空，仍然把这个空结果进行缓存。    缓存击穿  现象：key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 解决：使用互斥分布式锁进行从DB加载数据，其他的请求继续重试缓存。使用锁可能会死锁、线程池阻塞等问题，针对高热点key，最好是在并发量最小的时候，写定时器更新key的过期时间  缓存雪崩  现象：当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，也会给后端系统(比如DB)带来很大压力。与缓存击穿的区别在于这里针对很多key缓存，前者则是某一个key。 解决：  最简单尽量将过期时间分散开来：可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机， 设置过期标志更新缓存:  1，新加一个缓存key的标记。缓存数据key的value时，同时缓存key_sign, key_sign的过期时间小于key的过期时间。 2 在查询缓存时，先判断key_sign是否过期，a，如果过期，先直接返回value，再启用异步线程去更新key_sign以及加载db到key的value,重新设置两个的过期时间。b. 没有过期，直接返回value      ","date":"2020-12-11T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/redis%E5%9F%BA%E7%A1%80/","title":"redis基础"},{"content":"gateway整体逻辑 1.流程图 2.几个关键的类  org.springframework.web.reactive.DispatcherHandler: 请求分发处理器；    Spring WebFlux 的访问入口； 类似于spring mvc DispatcherServlet,可以类比spring mvc 接收到请求; DispatcherHandler匹配 HandlerMapping此处会匹配到scg的RoutePredicateHandlerMapping     org.springframework.cloud.gateway.handler.RoutePredicateHandlerMapping：HandlerMapping的实现；\n  通过RouteLocator匹配 Route: getHandlerInternal方法调用lookupRoute()方法，通过routeLocator获取所有配置的route,通过里面的Predicate配置来遍历判断找出符合的Route getHandlerInternal中返回FilteringWebHandler     org.springframework.cloud.gateway.handler.FilteringWebHandler: WebHandler的实现；\n  FilteringWebHandler被RoutePredicateHandlerMapping返回后，在DispatcherHandler中被SimpleHandlerAdapter执行handle方法； 责任链模式：获取Route的GatewayFilter数组，创建DefaultGatewayFilterChain的过滤链；链式调用GatewayFilter     3.项目结构 核心module为spring-cloud-gateway-server\n actuate: 实现springboot actuator的端点，暴露route filter predicate等信息 config: 使用springboot的配置注解的各类配置类 discover：通过注册中心获取路由Route的核心功能配置类及实现类 event：实现ApplicationEvent的事件类，例如路由刷新事件RefreshRoutesEvent filter: 包含特定路由的GatewayFilterFactory，GatewayFiler以及全局的GlobalFilter handler: 包含匹配route的断言工厂AbstractRoutePredicateFactory的所有默认实现，以及核心类FilteringWebHandler及RoutePredicateHandlerMapping route：路由的定义类，及路由定位类CachingRouteLocator的所有实现，及路由定义定位类CompositeRouteDefinitionLocator的所有实现，路由存储接口RouteDefinitionRepository及其所有实现 support：工具类；如HTTP协议处理，组件名处理，日期转换等  ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/01.gateway%E6%95%B4%E4%BD%93%E9%80%BB%E8%BE%91/","title":"01.gateway整体逻辑"},{"content":"reactor 使用 ![image (1)](https://gitee.com/fengzhenbing/picgo/raw/master/image (1).png)\nWebflux 模块的名称是 spring-webflux，名称中的 Flux 来源于 Reactor 中的类 Flux。 Reactor 两个核心概念做一些澄清，一个是Mono，另一个是Flux\n Flux ：表示的是包含 0 到 N 个元素的异步序列。包含三个类型    正常的包含元素的消息 序列结束的消息 序列出错的消息    Mono： 表示的是包含 0 或者 1 个元素的异步序列。该序列中同样可以包含与 Flux 相同的三种类型的消息通知。 示例代码：    https://github.com/fengzhenbing/spring-cloud-gateway-demo/blob/master/demo-gateway/src/main/java/org/fzb/demo/gateway/RectorController.java   ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/02.reactor%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0/","title":"02.reactor响应式编程学习"},{"content":"scg NettyWebServer启动过程 spring cloud gateway(下面简称scg)依赖spring webflux, 而spring webflux依赖于reactor-netty,也就是scg启动过程中最终会启动netty做为服务器。 springboot中定义一下几种服务器：\n1 启动ReactiveWebServerApplicationContext 从springboot启动开始分析\nSpringApplication.run(GatewayApplication.class, args); 设置webApplicationType的值：REACTIVE还是servlet的。\npublic SpringApplication(ResourceLoader resourceLoader, Class\u0026lt;?\u0026gt;... primarySources) { this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \u0026#34;PrimarySources must not be null\u0026#34;); this.primarySources = new LinkedHashSet\u0026lt;\u0026gt;(Arrays.asList(primarySources)); this.webApplicationType = WebApplicationType.deduceFromClasspath();//fzb 通过类路径中类，推测web应用类型：REACTIVE还是servlet的。 \tsetInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass(); } 再看deduceFromClasspath方法：判断DispatcherHandler存在还是DispatcherServlet存在\nstatic WebApplicationType deduceFromClasspath() {//fzb 判断DispatcherHandler存在还是DispatcherServlet存在 \tif (ClassUtils.isPresent(WEBFLUX_INDICATOR_CLASS, null) \u0026amp;\u0026amp; !ClassUtils.isPresent(WEBMVC_INDICATOR_CLASS, null) \u0026amp;\u0026amp; !ClassUtils.isPresent(JERSEY_INDICATOR_CLASS, null)) { return WebApplicationType.REACTIVE; } for (String className : SERVLET_INDICATOR_CLASSES) { if (!ClassUtils.isPresent(className, null)) { return WebApplicationType.NONE; } } return WebApplicationType.SERVLET; } 在此scg项目有DispatcherHandler类，所以是REACTIVE，响应式的。 下面接着创建相应的响应式容器\nprotected ConfigurableApplicationContext createApplicationContext() { Class\u0026lt;?\u0026gt; contextClass = this.applicationContextClass; if (contextClass == null) { try { switch (this.webApplicationType) {//fzb 根据webApplicationType类型创建不同的context容器 \tcase SERVLET: contextClass = Class.forName(DEFAULT_SERVLET_WEB_CONTEXT_CLASS); break; case REACTIVE: contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS);//fzb scg创建的容器 \tbreak; default: contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); } } catch (ClassNotFoundException ex) { throw new IllegalStateException( \u0026#34;Unable create a default ApplicationContext, please specify an ApplicationContextClass\u0026#34;, ex); } } return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass); } 各类容器选择总结\n2 ReactiveWebServerApplicationContext刷新 先创建了 org.springframework.boot.autoconfigure.web.reactive.ReactiveWebServerFactoryAutoConfiguration中配置了EmbeddedNetty，初始化了NettyReactiveWebServerFactory\nReactiveWebServerApplicationContext容器刷新时通过ReactiveWebServerFactory创建WebServerManager\nprotected void onRefresh() { super.onRefresh(); try { createWebServer();//fzb 创建WebServerManager \t} catch (Throwable ex) { throw new ApplicationContextException(\u0026#34;Unable to start reactive web server\u0026#34;, ex); } } private void createWebServer() {//fzb 通过ReactiveWebServerFactory创建WebServerManager \tWebServerManager serverManager = this.serverManager; if (serverManager == null) { String webServerFactoryBeanName = getWebServerFactoryBeanName(); ReactiveWebServerFactory webServerFactory = getWebServerFactory(webServerFactoryBeanName);//fzb 获取EmbeddedNetty配置的ReactiveWebServerFactory \tboolean lazyInit = getBeanFactory().getBeanDefinition(webServerFactoryBeanName).isLazyInit(); this.serverManager = new WebServerManager(this, webServerFactory, this::getHttpHandler, lazyInit);//fzb 创建 \tgetBeanFactory().registerSingleton(\u0026#34;webServerGracefulShutdown\u0026#34;, new WebServerGracefulShutdownLifecycle(this.serverManager)); getBeanFactory().registerSingleton(\u0026#34;webServerStartStop\u0026#34;, new WebServerStartStopLifecycle(this.serverManager)); } initPropertySources(); } 再看WebServerManager初始化时，再通过ReactiveWebServerFactory 初始化创建NettyWebServer\nWebServerManager(ReactiveWebServerApplicationContext applicationContext, ReactiveWebServerFactory factory, Supplier\u0026lt;HttpHandler\u0026gt; handlerSupplier, boolean lazyInit) { this.applicationContext = applicationContext; Assert.notNull(factory, \u0026#34;Factory must not be null\u0026#34;); this.handler = new DelayedInitializationHttpHandler(handlerSupplier, lazyInit); this.webServer = factory.getWebServer(this.handler);//fzb scg中创建NettyWebServer \t} 下面是ReactiveWebServerFactory初始化创建NettyWebServer的过程，实际是创建reactor.netty 的HttpServer，通过适配器模式ReactorHttpHandlerAdapter，适配为NettyWebServer 返回\npublic WebServer getWebServer(HttpHandler httpHandler) { HttpServer httpServer = createHttpServer();//fzb 创建reactor.netty 的HttpServer \tReactorHttpHandlerAdapter handlerAdapter = new ReactorHttpHandlerAdapter(httpHandler);//通过ReactorHttpHandlerAdapter适配器模式适配 \tNettyWebServer webServer = new NettyWebServer(httpServer, handlerAdapter, this.lifecycleTimeout, getShutdown());//HttpServer 适配为NettyWebServer \twebServer.setRouteProviders(this.routeProviders); return webServer; } 3 Netty的ServerBootstrap启动 继续跟进createHttpServer()\nprivate HttpServer createHttpServer() { HttpServer server = HttpServer.create();//fzb 创建httpServer  .... } 跟进HttpServer.create()，如下为\npublic static HttpServer create() { return HttpServerBind.INSTANCE; } 查看HttpServerBind初始化过程，实际最终创建的是TcpServer\nstatic final TcpServer DEFAULT_TCP_SERVER = TcpServer.create();// 创建TcpServer  HttpServerBind() { this(DEFAULT_TCP_SERVER); } 再跟进TcpServer\npublic static TcpServer create() { return TcpServerBind.INSTANCE; } 千呼万唤始出来 TcpServerBind,\nTcpServerBind() { this.serverBootstrap = createServerBootstrap();//创建启动Netty的服务端serverBootstrap \tBootstrapHandlers.channelOperationFactory(this.serverBootstrap, TcpUtils.TCP_OPS); } 至此找到了scg启动时最终启动的netty server.\nHttpServer为reactor netty项目中的 参考 Reactor Netty参考指南 https://projectreactor.io/docs/netty/release/reference/index.html\n","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/03.scg-nettywebserver%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B/","title":"03.scg NettyWebServer启动过程"},{"content":"一次请求的执行过程 ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/04.scg-%E4%B8%80%E6%AC%A1%E8%AF%B7%E6%B1%82%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/","title":"04.scg 一次请求的执行过程"},{"content":"route路由的配置加载  主要在sprng-cloud-gateway-server的route包定义路由相关的定义，构建和加载\n![路由](https://gitee.com/fengzhenbing/picgo/raw/master/image (2).png)\n0.相关配置  通过springboot spi方式，springboot会启动spring.factories中配置的 org.springframework.cloud.gateway.discovery.GatewayDiscoveryClientAutoConfiguration  org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ org.springframework.cloud.gateway.config.GatewayAutoConfiguration,\\ org.springframework.cloud.gateway.discovery.GatewayDiscoveryClientAutoConfiguration,\\ ... 1.路由定义  路由定义RouteDefinition  public class RouteDefinition { private String id; @NotEmpty @Valid//fzb 断言定义数组  private List\u0026lt;PredicateDefinition\u0026gt; predicates = new ArrayList\u0026lt;\u0026gt;(); @Valid//fzb 过滤器定义数组  private List\u0026lt;FilterDefinition\u0026gt; filters = new ArrayList\u0026lt;\u0026gt;(); @NotNull//fzb 路由路径  private URI uri; ... }  路由定义定位器  获取路由的定义，负责读取上述路由定义配置 RouteDefinition，最终会通过路由定义生成路由\npublic interface RouteDefinitionLocator { //fzb 获取路由定义对象  Flux\u0026lt;RouteDefinition\u0026gt; getRouteDefinitions(); } 有以下实现：\n![image (3)](https://gitee.com/fengzhenbing/picgo/raw/master/image (3).png)\n  CachingRouteDefinitionLocator: 它包装了CompositeRouteDefinitionLocator,缓存路由定义RouteDefinition列表   public class CachingRouteDefinitionLocator implements RouteDefinitionLocator, ApplicationListener{//fzb 事件监听RefreshRoutesEvent\nprivate static final String CACHE_KEY = \u0026quot;routeDefs\u0026quot;; private final RouteDefinitionLocator delegate; //fzb 路由定义flux private final Flux\u0026lt;RouteDefinition\u0026gt; routeDefinitions; //fzb 内存缓存RouteDefinition private final Map\u0026lt;String, List\u0026gt; cache = new ConcurrentHashMap\u0026lt;\u0026gt;(); ... //fzb 监听到路由刷新事件，重新获取路由并缓存到cache @Override public void onApplicationEvent(RefreshRoutesEvent event) { fetch().materialize().collect(Collectors.toList()) .doOnNext(routes -\u0026gt; cache.put(CACHE_KEY, routes)).subscribe(); }  \u0026hellip; }\n \u0026gt; * CompositeRouteDefinitionLocator: \u0026gt; 遍历执行所有RouteDefinitionLocator的查找路由定义方法,将找到的路由定义合并； 组合多种 RouteDefinitionLocator 的实现，为 routeDefinitions提供统一入口 ​```java public class CompositeRouteDefinitionLocator implements RouteDefinitionLocator { ... //fzb 组合多个路由定义定位器RouteDefinitionLocator private final Flux\u0026lt;RouteDefinitionLocator\u0026gt; delegates; // fzb 路由定义的id 生成器，默认UUID private final IdGenerator idGenerator; ... public CompositeRouteDefinitionLocator(Flux\u0026lt;RouteDefinitionLocator\u0026gt; delegates, IdGenerator idGenerator) { this.delegates = delegates; this.idGenerator = idGenerator;// 路由定义的id 生成器，默认UUID } @Override public Flux\u0026lt;RouteDefinition\u0026gt; getRouteDefinitions() { //遍历执行所有RouteDefinitionLocator的查找路由定义方法,将找到的路由定义合并 return this.delegates .flatMapSequential(RouteDefinitionLocator::getRouteDefinitions) .flatMap(routeDefinition -\u0026gt; { if (routeDefinition.getId() == null) { return randomId().map(id -\u0026gt; { routeDefinition.setId(id);// 设置uuid if (log.isDebugEnabled()) { log.debug( \u0026quot;Id set on route definition: \u0026quot; + routeDefinition); } return routeDefinition; }); } return Mono.just(routeDefinition); }); } ... }   PropertiesRouteDefinitionLocator: 从配置文件(GatewayProperties 例如，YML / Properties 等 ) 读取RouteDefinition   //fzb 从GatewayProperties读取路由定义  @Override public Flux\u0026lt;RouteDefinition\u0026gt; getRouteDefinitions() { return Flux.fromIterable(this.properties.getRoutes()); }   DiscoveryClientRouteDefinitionLocator 从注册中心如：Netflix Eureka， Consul 或 Zookeeper读取RouteDefinition   public DiscoveryClientRouteDefinitionLocator(ReactiveDiscoveryClient discoveryClient, DiscoveryLocatorProperties properties) { this(discoveryClient.getClass().getSimpleName(), properties); //fzb 通过服务发现客户端 ，比如eureka客户端从注册中心拿到所有的服务实例  serviceInstances = discoveryClient.getServices() .flatMap(service -\u0026gt; discoveryClient.getInstances(service).collectList()); } 将服务实例serviceInstances转为路由定义\n@Override public Flux\u0026lt;RouteDefinition\u0026gt; getRouteDefinitions() { SpelExpressionParser parser = new SpelExpressionParser(); Expression includeExpr = parser .parseExpression(properties.getIncludeExpression());//fzb 扩展点，可以通过properties 配置其他spel  Expression urlExpr = parser.parseExpression(properties.getUrlExpression()); ... }   RouteDefinitionRepository 从存储器(内存 / Redis / MySQL 等 )读取RouteDefinition，实现RouteDefinitionWriter接口； 提供删除和保存RouteDefinition的方法； 默认实现有：InMemoryRouteDefinitionRepository,没有RouteDefinitionRepository的实例，则默认用InMemoryRouteDefinitionRepository   public class InMemoryRouteDefinitionRepository implements RouteDefinitionRepository { private final Map\u0026lt;String, RouteDefinition\u0026gt; routes = synchronizedMap( new LinkedHashMap\u0026lt;String, RouteDefinition\u0026gt;());//fzb 内存中用 同步map 存储  @Override// fzb 保存路由到内存中 \tpublic Mono\u0026lt;Void\u0026gt; save(Mono\u0026lt;RouteDefinition\u0026gt; route) { return route.flatMap(r -\u0026gt; { if (StringUtils.isEmpty(r.getId())) { return Mono.error(new IllegalArgumentException(\u0026#34;id may not be empty\u0026#34;)); } routes.put(r.getId(), r);//保存 \treturn Mono.empty(); }); } @Override// fzb 从内存中删除路由 \tpublic Mono\u0026lt;Void\u0026gt; delete(Mono\u0026lt;String\u0026gt; routeId) { ... } @Override// fzb 获取路由定义 \tpublic Flux\u0026lt;RouteDefinition\u0026gt; getRouteDefinitions() { return Flux.fromIterable(routes.values()); } }   自定义路由存储器,redis存储：RedisRouteDefinitionRepository https://github.com/fengzhenbing/spring-cloud-gateway-demo/blob/master/demo-gateway/src/main/java/org/fzb/demo/gateway/route/RedisRouteDefinitionRepository.java   2.路由  路由Route  public class Route implements Ordered { private final String id; //fzb 路由地址  private final URI uri; //fzb 路由的优先级  private final int order; private final AsyncPredicate\u0026lt;ServerWebExchange\u0026gt; predicate; //fzb gatewayFilter列表  private final List\u0026lt;GatewayFilter\u0026gt; gatewayFilters; //fzb 元数据  private final Map\u0026lt;String, Object\u0026gt; metadata; ... }  路由定位器 RouteLocator 获取路由对象  public interface RouteLocator { //fzb 获取路由对象 \tFlux\u0026lt;Route\u0026gt; getRoutes(); } 类似RouteDefinitionLocator三个实现：\n  缓存：CachingRouteLocator： 缓存路由，查找时使用CompositeRouteLocator去查找 组合： CompositeRouteLocator：使用多个RouteLocator查找路由并合并 单个的 RouteDefinitionRouteLocator：最终使用RouteDefinitionLocator查找路由定义并转为路由对象Route 单个路由的 filters = GlobalFilter + 默认GatewayFilter + 本路由配置的GatewayFilter   public class RouteDefinitionRouteLocator implements RouteLocator, BeanFactoryAware, ApplicationEventPublisherAware { ... public RouteDefinitionRouteLocator(RouteDefinitionLocator routeDefinitionLocator, List\u0026lt;RoutePredicateFactory\u0026gt; predicates, List\u0026lt;GatewayFilterFactory\u0026gt; gatewayFilterFactories, GatewayProperties gatewayProperties, ConfigurationService configurationService) { this.routeDefinitionLocator = routeDefinitionLocator;//fzb //设置路由定义定位器 \tthis.configurationService = configurationService; initFactories(predicates);//fzb //初始化路由断言工厂 \tgatewayFilterFactories.forEach( factory -\u0026gt; this.gatewayFilterFactories.put(factory.name(), factory));//fzb //初始化网关过滤器 \tthis.gatewayProperties = gatewayProperties; } ... //fzb 获取路由对象： 先获取RouteDefinition，再转为 Route \t@Override public Flux\u0026lt;Route\u0026gt; getRoutes() { Flux\u0026lt;Route\u0026gt; routes = this.routeDefinitionLocator.getRouteDefinitions() .map(this::convertToRoute);//fzb RouteDefinition转为 Route  ... } //fzb 路由定义转为 路由 \tprivate Route convertToRoute(RouteDefinition routeDefinition) { //fzb 1,将本路由定义中各个断言 与运算 合并为一个 AsyncPredicate \tAsyncPredicate\u0026lt;ServerWebExchange\u0026gt; predicate = combinePredicates(routeDefinition); //fzb 2, 获取所有的默认的过滤器和本路由定义的过滤器 \tList\u0026lt;GatewayFilter\u0026gt; gatewayFilters = getFilters(routeDefinition); // 组装Route \treturn Route.async(routeDefinition).asyncPredicate(predicate) .replaceFilters(gatewayFilters).build(); } //fzb 过滤器定义filterDefinitions加载为过滤器GatewayFilter \t@SuppressWarnings(\u0026#34;unchecked\u0026#34;) List\u0026lt;GatewayFilter\u0026gt; loadGatewayFilters(String id, List\u0026lt;FilterDefinition\u0026gt; filterDefinitions) { ArrayList\u0026lt;GatewayFilter\u0026gt; ordered = new ArrayList\u0026lt;\u0026gt;(filterDefinitions.size()); for (int i = 0; i \u0026lt; filterDefinitions.size(); i++) { FilterDefinition definition = filterDefinitions.get(i); GatewayFilterFactory factory = this.gatewayFilterFactories .get(definition.getName()); if (factory == null) { throw new IllegalArgumentException( \u0026#34;Unable to find GatewayFilterFactory with name \u0026#34; + definition.getName()); } if (logger.isDebugEnabled()) { logger.debug(\u0026#34;RouteDefinition \u0026#34; + id + \u0026#34; applying filter \u0026#34; + definition.getArgs() + \u0026#34; to \u0026#34; + definition.getName()); } // @formatter:off \tObject configuration = this.configurationService.with(factory) .name(definition.getName()) .properties(definition.getArgs()) .eventFunction((bound, properties) -\u0026gt; new FilterArgsEvent( // TODO: why explicit cast needed or java compile fails \tRouteDefinitionRouteLocator.this, id, (Map\u0026lt;String, Object\u0026gt;) properties)) .bind(); // @formatter:on  // some filters require routeId \t// TODO: is there a better place to apply this? \tif (configuration instanceof HasRouteId) { HasRouteId hasRouteId = (HasRouteId) configuration; hasRouteId.setRouteId(id); } GatewayFilter gatewayFilter = factory.apply(configuration); if (gatewayFilter instanceof Ordered) { ordered.add(gatewayFilter); } else { ordered.add(new OrderedGatewayFilter(gatewayFilter, i + 1)); } } return ordered; } private List\u0026lt;GatewayFilter\u0026gt; getFilters(RouteDefinition routeDefinition) { List\u0026lt;GatewayFilter\u0026gt; filters = new ArrayList\u0026lt;\u0026gt;(); //fzb 1,加上默认的过滤器 \t// TODO: support option to apply defaults after route specific filters? \tif (!this.gatewayProperties.getDefaultFilters().isEmpty()) { filters.addAll(loadGatewayFilters(DEFAULT_FILTERS, new ArrayList\u0026lt;\u0026gt;(this.gatewayProperties.getDefaultFilters()))); } //fzb 2,加上本路由配置的过滤器 \tif (!routeDefinition.getFilters().isEmpty()) { filters.addAll(loadGatewayFilters(routeDefinition.getId(), new ArrayList\u0026lt;\u0026gt;(routeDefinition.getFilters()))); } //fzb 排序 按实现的Ordered接口 \tAnnotationAwareOrderComparator.sort(filters); return filters; } //fzb combinePredicates主要是对找出来的predicate进行and操作 \tprivate AsyncPredicate\u0026lt;ServerWebExchange\u0026gt; combinePredicates( RouteDefinition routeDefinition) { List\u0026lt;PredicateDefinition\u0026gt; predicates = routeDefinition.getPredicates(); if (predicates == null || predicates.isEmpty()) { // this is a very rare case, but possible, just match all \treturn AsyncPredicate.from(exchange -\u0026gt; true); } AsyncPredicate\u0026lt;ServerWebExchange\u0026gt; predicate = lookup(routeDefinition, predicates.get(0)); for (PredicateDefinition andPredicate : predicates.subList(1, predicates.size())) { AsyncPredicate\u0026lt;ServerWebExchange\u0026gt; found = lookup(routeDefinition, andPredicate); predicate = predicate.and(found);//fzb and 各个断言 与 合并为一个 AsyncPredicate \t} return predicate; } ... } 类比  Route -\u0026gt; RouteDefinition -\u0026gt;RouteDefinitionLocator -\u0026gt; xxxRouteDefinitionRepository Bean -\u0026gt; BeanDefinition -\u0026gt; BeanDefinitionRegistry -\u0026gt; DefaultListableBeanFactory#Map\u0026lt;String, BeanDefinition\u0026gt; beanDefinitionMap = new ConcurrentHashMap\u0026lt;\u0026gt;(256);  ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/05.route%E8%B7%AF%E7%94%B1%E7%9A%84%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/","title":"05.route路由的配置加载"},{"content":"route通过注册中心Eureka自动加载配置 配置  gateway及后端微服务引入注册中心客户端eureka-client  \u0026lt;!-- 引入 Spring Cloud Netflix Eureka Client 相关依赖，将 Eureka 作为注册中心的客户端，并实现对其的自动配置 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  eureka-client starter的引入，会同时引入ribbon,作为后续请求，负载均衡的实现  spring.cloud.gateway.discovery.locator.enabled设为true,启用服务发现的DiscoveryClientRouteDefinitionLocator\nspring:cloud:# Spring Cloud Gateway 配置项，对应 GatewayProperties 类gateway:discovery:locator:enabled:true# default fase，设为true开启@Configuration(proxyBeanMethods = false) @ConditionalOnProperty(value = \u0026#34;spring.cloud.discovery.reactive.enabled\u0026#34;,//fzb 默认使用响应式的方式 \tmatchIfMissing = true) public static class ReactiveDiscoveryClientRouteDefinitionLocatorConfiguration { @Bean//fzb spring.cloud.gateway.discovery.locator.enabled配为true时，才开启DiscoveryClientRouteDefinitionLocator \t@ConditionalOnProperty(name = \u0026#34;spring.cloud.gateway.discovery.locator.enabled\u0026#34;) public DiscoveryClientRouteDefinitionLocator discoveryClientRouteDefinitionLocator( ReactiveDiscoveryClient discoveryClient,//响应式的客服端 Eureka就是 EurekaReactiveDiscoveryClient \tDiscoveryLocatorProperties properties) { return new DiscoveryClientRouteDefinitionLocator(discoveryClient, properties); } }  eureka-client的引入，会开启TimedSupervisorTask执行HeartbeatThread的心跳任务， 默认每隔30s一次  RouteRefreshListener 每隔30s接收到HeartBeatEvent事件，同时会发送RefreshRoutes事件\nCachingRouteLocator 收到RefreshRoutesEvent事件，重新获取路由时会发现多了一个DiscoveryClientRouteDefinitionLocator （负责从注册中心获取新路由配置）\n验证 http://localhost:7070/ORDER/order/get 前边ORDER为eureka-client从eureka-server获取的服务名，\nLoadBalancerClientFilter在NettyRoutingFilter之前通过ribbon执行负载均衡策略，选择一个服务实例\n总结  springgateway 接收注册中心心跳事件，发送路由刷新事件， CachingRouteLocator 最终调用DiscoveryClientRouteDefinitionLocator#getRouteDefinitions 获取注册中心最新的路由  代码实现:https://github.com/fengzhenbing/spring-cloud-gateway-demo.git\n","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/06.route%E9%80%9A%E8%BF%87%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/","title":"06.route通过注册中心自动配置加载"},{"content":"precidate选择路由 ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/07.precidate%E7%9A%84%E5%AF%B9%E8%B7%AF%E7%94%B1%E8%BF%9B%E8%A1%8C%E9%80%89%E6%8B%A9/","title":"07.precidate的对路由进行选择"},{"content":"","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/08.filter%E7%9A%84%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD%E5%8F%8A%E5%90%88%E5%B9%B6/","title":"08.filter的配置加载及合并"},{"content":"ShenYu介绍 官网\n 高性能，多协议，易扩展，响应式的API Gateway     丰富的协议 支持 dubbo ，tars， springcloud grpc。     插件化 插件化设计思想，插件热插拔，易扩展。   流控 灵活的流量筛选，能满足各种流量控制。   内置插件 内置丰富的插件支持，鉴权，限流，熔断，防火墙等。   高性能 流量配置动态化，性能极高，网关消耗在 1~2ms。   集群部署 支持集群部署，支持 A/B Test，蓝绿发布。    soul项目结构   soul-admin\nsoul网关管理端，配合soul-dashbord\n  Soul-bootstrap\n网关启动工程： 实际引入soul-spring-boot-starter-gateway(soul-web)\n  Soul-client\n为下游服务提供者提供各类服务接入网关soul的客户端\n Soul-client-common Soul-client-dubbo Soul-client-grpc Soul-client-http Soul-client-sofa Soul-client-tars    Soul-common\n  Soul-dashbord\n  Soul-dist\n  Soul-example\n  Soul-metrics\n  Soul-plugin\n  Soul-register-center\n  Soul-spi\n  Soul-spring-boot-starter\n  Soul-sync-data-center\n Soul-sync-data-api Soul-sync-data-http Soul-sync-data-nacos Soul-sync-data-websocket Soul-sync-data-zookeeper    Soul-web\n  ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/soul%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84/","title":"soul整体结构"},{"content":"下载 https://redis.io/download\n编译 wget https://download.redis.io/releases/redis-6.0.10.tar.gz tar xzf redis-6.0.10.tar.gz cd redis-6.0.10 sudo make 运行 #复制配置文件及命令 mkdir ./bin mkdir ./conf sudo cp ./src/mkreleasehdr.sh ./bin sudo cp ./src/redis-benchmark ./bin sudo cp ./src/redis-check-rdb ./bin sudo cp ./src/redis-cli ./bin sudo cp ./src/redis-server ./bin sudo cp ./redis.conf ./conf #运行 ./bin/redis-server ./conf/redis.conf 配置 redis.conf\n#修改为守护模式 daemonize yes #设置进程锁文件 pidfile /usr/local/redis-4.0.11/redis.pid #端口 port 6379 #客户端超时时间 timeout 300 #日志级别 loglevel debug #日志文件位置 logfile /usr/local/redis-4.0.11/log-redis.log #设置数据库的数量，默认数据库为0，可以使用SELECT \u0026lt;dbid\u0026gt;命令在连接上指定数据库id databases 16 ##指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 #save \u0026lt;seconds\u0026gt; \u0026lt;changes\u0026gt; #Redis默认配置文件中提供了三个条件： save 900 1 save 300 10 save 60 10000 #指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间， #可以关闭该#选项，但会导致数据库文件变的巨大 rdbcompression yes #指定本地数据库文件名 dbfilename dump.rdb #指定本地数据库路径 dir /usr/local/redis-4.0.11/db/ #指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能 #会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有 #的数据会在一段时间内只存在于内存中 appendonly no #指定更新日志条件，共有3个可选值： #no：表示等操作系统进行数据缓存同步到磁盘（快） #always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全） #everysec：表示每秒同步一次（折衷，默认值） appendfsync everysec Docker #最新镜像 docker pull redis #运行 docker run -itd --name redis-test -p 6379:6379 redis docker image inspect redis:latest|grep -i version #运行 docker exec -it redis-test /bin/bash $ redis-cli \u0026gt; info   指定配置文件运行\ndocker run -p 6379:6379 --name redis-test -v /etc/redis/redis.conf:/etc/redis/redis.conf -v /etc/redis/data:/data -d redis redis-server /etc/redis/redis.conf --appendonly yes   停止\ndocker stop redis-test   ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/%E5%AE%89%E8%A3%85redis/","title":"安装redis"},{"content":"通过Hugo搭建静态博客网站，再通过github pages部署运行\nHugo介绍  Hugo是一种用Go语言编写的快速，现代的静态网站生成器，旨在让网站创建再次变得有趣。 性能高，安全性和易用性是主要特点 拥有超快的速度，强大的内容管理和强大的模板语言，使其非常适合各种静态网站。  Hugo安装 # mac上安装 brew install hugo # windows可通过Chocolatey上安装 choco install hugo -confirm # 版本验证 hugo version hugo主题  查找你喜欢的主题 在此我选择的主题为toha 详情  初始化网站模板 # 首先在github下创建xxx.github.io的仓库 git clone https://github.com/fengzhenbing/fengzhenbing.github.io.git cd ./fengzhenbing.github.io # 初始化模板 hugo new site ./ -f=yaml --force #添加hugo-toha主题 git submodule add https://github.com/hugo-toha/toha.git themes/toha #本地运行 hugo server -t toha -w 修改配置 参考themes/toha/exampleSite，配置网站根目录下的config.yml文件，配置网站各个模块\nbaseURL:https://fengzhenbing.github.io/languageCode:en-usdefaultContentLanguage:cntitle:\u0026#34;Feng Zhenbing\u0026#39;s Blog\u0026#34;theme:\u0026#34;toha\u0026#34;# Manage languages# For any more details, you can check the official documentation: https://gohugo.io/content-management/multilingual/languages:cn:languageName:中文weight:1# en:# languageName: English# weight: 2# Control TOC depthmarkup:tableOfContents:startLevel:2endLevel:6ordered:false# Enable global emoji supportenableEmoji:true# Site parametersparams:# GitHub repo URL of your sitegitRepo:https://github.com/fengzhenbing/fengzhenbing.github.iogitBranch:master# specify whether you want to write some blog posts or notenableBlogPost:true# specify whether you want to show Table of Contents in reading pageenableTOC:true# Provide newsletter configuration. This feature hasn\u0026#39;t been implemented yet.# Currently, you can just hide it from the footer.newsletter:enable:true 至此浏览器中查看http://localhost:1313/ 可看到大致的博客网站, 更多细节查看https://toha-guides.netlify.app/posts/getting-started/prepare-site/  Github Pages 中部署  创建分支  #github 创建部署文件的分支 git branch -b gh-pages git push   github中setting中将部署分支设为gh-pages分支  img.png \n  开启github actions  img_1.png \n  编写github actions的部署文件\n  # 进入网站根目录，创建./github/workflows目录mkdir ./github/workflowscd ./github/workflowsvim deploy-site.yamldeploy-site.yaml\nname:Deploy to Github Pages# run when a commit or pr is pushed to \u0026#34;master\u0026#34; branchon:pull_request:push:branches:- masterjobs:deploy:runs-on:ubuntu-18.04steps:# checkout to the commit that has been pushed- uses:actions/checkout@v2with:submodules:true# Fetch Hugo themes (true OR recursive)fetch-depth:0# Fetch all history for .GitInfo and .Lastmod# install Hugo- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:\u0026#39;0.77.0\u0026#39;extended:true# build website- name:Buildrun:hugo --minify# push the generated content into the `main` (former `master`) branch.- name:Deployuses:peaceiris/actions-gh-pages@v3if:github.event_name == \u0026#39;push\u0026#39; \u0026amp;\u0026amp; github.ref == \u0026#39;refs/heads/master\u0026#39;with:github_token:${{ secrets.GITHUB_TOKEN }}publish_branch:gh-pages# if your main branch is `master` use that here.publish_dir:./public提交后，每次改动master分支，github action会运行以上任务，自动打包文件到gh-pages分支，并部署\n","date":"2020-12-08T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/hugo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/","title":"Hugo搭建博客"},{"content":"配置中心刷新原理 nacos和spring cloud config两种配置中心动态刷新的范围都是以下两种：\n @ConfigurationProperties 注解的配置类 @RefreshScope 注解的bean  手动刷新 post请求config客户端的/refresh端点\n自动刷新   WebHooks动态触发刷新\n  spring-cloud-bus动态刷新\n  这时Spring Cloud Bus做配置更新步骤如下:\n 提交代码触发post给Server端发送bus/refresh Server端接收到请求并发送给Spring Cloud Bus Spring Cloud bus接到消息并通知给其它客户端 其它客户端接收到通知，请求Server端获取最新配置 全部客户端均获取到最新的配置  这样的话我们在server端的代码做一些改动，来支持/actuator/bus-refresh\nSpring Cloud Bus Spring Cloud Bus 使用轻量级的消息代理来连接微服务架构中的各个服务，可以将其用于广播状态更改（例如配置中心配置更改）或其他管理指令\n目前 Spring Cloud Bus 支持两种消息代理：RabbitMQ 和 Kafka。\n参考\nhttps://blog.csdn.net/woshilijiuyi/article/details/88293782\nhttps://www.cnblogs.com/babycomeon/p/11141160.html\n","date":"2020-12-08T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%88%B7%E6%96%B0/","title":"配置中心刷新"},{"content":"小程序原理  使用两个线程：保证平台安全性，不能让开发者控制 render 线程，控制 render 线程将会造成小程序平台方管控困难 worker线程： 用户控制  响应 render 线程的事件，并执行小程序业务逻辑。 准备好数据，通过 setData 传到 page 中，由 page 进行渲染。   render线程：接收数据渲染到页面  TARO   官网\nhttps://taro-ui.jd.com/\n  特点\n  编译时转换\n  React vue \u0026hellip;\n  一套组件可以在 微信小程序，支付宝小程序，百度小程序，H5 多端适配运行\n    创建项目\n# 使用 npm 安装 CLI $ npm install -g @tarojs/cli # OR 使用 yarn 安装 CLI $ yarn global add @tarojs/cli # OR 安装了 cnpm，使用 cnpm 安装 CLI $ cnpm install -g @tarojs/cli   Remax   官网\n  特点\n  运行时转换:worker 线程维护一棵 vdom tree，然后同步到 render 线程通过 w|axml 来进行渲染。\n  react开发\n  多平台支持：支持阿里程序、微信小程序(QQ小程序)、头条小程序以及 Web 应用的开发。\n    创建项目\nnpx create-remax-app my-app cd my-app \u0026amp;\u0026amp; npm install npm run dev \u0026lt;platform\u0026gt; # 跨平台，如：要在阿里小程序环境运行，则 npm run dev ali  案例https://github.com/remaxjs/awesome-remax    WebPY   官网\n  特点\n 类似Vue开发 腾讯出品：小程序最早的框架之一    创建项目\n$ npm install @wepy/cli -g # 全局安装 WePY CLI 工具 $ wepy init standard myproj # 使用 standard 模板初始化项目 $ cd myproj # 进入到项目目录 $ npm install # 安装项目依赖包 $ npm run dev # 监听并且编译项目  案例https://github.com/aben1188/awesome-wepy    Kbone  特点  运行时转换:worker 线程维护一棵 vdom tree，然后同步到 render 线程通过 w|axml 来进行渲染。    uniapp 参考：小程序多平台同构方案分析-kbone 与 remax\n","date":"2020-11-23T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E5%B0%8F%E7%A8%8B%E5%BA%8F%E6%A1%86%E6%9E%B6/","title":"小程序框架"},{"content":"kafka相关概念 二代mq, scala开发\n broker topic patition producer customer Customer group leader follower rebalance  服务端partition数量扩大 消费者组中某个消费者down掉    Topic特性  通过partition增加可扩展性：线上改partion数，rebalance ，会照成性能抖动。 partition有序达到高吞吐 partition多副本增加容错性  kafka单机   安装 http://kafka.apache.org/downloads\n  修改配置\ncd kafka_2.13-2.7.0 # 打开 listeners=PLAINTEXT://localhost:9092 vim config/server.properties # 启动zookeeper bin/zookeeper-server-start.sh config/zookeeper.properties # 启动kafaka bin/kafka-server-start.sh config/server.properties   命令测试\n# 创建topic mokernetdeMac-mini:kafka_2.13-2.7.0 mokernet$ bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic testf --partitions 4 --replication-factor 1 Created topic testf. # 查看 bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic testf # 消费者从头开始消费 bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic testf # 生产者生产 bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic testf # 生产者性能测试 100万条数据 每条1000byte 限流100万条 bin/kafka-producer-perf-test.sh --topic testf --num-records 1000000 --record-size 1000 --throughput 1000000 --producer-props bootstrap.servers=localhost:9092 # 消费者性能测试 消费100万条数据 一个线程 bin/kafka-consumer-perf-test.sh --bootstrap-server localhost:9092 --topic testf --fetch-size 1048576 --messages 1000000 --threads 1   kafka集群   修改各个节点的三个属性配置如下：\n# 复制新的配置文件 cp config/server.properties config/server-1.properties cp config/server.properties config/server-2.properties # 修改 id 端口 数据文件目录 # config/server-1.properties: broker.id=1 listeners=PLAINTEXT://:9093 log.dir=/tmp/kafka-logs-1 # 修改 id 端口 数据文件目录 # config/server-2.properties: broker.id=2 listeners=PLAINTEXT://:9094 log.dir=/tmp/kafka-logs-2   启动\nbin/kafka-server-start.sh config/server.properties \u0026amp; bin/kafka-server-start.sh config/server-1.properties \u0026amp; bin/kafka-server-start.sh config/server-2.properties \u0026amp;   测试\n# 创建topic test42 4个patition 2个副本 bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic test42 --partitions 4 --replication-factor 2 # 查看 mokernetdeMac-mini:kafka_2.13-2.7.0 mokernet$ bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic test42 Topic: test42 PartitionCount: 4 ReplicationFactor: 2 Configs: Topic: test42 Partition: 0 Leader: 1 Replicas: 1,2 Isr: 1,2 Topic: test42 Partition: 1 Leader: 2 Replicas: 2,0 Isr: 2,0 Topic: test42 Partition: 2 Leader: 0 Replicas: 0,1 Isr: 0,1 Topic: test42 Partition: 3 Leader: 1 Replicas: 1,0 Isr: 1,0 # 消费者从头开始消费 bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic test42 # 生产者生产 bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test42 # 容错性测试 ，关闭id=1的broker ps aux | grep server-1.properties \u0026gt; mokernet 30302 0.0 1.9 7323756 405020 s006 S+ 10:31PM 0:16.87 /usr/bin/java -Xmx kill -9 30302 #查看 如下，rebalance mokernetdeMac-mini:kafka_2.13-2.7.0 mokernet$ bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic test42 Topic: test42 PartitionCount: 4 ReplicationFactor: 2 Configs: Topic: test42 Partition: 0 Leader: 2 Replicas: 1,2 Isr: 2 Topic: test42 Partition: 1 Leader: 2 Replicas: 2,0 Isr: 2,0 Topic: test42 Partition: 2 Leader: 0 Replicas: 0,1 Isr: 0 Topic: test42 Partition: 3 Leader: 0 Replicas: 1,0 Isr: 0   Kafka connect  通过kafka导入导出数据  # 写入数据到输入文件 test.txt echo -e \u0026#34;testtest\u0026#34; \u0026gt; test.txt # 启动connect # 三个配置文件：  # 1. Kafka Connect的配置文件，包含常用的配置，如Kafka brokers连接方式和数据的序列化格式。 # 2. 源连接器配置，用于从输入文件读取行，并将其输入到 Kafka topic # 3. 接收器连接器配置，它从Kafka topic中读取消息，并在输出文件中生成一行。 bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties # 查看输出文件， 可以看到不断地往 test.txt写入数据时，test.sink.txt 会持续产生数据 tail -f test.sink.txt kafak stream        Kafka 特点  高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒； 可扩展性：kafka集群支持热扩展； 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止丢失； 容错性：允许集群中的节点失败(若分区副本数量为n,则允许n-1个节点失败)； 高并发：单机可支持数千个客户端同时读写；  使用场景   消息系统\n  日志聚合\n  度量监控\n  流式处理\n  跟踪网站浏览记录\n  相关资料 参考 https://kafka.apachecn.org/\n大白话 kafka 架构原理\n","date":"2020-10-20T14:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/kafka%E5%9F%BA%E7%A1%80/","title":"kafka基础"},{"content":"消息队列作用  异步通信：异步通信，减少线程等待，特别是处理批量等大事务、耗时操作。 系统解耦:系统不直接调用，降低依赖，特别是不在线也能保持通信最终完成。 削峰填谷:压力大的时候，缓冲部分请求消息，类似于背压处理。 可靠通信:提供多种消息模式、服务质量、顺序保障等。  消息处理模式  点对点： PTP =\u0026gt; queue 发布订阅： PubSub =\u0026gt; Topic  消息语义 QOS  At most once At least once Exactly once  ","date":"2020-10-11T09:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/%E6%B6%88%E6%81%AF%E5%9F%BA%E7%A1%80/","title":"消息基础"},{"content":"单例模式  在运行期间，保证某个类只创建一个实例，保证一个类仅有一个实例，并提供一个访问它的全局访问点\n 饿汉式 public class Singleton { private static Singleton instance = new Singleton(); private Singleton() { } public static Singleton getInstance() { return instance; } }  优点就是实现简单，而且安全可靠 缺点，没有懒加载，可能用不到，却实例化了  懒汉式 public class SingletonSafe { // 防止指令重排  private static volatile SingletonSafe singleton; private SingletonSafe() { } public static SingletonSafe getSingleton() { if (singleton == null) { synchronized (SingletonSafe.class) { if (singleton == null) { singleton = new SingletonSafe(); } } } return singleton; } } 双重检查，保证线程安全\n静态内部类 public class Singleton { private static class SingletonHolder { private static Singleton instance = new Singleton(); } private Singleton() { } public static Singleton getInstance() { return SingletonHolder.instance; } }  通过静态内部类的方式实现单例模式是线程安全的，同时静态内部类不会在Singleton类加载时就加载，而是在调用getInstance()方法时才进行加载，达到了懒加载的效果。 缺点：可能会有反射攻击或者反序列化攻击  通过枚举实现单例模式（推荐） effective java推荐\npublic enum Singleton { /** * Inst singleton. */ INST; /** * The Singles. */ private static final Map\u0026lt;String, Object\u0026gt; SINGLES = new ConcurrentHashMap\u0026lt;\u0026gt;(); /** * Single. * * @param clazz the clazz * @param o the o */ public void single(final Class clazz, final Object o) { SINGLES.put(clazz.getName(), o); } /** * Get t. * * @param \u0026lt;T\u0026gt; the type parameter * @param clazz the clazz * @return the t */ @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public \u0026lt;T\u0026gt; T get(final Class\u0026lt;T\u0026gt; clazz) { return (T) SINGLES.get(clazz.getName()); } } ","date":"2020-08-21T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E5%8D%95%E4%BE%8B/","title":"单例"},{"content":"两段式提交 （2 Phase Commit，2PC）\n准备阶段 重操作\n提交阶段 轻操作\n三段式提交 （3 Phase Commit，3PC）\n  在事务需要回滚的场景中：三段式的性能通常是要比两段式好很多的。\n  但在事务能够正常提交的场景中：两者的性能都依然很差，甚至三段式因为多了一次询问，还要稍微更差一些。\n  CanCommit 轻操作\nPreCommit 重操作\nCanCommit 轻操作\n","date":"2020-08-13T14:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/%E5%85%A8%E5%B1%80%E4%BA%8B%E7%89%A9/","title":"全局事物"},{"content":"1 介绍 本地事务（局部事务）在程序代码层面，最多只能对事务接口做一层标准化的包装（如 JDBC 接口），并不能深入参与到事务的运作过程当中，事务的开启、终止、提交、回滚、嵌套、设置隔离级别，乃至与应用代码贴近的事务传播方式，全部都要依赖底层数据源的支持才能工作。\n2 原子性（A）和持久性（D） 崩溃 （Crash）：数据库、操作系统一侧的崩溃，甚至是机器突然断电宕机等意外情况。\n崩溃恢复（Crash Recovery，也有资料称作 Failure Recovery 或 Transaction Recovery）:为了保证原子性和持久性，就只能在崩溃后采取恢复的补救措施\nCommit Logging 为了能够顺利地完成崩溃恢复，在磁盘中写入数据就不能像程序修改内存中变量值那样，直接改变某表某行某列的某个值，而是必须将修改数据这个操作所需的全部信息，包括修改什么数据、数据物理上位于哪个内存页和磁盘块中、从什么值改成什么值，等等，以日志的形式——即仅进行顺序追加的文件写入的形式（这是最高效的写入方式）先记录到磁盘中。只有在日志记录全部都安全落盘，数据库在日志中看到代表事务成功提交的“提交记录”（Commit Record）后，才会根据日志上的信息对真正的数据进行修改，修改完成后，再在日志中加入一条“结束记录”（End Record）表示事务已完成持久化.\n阿里的OceanBase 采用Commit Logging 机制来实现事务\n缺点：所有对数据的真实修改都必须发生在事务提交以后，即日志写入了 Commit Record 之后。在此之前，即使磁盘 I/O 有足够空闲、即使某个事务修改的数据量非常庞大，占用了大量的内存缓冲区，无论有何种理由，都决不允许在事务提交之前就修改磁盘上的数据。\nWrite-Ahead Logging 按照事务提交时点为界，划分为 FORCE 和 STEAL 两类情况。\n FORCE：当事务提交后，要求变动数据必须同时完成写入则称为 FORCE，如果不强制变动数据必须同时完成写入则称为 NO-FORCE。现实中绝大多数数据库采用的都是 NO-FORCE 策略，因为只要有了日志，变动数据随时可以持久化，从优化磁盘 I/O 性能考虑，没有必要强制数据写入立即进行。 STEAL：在事务提交前，允许变动数据提前写入则称为 STEAL，不允许则称为 NO-STEAL。从优化磁盘 I/O 性能考虑，允许数据提前写入，有利于利用空闲 I/O 资源，也有利于节省数据库缓存区的内存。  Write-Ahead Logging 在崩溃恢复时会执行以下三个阶段的操作：\n 分析阶段（Analysis）：该阶段从最后一次检查点（Checkpoint，可理解为在这个点之前所有应该持久化的变动都已安全落盘）开始扫描日志，找出所有没有 End Record 的事务，组成待恢复的事务集合，这个集合至少会包括 Transaction Table 和 Dirty Page Table 两个组成部分。 重做阶段（Redo）：该阶段依据分析阶段中产生的待恢复的事务集合来重演历史（Repeat History），具体操作为：找出所有包含 Commit Record 的日志，将这些日志修改的数据写入磁盘，写入完成后在日志中增加一条 End Record，然后移除出待恢复事务集合。 回滚阶段（Undo）：该阶段处理经过分析、重做阶段后剩余的恢复事务集合，此时剩下的都是需要回滚的事务，它们被称为 Loser，根据 Undo Log 中的信息，将已经提前写入磁盘的信息重新改写回去，以达到回滚这些 Loser 事务的目的。  Shadow Paging 副本方式\n对数据的变动会写到硬盘的数据中，但并不是直接就地修改原先的数据，而是先将数据复制一份副本，保留原数据，修改副本数据。\n事务完成修改数据引用指针 （修改指针保证原子性）\n3 隔离性 锁 写锁 （Write Lock，也叫作排他锁，eXclusive Lock，简写为 X-Lock）：如果数据有加写锁，就只有持有写锁的事务才能对数据进行写入操作，数据加持着写锁时，其他事务不能写入数据，也不能施加读锁。\n读锁 （Read Lock，也叫作共享锁，Shared Lock，简写为 S-Lock）：多个事务可以对同一个数据添加多个读锁，数据被加上读锁后就不能再被加上写锁，所以其他事务不能对该数据进行写入，但仍然可以读取。对于持有读锁的事务，如果该数据只有它自己一个事务加了读锁，允许直接将其升级为写锁，然后写入数据。\n范围锁 对于某个范围直接加排他锁，在这个范围内的数据不能被写入\nSELECT*FROMbooksWHEREprice\u0026lt;200FORUPDATE;以下四种隔离级别属于数据库理论的基础知识， 其实不同隔离级别以及幻读、不可重复读、脏读等问题都只是表面现象，是各种锁在不同加锁时间上组合应用所产生的结果，以锁为手段来实现隔离性才是数据库表现出不同隔离级别的根本原因。\n完全不加锁。。。 读未提交 缺点：出现赃读\nSELECT*FROMbooksWHEREid=1;/* 时间顺序：1，事务： T1 *//* 注意没有COMMIT */UPDATEbooksSETprice=90WHEREid=1;/* 时间顺序：2，事务： T2 *//* 这条SELECT模拟购书的操作的逻辑 */SELECT*FROMbooksWHEREid=1;/* 时间顺序：3，事务： T1 */ROLLBACK;/* 时间顺序：4，事务： T2 */实现方式：不加读锁 ，还是有写锁的，不会出现赃写\n读已提交 缺点：不可重复读，只能读到提交的数据。 缺乏贯穿整个事务周期的读锁，无法禁止读取过的数据发生变化，\nSELECT*FROMbooksWHEREid=1;/* 时间顺序：1，事务： T1 */UPDATEbooksSETprice=110WHEREid=1;COMMIT;/* 时间顺序：2，事务： T2 */SELECT*FROMbooksWHEREid=1;COMMIT;/* 时间顺序：3，事务： T1 */实现方式：对事务涉及的数据加的写锁会一直持续到事务结束，但加的读锁在查询操作完成后就马上会释放。\n可重复读 相对于读已提交，读锁存在时间为整个事物周期\n缺点：可能出现幻读\nSELECTcount(1)FROMbooksWHEREprice\u0026lt;100/* 时间顺序：1，事务： T1 */INSERTINTObooks(name,price)VALUES(\u0026#39;深入理解Java虚拟机\u0026#39;,90)/* 时间顺序：2，事务： T2 */SELECTcount(1)FROMbooksWHEREprice\u0026lt;100/* 时间顺序：3，事务： T1 */实现方式：对事务所涉及的数据加读锁和写锁，且一直持有至事务结束，但不再加范围锁。\n可串行化 缺点：顺序执行，吞吐量低，性能低\n实现方式： 对事务所有读、写的数据全都加上读锁、写锁和范围锁；实际还是很复杂的，要分成 Expanding 和 Shrinking 两阶段去处理读锁、写锁与数据间的关系，称为Two-Phase Lock，2PL\n4 多版本并发控制 参考 （https://icyfenix.cn/architect-perspective/general-architecture/transaction/local.html）\n","date":"2020-08-12T14:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/%E6%9C%AC%E5%9C%B0%E4%BA%8B%E5%8A%A1/","title":"本地事务"},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/","title":""}]