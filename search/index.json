[{"content":"1. jmx_exporter 下载jmx_exporter ubuntu:/# mkdir -p /usr/local/prometheus/jmx_exporter ubuntu:/# cd /usr/local/prometheus/jmx_exporter ubuntu:/# wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.16.1/jmx_prometheus_javaagent-0.16.1.jar 配置文件jmx_exporter jmx_exporter.yml\n vim /usr/local/prometheus/jmx_exporter/jmx_exporter.yml --- rules: - pattern: \u0026quot;.*\u0026quot; java agent运行jmx_exporter 以java agent的方式启动你的一个java应用\njava -javaagent:/usr/local/prometheus/jmx_prometheus_javaagent-0.16.1.jar=3010:/usr/local/prometheus/jmx_exporter.yml -jar xxx-web-0.1-SNAPSHOT.jar 2. prometheus docker方式下载运行 # docker pull prom/prometheus #下载docker镜像 # mkdir -p /etc/prometheus # vim /etc/prometheus/prometheus.yml #配置 # docker run -d \\ -p 192.168.3.13:9090:9090 \\  -v /etc/prometheus:/etc/prometheus \\  prom/prometheus; prometheus中配置上步的jmx的metrics global:scrape_interval:15sscrape_timeout:10sevaluation_interval:15salerting:alertmanagers:- follow_redirects:truescheme:httptimeout:10sapi_version:v2static_configs:- targets:[]scrape_configs:- job_name:prometheushonor_timestamps:truescrape_interval:15sscrape_timeout:10smetrics_path:/metricsscheme:httpfollow_redirects:truestatic_configs:- targets:- 192.168.3.13:9090### 以下为jmx_exporter地址：需改为你实际的- job_name:\u0026#39;jmx\u0026#39;static_configs:scrape_interval:15s- targets:[\u0026#39;192.168.3.14:3010\u0026#39;]验证 访问http://192.168.3.13:9090 可看到读取的jmx_exporter的metrics\n3. Grafana连接Prometheus docker pull grafana/grafana docker run -d --name=grafana -p 3000:3000 grafana/grafana 访问http://192.168.3.13:3000 使用admin/admin即可登录\n配置prometheus数据源 配置dashboard 找一个合适的dashboard导入。如https://grafana.com/grafana/dashboards/3457\n效果 参考  prometheus安装 https://prometheus.io/docs/prometheus/latest/installation/   JMX Exporter 项目地址: https://github.com/prometheus/jmx_exporter JVM 监控面板: https://grafana.com/grafana/dashboards/3457 Grafana(https://grafana.com/)  ","date":"2021-08-15T14:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/prometheus%E7%9B%91%E6%8E%A7jvm/","title":"Prometheus监控JVM"},{"content":"Disruptor通过以下设计来解决队列速度慢的问题：   环形数组结构\n  元素位置定位\n数组长度2^n， 位运算，加快定位的速度\n  无锁设计\nCas操作保证线程安全\n  参考 https://tech.meituan.com/2016/11/18/disruptor.html\nhttps://blog.csdn.net/liweisnake/article/details/9113119\n","date":"2021-07-20T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/disruptor%E9%AB%98%E6%80%A7%E8%83%BD%E9%98%9F%E5%88%97/","title":"Disruptor高性能队列"},{"content":"Envoy概述 Envoy 是以 C++ 开发的高性能代理;\n其内置服务发现、负载均衡、TLS终止、HTTP/2、GRPC代理、熔断器、健康检查，基于百分比流量拆分的灰度发布、故障注入等功能\n Downstream：下游主机，指连接到Envoy的主机，这些主机用来发送请求并接受响应。 Upstream：上游主机，指接收来自Envoy连接和请求的主机，并返回响应。 Listener：服务或程序的监听器， Envoy暴露一个或多个监听器监听下游主机的请求，当监听到请求时，通过Filter Chain把对请求的处理全部抽象为Filter， 例如ReadFilter、WriteFilter、HttpFilter等。 Cluster：服务提供集群，指Envoy连接的一组逻辑相同的上游主机。Envoy通过服务发现功能来发现集群内的成员，通过负载均衡功能将流量路由到集群的各个成员。 xDS：xDS中的x是一个代词，类似云计算里的XaaS可以指代IaaS、PaaS、SaaS等。DS为Discovery Service，即发现服务的意思。xDS包括CDS（cluster discovery service）、RDS（route discovery service）、EDS（endpoint discovery service）、ADS（aggregated discovery service），其中ADS称为聚合的发现服务，是对CDS、RDS、LDS、EDS服务的统一封装，解决CDS、RDS、LDS、EDS信息更新顺序依赖的问题，从而保证以一定的顺序同步各类配置信息。以上Endpoint、Cluster、Route的概念介绍如下：  Endpoint：一个具体的“应用实例”，类似于Kubernetes中的一个Pod； Cluster：可以理解“应用集群”，对应提供相同服务的一个或多个Endpoint， 类似Kubernetes中Service概念，即一个Service提供多个相同服务的Pod； Route：当我们做金丝雀发布部署时，同一个服务会有多个版本，这时需要Route规则规定请求如何路由到其中的某个版本上。    http://www.dockone.io/article/9116\n","date":"2021-07-20T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/envoy/","title":"Envoy"},{"content":"背景 随着业务的发展，系统规模也会越来越大，各微服务间的调用关系也越来越错综复杂，每一个前端请求都会形成一条复杂的分布式服务调用链路，在每条链路中任何一个依赖服务出现延迟过高或错误的时候都会引起请求最后的失败。\n链路追踪原理 实现请求跟踪 当请求发送到分布式系统的入口端点时，只需要服务跟踪框架为该请求创建一个唯一的跟踪标识Trace ID，\n同时在分布式系统内部流转的时候，框架失踪保持该唯一标识，直到返回给请求方位置。\ntrace：服务追踪的追踪单元是从客户发起请求（request）抵达被追踪系统的边界开始，到被追踪系统向客户返回响应（response）为止的过程，称为一\n个“trace”\n统计各处理单元的时间延迟 当请求到达各个服务组件时，也是通过一个唯一标识Span ID来标记它的开始，具体过程以及结束。对每一个Span来说，它必须有开始和结束两个节点，通过记录开始Span和结束Span的时间戳，就能统计出该Span的时间延迟，除了时间戳记录之外，它还可以包含一些其他元数据，比如时间名称、请求信息等。\nUI可视化 APM技术组件 Zipkin+Sleuth Apache SkyWalking Cat Pinpoint 特点对比 ","date":"2021-06-12T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E6%9C%8D%E5%8A%A1%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/","title":"服务链路追踪"},{"content":"kubelet日志 journalctl -fu kubelet kubectl -h kubectl get namespaces kubectl get pods -A kubectl logs -f --tail=200 -l app=account -n bookstore-microservices 常用命令 #查看端口映射 kubectl get svc -n kube-system #查看 secret kubectl get secret -n kube-system #查看 token kubectl describe secret kubernetes-dashboard --namespace=kube-system #k8s 无法启动，查看日志，查找Failed journalctl -xefu kubelet #查看pod错误日志 kubectl logs kubernetes-dashboard-8556c848b7-4kpzd --namespace=kube-system #对资源进行配置 kubectl apply -f kubernetes-dashboard.yaml kubectl delete -f kubernetes-dashboard.ya YAML配置文件管理对象 对象管理： # 创建deployment资源 kubectl create -f nginx-deployment.yaml # 查看deployment kubectl get deploy # 查看ReplicaSet kubectl get rs # 查看pods所有标签 kubectl get pods --show-labels # 根据标签查看pods kubectl get pods -l app=nginx # 滚动更新镜像 kubectl set image deployment/nginx-deployment nginx=nginx:1.11 或者 kubectl edit deployment/nginx-deployment 或者 kubectl apply -f nginx-deployment.yaml # 实时观察发布状态： kubectl rollout status deployment/nginx-deployment # 查看deployment历史修订版本 kubectl rollout history deployment/nginx-deployment kubectl rollout history deployment/nginx-deployment --revision=3 # 回滚到以前版本 kubectl rollout undo deployment/nginx-deployment kubectl rollout undo deployment/nginx-deployment --to-revision=3 # 扩容deployment的Pod副本数量 kubectl scale deployment nginx-deployment --replicas=10 # 设置启动扩容/缩容 kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80 ","date":"2021-06-10T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/k8s-kubectl%E5%91%BD%E4%BB%A4/","title":"k8s Kubectl命令"},{"content":"单体架构(spring boot)   优点：所有代码都运行在同一个进程空间之内，所有模块、方法的调用都无须考虑网络分区、对象复制这些麻烦的事和性能损失。\n  缺点：损失了各个功能模块的自治、隔离能力；\n​\t由于隔离能力的缺失难以阻断错误传播、不便于动态更新程序以外，还面临难以技术异构的困难\n​\t可以 使用OSGi 这种运行时模块化框架，但是太复杂了。\n  SOA 架构（Service-Oriented Architecture） 面向服务的架构是一次具体地、系统性地成功解决分布式服务主要问题的架构模式。\nSOAP 协议被逐渐边缘化的本质原因：过于严格的规范定义带来过度的复杂性。而构建在 SOAP 基础之上的 ESB、BPM、SCA、SDO 等诸多上层建筑，进一步加剧了这种复杂性。\n微服务架构(spring cloud) 微服务是一种软件开发技术，是一种 SOA 的变体形式。\n升级背景：\n 制约软件质量与业务能力提升的最大因素是人而非硬件： 单体架构没有什么有效阻断错误传播的手段 技术异构的需求从可选渐渐成为必须：很多 Java 不擅长的事情 人工智能python 分布式协调工具 Etcd ,NSI C 编写的 Redis， \u0026hellip;  由于隔离能力的缺失，单体除了难以阻断错误传播、不便于动态更新程序以外，还面临难以技术异构的困难，每个模块的代码都通常需要使用一样的程序语言，乃至一样的编程框架去开发。\n随着软件架构演进，构筑可靠系统从“追求尽量不出错”，到正视“出错是必然”的观念转变，才是微服务架构得以挑战并逐步开始取代运作了数十年的单体架构的底气所在\n微服务时代充满着自由的气息，微服务时代充斥着迷茫的选择。\n微服务架构(Kubernetes) 升级背景：\n  微服务中的各种新技术名词，如配置中心、服务发现、网关、熔断、负载均衡等等带来的技术组件 Config、Eureka、Zuul、Hystrix、Ribbon、Feign 等\n占据了产品的大部分编译后的代码容量\n之前在应用层面而不是基础设施层面去解决这些分布式问题，完全是因为由硬件构成的基础设施，跟不上由软件构成的应用服务的灵活性的无奈之举\n  以 Docker Swarm、Apache Mesos 与 Kubernetes 为主要竞争者的“容器战争”终于有了明确的结果，Kubernetes 登基加冕\n容器动态构建出 DNS 服务器、服务负载均衡器等一系列虚拟化的基础设施，去代替原有的应用层面的技术组件\n  Microservice https://martinfowler.com/articles/microservices.html\n服务网格（Service Mesh） 升级背景：\n  基础设施是针对整个容器来管理的，粒度相对粗旷，只能到容器层面，对单个远程服务就很难有效管控。\n  服务的监控、认证、授权、安全、负载均衡等都有可能面临细化管理的需求\n譬如服务调用时的负载均衡，往往需要根据流量特征，调整负载均衡的层次、算法，等等，而 DNS 尽管能实现一定程度的负载均衡，但通常并不能满足这些额外的需求。\n  “边车代理模式”（Sidecar Proxy）   数据平面通信：这个代理除了实现正常的服务间通信外（称为数据平面通信）\n  控制平面通信：还接收来自控制器的指令（称为控制平面通信），根据控制平面中的配置，对数据平面通信的内容进行分析处理，以实现熔断、认证、度量、监控、负载均衡等各种附加功能。\n通过\n主要框架：Istio\n  上帝的归上帝，凯撒的归凯撒，业务与技术完全分离，远程与本地完全透明，也许这就是最好的时代了吧？\n无服务 更应该成为 无服务器\n包含两方面：\n 后端设施：指数据库、消息队列、日志、存储，等等这一类用于支撑业务逻辑运行，但本身无业务含义的技术组件，这些后端设施都运行在云中，无服务中称其为“后端即服务”（Backend as a Service，BaaS）。 函数： 指业务逻辑代码，这里函数的概念与粒度，都已经很接近于程序编码角度的函数了，其区别是无服务中的函数运行在云端，不必考虑算力问题，不必考虑容量规划（从技术角度可以不考虑，从计费的角度你的钱包够不够用还是要掂量一下的），无服务中称其为“函数即服务”（Function as a Service，FaaS）。  ","date":"2021-06-10T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E6%9E%B6%E6%9E%84%E6%BC%94%E5%8F%98/","title":"架构演变"},{"content":"1.环境准备 安装 Kubernetes 最小需要 2 核处理器、2 GB 内存，且为 x86 架构（暂不支持 ARM 架构)\n本次实验操作系统：ubantu 20.04LTS\nKubernetes 并不在主流 Debian 系统自带的软件源中，所以要手工注册，然后才能使用apt-get安装\n# 添加GPG Key $ sudo curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - # 添加K8S软件源 $ sudo add-apt-repository \u0026#34;deb https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; 如果不能科学上网，可以使用阿里云的软件源地址\n# 添加GPG Key $ curl -fsSL http://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - # 添加K8S软件源 $ sudo add-apt-repository \u0026#34;deb http://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main\u0026#34; 添加源后需要更新\nsudo apt-get update 2. 安装 kubelet、kubectl、kubeadm 官网介绍：https://kubernetes.io/docs/reference/setup-tools/kubeadm/\n kubeadm: 引导启动 Kubernate 集群的命令行工具。 kubelet: 在群集中的所有计算机上运行的组件, 并用来执行如启动 Pods 和 Containers 等操作。 kubectl: 用于操作运行中的集群的命令行工具。  2.1关闭Swap 分区 kubeadm 初始化集群之前，需要关闭Swap 分区，首先基于安全性（如在官方文档中承诺的 Secret 只会在内存中读写，不会落盘）、利于保证节点同步一致性等原因，从 1.8 版开始，Kubernetes 就在它的文档中明确声明了它默认不支持Swap 分区，在未关闭 Swap 分区的机器中，集群将直接无法启动；\n其他参考\n 主要有两个方面原因：\n第一是因为性能问题，在生产环境我们经常会遇到容器性能突然降低的情况，查看原因后，大部分都是因为开启了swap导致的。swap看似解决了有限内存的问题，但这种通过时间换空间的做法也给性能带来了很大问题，尤其是在高并发场景中，很容易导致系统不稳定。\n第二是因为k8s定义的资源模型中，CPU和内存都是确定的可用资源，在调度的时候都会考虑在内。比如，设置了内存设置了limit 2G，就代表最大可用内存是2G，而引入swap（cgroup支持swap限制）后这个模型就变得复杂了，而且需要结合Qos，swap的使用完全是由操作系统根据水位自行调节的，并不直接受kubelet管理\n 一次性关闭\nsudo swapoff -a 永久关闭\n编辑器打开/etc/fstab，注释其中带有“swap”的行即可，\n# 先备份 $ yes | sudo cp /etc/fstab /etc/fstab_bak # 进行修改 $ sudo cat /etc/fstab_bak | grep -v swap \u0026gt; /etc/fstab 3 预拉取镜像 先查看kubeadm版本 v1.21.3\nroot@fengzhenbing-ubuntu:/home/fengzhenbing# kubeadm version kubeadm version: \u0026amp;version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;21\u0026#34;, GitVersion:\u0026#34;v1.21.3\u0026#34;, GitCommit:\u0026#34;ca643a4d1f7bfe34773c74f79527be4afd95bf39\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-07-15T21:03:28Z\u0026#34;, GoVersion:\u0026#34;go1.16.6\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 首先使用以下命令查询当前版本需要哪些镜像：\nfengzhenbing@fengzhenbing-ubuntu:/etc/docker$ kubeadm config images list --kubernetes-version v1.21.3 k8s.gcr.io/kube-apiserver:v1.21.3 k8s.gcr.io/kube-controller-manager:v1.21.3 k8s.gcr.io/kube-scheduler:v1.21.3 k8s.gcr.io/kube-proxy:v1.21.3 k8s.gcr.io/pause:3.4.1 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/coredns/coredns:v1.8.0 k8s.gcr.io 为google官方（Google Container Registry），对于不能科学上网的同学来说，可以使用阿里云加速\n3.1 配置阿里云加速地址 阿里云加速地址获取 https://cr.console.aliyun.com/cn-shanghai/instances/mirrors\nsudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://xxxx.mirror.aliyuncs.com\u0026#34;] } EOF sudo systemctl daemon-reload sudo systemctl restart docker 3.2 拉取镜像，修改tag 对于通过kubeadm config images list --kubernetes-version v1.21.3查找到需要下载的镜像名称及版本后，可以从DockerHub上找存有相同镜像的仓库来拉取。 我是用的k8simage的仓库，找到对应的v1.21.3版本的对应image\n下面为从k8simage拉取到的镜像\nfengzhenbing@fengzhenbing-ubuntu:/etc/docker$ sudo docker image list REPOSITORY TAG IMAGE ID CREATED SIZE k8simage/kube-apiserver v1.21.3 3d174f00aa39 9 days ago 126MB k8simage/kube-scheduler v1.21.3 6be0dc1302e3 9 days ago 50.6MB k8simage/kube-controller-manager v1.21.3 bc2bb319a703 9 days ago 120MB k8simage/kube-proxy v1.21.3 adb2816ea823 9 days ago 103MB k8simage/pause 3.4.1 0f8457a4c2ec 6 months ago 683kB k8simage/etcd 3.4.13-0 0369cf4303ff 11 months ago 253MB k8simage/coredns 1.7.0 bfe3a36ebd25 13 months ago 45.2MB 对每一镜像，1先拉取，2修改tag 3最后删除原来的。以kube-apiserver v1.21.3为例子\nsudo docker pull k8simage/kube-apiserver:v1.21.3 sudo docker tag k8simage/kube-apiserver:v1.21.3 k8s.gcr.io/kube-apiserver:v1.21.3 sudo docker rmi k8simage/kube-apiserver:v1.21.3 上面全部操作结束后 可看到都更新为kubeadm config images list --kubernetes-version v1.21.3所需要的镜像了\nroot@fengzhenbing-ubuntu:/home/fengzhenbing# sudo docker image list REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-apiserver v1.21.3 3d174f00aa39 9 days ago 126MB k8s.gcr.io/kube-scheduler v1.21.3 6be0dc1302e3 9 days ago 50.6MB k8s.gcr.io/kube-proxy v1.21.3 adb2816ea823 9 days ago 103MB k8s.gcr.io/kube-controller-manager v1.21.3 bc2bb319a703 9 days ago 120MB quay.io/coreos/flannel v0.14.0 8522d622299c 2 months ago 67.9MB k8s.gcr.io/pause 3.4.1 0f8457a4c2ec 6 months ago 683kB k8s.gcr.io/etcd 3.4.13-0 0369cf4303ff 11 months ago 253MB k8s.gcr.io/coredns/coredns v1.8.0 bfe3a36ebd25 13 months ago 45.2MB 4 初始化集群控制平面  确保 kubelet 是开机启动的  fengzhenbing@fengzhenbing-ubuntu:/etc/docker$ sudo systemctl start kubelet fengzhenbing@fengzhenbing-ubuntu:/etc/docker$ sudo systemctl enable kubelet  su 直接切换到 root 用户， 保证是root启动部署  kubeadm init \\ --pod-network-cidr=10.244.0.0/16 \\ --kubernetes-version v1.21.3 \\ --control-plane-endpoint 192.168.3.13  参数介绍   --kubernetes-version参数（要注意版本号与 kubelet 一致）的目的是与前面预拉取是一样的，避免额外的网络访问去查询版本号；如果能够科学上网，不需要加这个参数。\n--pod-network-cidr参数是给Flannel网络做网段划分使用的，着在稍后介绍完 CNI 网络插件时会去说明。\n--control-plane-endpoint参数是控制面的地址，强烈建议使用一个域名代替直接的IP地址来建立Kubernetes集群。因为CA证书直接与地址相关，Kubernetes中诸多配置（配置文件、ConfigMap资源）也直接存储了这个地址，一旦更换IP，要想要不重置集群，手工换起来异常麻烦。所以最好使用hostname（仅限单节点实验）或者dns name。\n 执行完，如下提示表明成功\n5 为当前用户生成 kubeconfig 使用 Kubernetes 前需要为当前用户先配置好 admin.conf 文件\nroot@fengzhenbing-ubuntu:~# mkdir -p $HOME/.kube root@fengzhenbing-ubuntu:~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config root@fengzhenbing-ubuntu:~# sudo chown $(id -u):$(id -g) $HOME/.kube/config 6 CNI插件 CNI 即“容器网络接口”\n部署 Kubernetes 时，我们可以有两种网络方案使得以后受管理的容器之间进行网络通讯：\n 使用 Kubernetes 的默认网络： 操作太复杂 使用 CNI 及其插件：  Flannel 插件为比较推荐的\n安装Flannel 插件 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml https://raw.githubusercontent.com 无法访问时，可以找到该yml文件上传到服务器，如下执行\nroot@fengzhenbing-ubuntu:/home/fengzhenbing# kubectl apply -f kube-flannel.yml Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ podsecuritypolicy.policy/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.apps/kube-flannel-ds created $ kubectl taint nodes --all node-role.kubernetes.io/master- 7 kubectl 命令自动补全功能 $ echo \u0026#39;source \u0026lt;(kubectl completion bash)\u0026#39; \u0026gt;\u0026gt; ~/.bashrc $ echo \u0026#39;source /usr/share/bash-completion/bash_completion\u0026#39; \u0026gt;\u0026gt; ~/.bashrc 8 加入其他 Node 节点到 Kubernetes 集群中 kubeadm init执行成功后的反馈内容中有提到：如上面的截图\nYou can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 192.168.3.13:6443 --token qxu02l.g995k2yhkxjh3k80 \\  --discovery-token-ca-cert-hash sha256:b9887f55b739bd89d3b0dbb038e693df9b5b7d9759902ffb2a250288ba1ffc25 \\  --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.3.13:6443 --token qxu02l.g995k2yhkxjh3k80 \\  --discovery-token-ca-cert-hash sha256:b9887f55b739bd89d3b0dbb038e693df9b5b7d9759902ffb2a250288ba1ffc25 Token 的有效时间为 24 小时，如果超时，使用以下命令重新获取：\n$ kubeadm token create --print-join-command 9相关问题 无法访问api端点如下\n对于实验测试非线上来讲，可以直接将system:anonymous加为用户\nkubectl create clusterrolebinding test:anonymous --clusterrole=cluster-admin --user=system:anonymous 对于正式环境，需要创建一个用户并授权\n ","date":"2021-06-08T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/kubeadm%E5%AE%89%E8%A3%85kubernetes/","title":"Kubeadm安装Kubernetes"},{"content":"1, ShutdownHook初识  在Java程序中可以通过添加关闭钩子，实现在程序退出时关闭资源、平滑退出的功能。 并且在以下几种场景将调用该钩子\n 程序正常退出 使用System.exit() 终端使用Ctrl+C触发的中断 系统关闭 使用Kill pid命令干掉进程   具体来讲Runtime.addShutdownHook 添加钩子到 ApplicationShutdownHooks中。\n// Runtime添加钩子（钩子具体来讲就是一个要执行的线程任务） public void addShutdownHook(Thread hook) { SecurityManager sm = System.getSecurityManager(); if (sm != null) { sm.checkPermission(new RuntimePermission(\u0026#34;shutdownHooks\u0026#34;)); } ApplicationShutdownHooks.add(hook); } // Runtime去除钩子  public boolean removeShutdownHook(Thread hook) { SecurityManager sm = System.getSecurityManager(); if (sm != null) { sm.checkPermission(new RuntimePermission(\u0026#34;shutdownHooks\u0026#34;)); } return ApplicationShutdownHooks.remove(hook); } 再看ApplicationShutdownHooks\nclass ApplicationShutdownHooks { /* The set of registered hooks */ private static IdentityHashMap\u0026lt;Thread, Thread\u0026gt; hooks; static { try { Shutdown.add(1 /* shutdown hook invocation order */, false /* not registered if shutdown in progress */, new Runnable() { public void run() { runHooks(); } } ); hooks = new IdentityHashMap\u0026lt;\u0026gt;(); } catch (IllegalStateException e) { // application shutdown hooks cannot be added if  // shutdown is in progress.  hooks = null; } } private ApplicationShutdownHooks() {} /* Add a new shutdown hook. Checks the shutdown state and the hook itself, * but does not do any security checks. */ static synchronized void add(Thread hook) { if(hooks == null) throw new IllegalStateException(\u0026#34;Shutdown in progress\u0026#34;); if (hook.isAlive()) throw new IllegalArgumentException(\u0026#34;Hook already running\u0026#34;); if (hooks.containsKey(hook)) throw new IllegalArgumentException(\u0026#34;Hook previously registered\u0026#34;); hooks.put(hook, hook); } /* Remove a previously-registered hook. Like the add method, this method * does not do any security checks. */ static synchronized boolean remove(Thread hook) { if(hooks == null) throw new IllegalStateException(\u0026#34;Shutdown in progress\u0026#34;); if (hook == null) throw new NullPointerException(); return hooks.remove(hook) != null; } /* Iterates over all application hooks creating a new thread for each * to run in. Hooks are run concurrently and this method waits for * them to finish. */ static void runHooks() { Collection\u0026lt;Thread\u0026gt; threads; synchronized(ApplicationShutdownHooks.class) { threads = hooks.keySet(); hooks = null; } for (Thread hook : threads) { hook.start(); } for (Thread hook : threads) { while (true) { try { hook.join(); break; } catch (InterruptedException ignored) { } } } } } 2, 自定义hooks 往往在应用程序里已经有了一些第三方注册好的hook, 当我们要对将自己自定义的hook放到其他hook之前执行，需要强制干预ApplicationShutdownHooks的hooks，可以通过反射来做\nString className = \u0026#34;java.lang.ApplicationShutdownHooks\u0026#34;; Class\u0026lt;?\u0026gt; clazz = Class.forName(className); Field field = clazz.getDeclaredField(\u0026#34;hooks\u0026#34;); field.setAccessible(true); // 先反射拿到其他的hook, IdentityHashMap\u0026lt;Thread, Thread\u0026gt; otherHookMap = (IdentityHashMap\u0026lt;Thread, Thread\u0026gt;) field.get(clazz); // 将otherHookMap 中 各个hook封装一个延时（比如3s）  // 再将自定义hook 加入 Runtime.getRuntime().addShutdownHook(new Thread(()-\u0026gt;{ System.out.println(\u0026#34;exec my hook\u0026#34;); })); 3, ShenYu中应用 ShenYu项目下线时，会将一些client服务，比如注册中心客户端，优雅停掉。\npublic class ShenyuClientShutdownHook { // 钩子名称  private static String hookNamePrefix = \u0026#34;ShenyuClientShutdownHook\u0026#34;; private static AtomicInteger hookId = new AtomicInteger(0); private static Properties props; private static AtomicBoolean delay = new AtomicBoolean(false); //等待加入延时，但是还没有加入的hooks  private static IdentityHashMap\u0026lt;Thread, Thread\u0026gt; delayHooks = new IdentityHashMap\u0026lt;\u0026gt;(); //已经做了延时的hook  private static IdentityHashMap\u0026lt;Thread, Thread\u0026gt; delayedHooks = new IdentityHashMap\u0026lt;\u0026gt;(); /** * Add shenyu client shutdown hook. * * @param result ShenyuClientRegisterRepository * @param props Properties */ public static void set(final ShenyuClientRegisterRepository result, final Properties props) { String name = hookNamePrefix + \u0026#34;-\u0026#34; + hookId.incrementAndGet(); Runtime.getRuntime().addShutdownHook(new Thread(() -\u0026gt; { result.close(); }, name)); log.info(\u0026#34;Add hook {}\u0026#34;, name); ShenyuClientShutdownHook.props = props; } /** * Delay other shutdown hooks.// 将其他的hooks做延时处理 */ public static void delayOtherHooks() { if (!delay.compareAndSet(false, true)) { return; } TakeoverOtherHooksThread thread = new TakeoverOtherHooksThread(); thread.start(); } /** * Delay other shutdown hooks thread. * 延时处理的线程， 1，反射拿到其他hooks *\t2，遍历hooks,通过线程包装原先的hook线程任务，并在其中加3s的延时 *\t3，将原先的hook线程任务重新加入到ApplicationShutdownHooks中 */ private static class TakeoverOtherHooksThread extends Thread { @SneakyThrows @Override public void run() { int shutdownWaitTime = Integer.parseInt(props.getProperty(\u0026#34;shutdownWaitTime\u0026#34;, \u0026#34;3000\u0026#34;)); int delayOtherHooksExecTime = Integer.parseInt(props.getProperty(\u0026#34;delayOtherHooksExecTime\u0026#34;, \u0026#34;2000\u0026#34;)); Class\u0026lt;?\u0026gt; clazz = Class.forName(props.getProperty(\u0026#34;applicationShutdownHooksClassName\u0026#34;, \u0026#34;java.lang.ApplicationShutdownHooks\u0026#34;)); Field field = clazz.getDeclaredField(props.getProperty(\u0026#34;applicationShutdownHooksFieldName\u0026#34;, \u0026#34;hooks\u0026#34;)); field.setAccessible(true); IdentityHashMap\u0026lt;Thread, Thread\u0026gt; hooks = (IdentityHashMap\u0026lt;Thread, Thread\u0026gt;) field.get(clazz); long s = System.currentTimeMillis(); while (System.currentTimeMillis() - s \u0026lt; delayOtherHooksExecTime) { for (Iterator\u0026lt;Thread\u0026gt; iterator = hooks.keySet().iterator(); iterator.hasNext();) { Thread hook = iterator.next(); if (hook.getName().startsWith(hookNamePrefix)) {// 为当前自己自定义的hook 就不做延时  continue; } if (delayHooks.containsKey(hook) || delayedHooks.containsKey(hook)) { continue; } Thread delayHook = new Thread(() -\u0026gt; {// 通过线程包装原先的hook线程任务，并在其中加3s的延时  log.info(\u0026#34;sleep {}ms\u0026#34;, shutdownWaitTime); try { TimeUnit.MILLISECONDS.sleep(shutdownWaitTime); } catch (InterruptedException ex) { ex.printStackTrace(); } hook.run(); }, hook.getName()); delayHooks.put(delayHook, delayHook);// 加入的待加入延时集合  iterator.remove();// 从原来的ApplicationShutdownHooks的hooks集合中去掉  } for (Iterator\u0026lt;Thread\u0026gt; iterator = delayHooks.keySet().iterator(); iterator.hasNext();) { Thread delayHook = iterator.next(); Runtime.getRuntime().addShutdownHook(delayHook);// 加入到ApplicationShutdownHooks的hooks集合中去掉  delayedHooks.put(delayHook, delayHook); iterator.remove(); log.info(\u0026#34;hook {} will sleep {}ms when it start\u0026#34;, delayHook.getName(), shutdownWaitTime); } TimeUnit.MILLISECONDS.sleep(100); } hookNamePrefix = null; hookId = null; props = null; delayHooks = null; delayedHooks = null; } } } ","date":"2021-05-19T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/%E4%BC%98%E9%9B%85%E5%81%9C%E6%9C%BA/","title":"优雅停机"},{"content":"jvm垃圾回收 垃圾回收算法 针对 堆内存回收；\n方法区：\n  java 8 前 永久区（也参与垃圾回收，一样的算法，省事了，但是有默认最大内存限制，容易oom）\n  java 8 彻底抛弃了 永久区，叫元数据区，使用本地内存\n  分代收集理论 新生代\n老年代\n跨代引用： Remember set\n标记清除 产生内存碎片，内存分配复杂了。 可能需要类似硬盘的 “分区空闲分配链表” 等复杂方式解决\nCms搜集器在old 区回收时采用， 但是内存碎片达到一定量，会采取一次标记清理。（和稀泥的做法，结合两者，）\n标记复制 一般用于新生代回收: Serial ParNew 的新生代采用该算法\nĒden scurvivor scurvivor 8:1:1\n对象存活率较高时，需要更多的复制操作，效率会降低\n标记整理 用于old区：\n相对于 标记清除， 标记后，需要移动：将存活的对象移动到内存区域的一端。\n  移动：增大的延迟，stw时间长些，但解决了内存碎片，内存分配复杂的问题，可以提高吞吐量。\n  不移动：降低了延迟，但内存碎片，内存分配复杂， 吞吐量有所下降。\n  垃圾回收器 ","date":"2021-03-14T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/jvm%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","title":"jvm垃圾回收"},{"content":"相关概念  计算存储分离   安装测试 # 下载 wget https://mirrors.bfsu.edu.cn/apache/pulsar/pulsar-2.7.1/apache-pulsar-2.7.1-bin.tar.gz tar xvfz apache-pulsar-2.7.1-bin.tar.gz cd apache-pulsar-2.7.1 # 单机启动 bin/pulsar standalone # 消费消息 bin/pulsar-client consume my-topic -s \u0026#34;first-subscription\u0026#34; # 生产消息 bin/pulsar-client produce my-topic --messages \u0026#34;hello-pulsar\u0026#34; 相关资料 官网快速启动\n","date":"2021-02-23T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/pulsar/","title":"pulsar"},{"content":"相关概念 二代mq, 纯java开发，和kafka无本质区别\n安装测试 # 下载 4.8.0 wget https://downloads.apache.org/rocketmq/4.8.0/rocketmq-all-4.8.0-bin-release.zip unzip rocketmq-all-4.8.0-bin-release.zip #运行命名服务， 替代kafka的zk nohup sh bin/mqnamesrv \u0026amp; #查看日志 tail -f ~/logs/rocketmqlogs/namesrv.log #运行broker nohup sh bin/mqbroker -n localhost:9876 \u0026amp; #查看日志 tail -f ~/logs/rocketmqlogs/broker.log # 发送消息 export NAMESRV_ADDR=localhost:9876 sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer #消费消息 sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer 相关资料 官网快速开始\n中文文档\n","date":"2021-02-20T09:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/rocketmq/","title":"rocketMQ"},{"content":"Rabbitmq相关概念 一代mq，erlang开发， 改进activemq\n Publisher 消息生产者, 返送消息时指定exchange 和routing key, 即可以将消息路由到匹配的queue中 Routing key Binding 通过routing key 将queue和exchange绑定 Exchange 工具人。。交易所。。代理  FanoutExchange: 将消息分发到所有的绑定队列，无routingkey的概念，发送时不指定routing key HeadersExchange ：通过添加属性key-value匹配 DirectExchange: 按照routingkey分发到指定队列 TopicExchange:多关键字匹配 正则   Consumer  Docker方式安装运行 docker pull rabbitmq:management docker run -itd --name rabbitmq-test -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin -p 15672:15672 -p 5672:5672 rabbitmq:management docker exec -it rabbitmq-test /bin/bash ","date":"2021-02-19T10:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/rabbitmq/","title":"rabbitMQ"},{"content":"康威定律：组织决定产品形态 第一定律 组织设计的产品/设计等价于这个组织的沟通结构。\n","date":"2021-01-13T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E5%BA%B7%E5%A8%81%E5%AE%9A%E5%BE%8B/","title":"康威定律"},{"content":"Synchronized  Object.wait()：释放当前对象锁，并进入阻塞队列(wait set) Object.notify()：唤醒当前对象阻塞队列(wait set)里的任一线程（并不保证唤醒哪一个） Object.notifyAll()：唤醒当前对象阻塞队列(wait set)里的所有线程, 进到entry set 去竞争锁  为什么wait,notify和notifyAll要与synchronized一起使用？ Wait 只有通过synchronized拿到锁，才能进入wait set\nnotify notifyAll只有通过synchronized拿到锁，才能去唤醒 wait set 里线程 到entry set\nobject monitor 对象在内存中的存储 Markword 32位jvm 结构如下： 重量级锁即为 Synchronized 的锁\n锁升级 参考 https://mp.weixin.qq.com/s/2yxexZUr5MWdMZ02GCSwdA\n","date":"2020-12-15T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/synchronized%E9%94%81/","title":"Synchronized锁"},{"content":"安装 docker docker pull hazelcast/hazelcast docker run -e HZ_NETWORK_PUBLICADDRESS=192.168.3.14:5701 -p 5701:5701 hazelcast/hazelcast:$HAZELCAST_VERSION docker run -e HZ_NETWORK_PUBLICADDRESS=192.168.3.14:5702 -p 5702:5701 hazelcast/hazelcast:$HAZELCAST_VERSION docker run -p 8080:8080 hazelcast/management-center ","date":"2020-12-14T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/hazelcast/","title":"hazelcast"},{"content":"一.业务数据缓存* 经典用法。\n  通用数据缓存，string，int，list，map等。\n 验证码等    实时热数据，最新500条数据。\n 如热搜新闻。。    会话缓存，token缓存等。\n spring-session-data-redis sesion共享    二.业务数据处理   非严格一致性要求的数据\n  评论，点击，点赞等。\nset key 0 incr key // incr readcount::{帖子id} 每阅读一次 get key // get readcount::{帖子id} 获取阅读量     业务数据去重\n 订单处理的幂等校验等。 如订单id放到redis 的set中去重复， bitmap 等    业务数据排序\n 排名，排行榜等。 使用sortedset    三.全局一致计数 *   全局流控计数\n多个服务节点使用同一个redis的计数。\n  秒杀的库存计算\n和全局计数类似\n  抢红包\n和全局计数类似\n  全局ID生成\n例如：userId, 直接获取一段userId的最大值，缓存到本地服务慢慢累加，快到了userId的最大值时，再去获取一段，一个用户服务宕机了，也就一小段userId没有用到。 用数据库也可以。\nset userId 0 incr usrId //返回1 incrby userId 1000 //返回10001   四.高效统计计数   id去重，记录访问ip等\n全局bitmap操作\n  UV、PV等访问量==\u0026gt;非严格一致性要求\n  五.发布订阅与Stream   Pub-Sub 模拟队列\n127.0.0.1:6379\u0026gt; subscribe fzb Reading messages... (press Ctrl-C to quit) 1) \u0026quot;subscribe\u0026quot; 2) \u0026quot;fzb\u0026quot; 3) (integer) 1 1) \u0026quot;message\u0026quot; 2) \u0026quot;fzb\u0026quot; 3) \u0026quot;fff\u0026quot; 1) \u0026quot;message\u0026quot; 2) \u0026quot;fzb\u0026quot; 3) \u0026quot;ffff\u0026quot; 127.0.0.1:6379\u0026gt; publish fzb fff (integer) 1 127.0.0.1:6379\u0026gt; publish fzb ffff (integer) 1   Redis Stream 是 Redis 5.0 版本新增加的数据结构。 Redis Stream 主要用于消息队列(MQ，Message Queue)。\n具体可以参考 https://www.runoob.com/redis/redis-stream.html\n  六.分布式锁* 1、获取锁\u0026ndash;单个原子性操作\n SET dlock my_random_value NX PX 30000  127.0.0.1:6379\u0026gt; set myLock 1 NX PX 30000 OK 127.0.0.1:6379\u0026gt; set myLock 1 NX PX 30000 (nil) 2、释放锁\u0026ndash;lua脚本-保证原子性+单线程，从而具有事务性 . =\u0026gt; 因为内存操作是单线程的\nif redis.call(\u0026quot;get\u0026quot;,KEYS[1]) == ARGV[1] then return redis.call(\u0026quot;del\u0026quot;,KEYS[1]) else return 0 end  关键点:原子性、互斥、超时  更多细节：https://www.cnblogs.com/yunlongaimeng/p/10266690.html\n","date":"2020-12-13T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/redis%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/","title":"redis应用场景"},{"content":"redis线程 redis做为一个进程，一直是多线程的。处理io和处理内存的是不同线程。\n  io线程\n  redis6之前(2020.05)：io处理是单线程\n  redis6：io处理多线程，采用nio模型 =\u0026gt; 主要的性能提升点\n    内存处理线程\n 单线程 =\u0026gt;高性能核心，不用考虑线程调度    压测redis-benchmark  环境mac 4核8g  mokernetdeMac-mini:redis-6.0.9 mokernet$ ./bin/redis-benchmark -n 100000 -c 32 -t SET,GET,INCR,HSET,LPUSH,MSET -q SET: 121065.38 requests per second GET: 118764.84 requests per second INCR: 117508.81 requests per second LPUSH: 123001.23 requests per second HSET: 123915.74 requests per second MSET (10 keys): 96711.80 requests per second redis的5种基本数据结构 https://redis.io/commands\n1.字符串(string) 简单来说就是三种:int、string、byte[]\nRedis中字符串类型的value最多可以容纳的数据长度是512M\nset/get/setnx/getset/del/exists/append incr/decr/incrby/decrby 2.散列(hash) Redis中的Hash类型可以看成具有String key 和String value的map容器。\nhset/hget/hmset/hmget/hgetall/hdel/hincrby hexists/hlen/hkeys/hvals  hmset 相对于hset可一次设置多个键值对 hmget 相对于hget可一次获取多个键的值  3.列表(list) java的LinkedList\n在Redis中，List类型是按照插入顺序排序的字符串链表。和数据结构中的普通链表 一 样，我们可以在其头部(Left)和尾部(Right)添加新的元素。在插入时，如果该键并不存 在，Redis将为该键创建一个新的链表。与此相反，如果链表中所有的元素均被移除， 那么该键也将会被从数据库中删除。\nlpush/rpush/lrange/lpop/rpop 4.集合(set) java的set，不重复的list\n在redis中，可以将Set类型看作是没有排序的字符集合，和List类型一样，我们也可以 在该类型的数值上执行添加、删除和判断某一元素是否存在等操作。这些操作的时间复 杂度为O(1),即常量时间内完成依次操作。\n和List类型不同的是，Set集合中不允许出现重复的元素。\nsadd/srem/smembers/sismember 类比java中set的add, remove, all, contains, spop key [count] 随机返回集合中一个或多个 移除 SRANDMEMBER key [count] 返回集合中一个或多个随机数，不移除, 抽奖 sdiff/sinter/sunion 集合求差集，求交集，求并集 5.有序集合(sorted set) 按权重排序\nsortedset在set基础上给每个元素加了个分数score。\nredis 正是通过分数来为集合的成员进行从小到大的排序。sortedset中分数是可以重复的。\nzadd key score member score2 member2... : 将成员以及该成员的分数存放到sortedset中 zscore key member : 返回指定成员的分数 zcard key : 获取集合中成员数量 zrem key member [member...] : 移除集合中指定的成员，可以指定多个成员 zrange key start end [withscores] : 获取集合中脚注为start-end的成员，[withscores]参数表明返回的成员 包含其分数 zrevrange key start stop [withscores] : 按照分数从大到小的顺序返回索引从start到stop之间的所有元素 (包含两端的元素) zremrangebyrank key start stop : 按照排名范围删除元素 redis高级数据结构 1.Bitmaps bitmaps不是一个真实的数据结构。而是String类型上的一组面向bit操作的集合。由于 strings是二进制安全的blob，并且它们的最大长度是512m，所以bitmaps能最大设置 2^32个不同的bit。\nsetbit/getbit/bitop/bitcount/bitpos 可用作集中式的冥等去重 数据压缩在字节位上，极大的节约了空间\n2.HyperLogLog Redis 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。\n在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。\n但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。\npfadd 添加 pfcount 获得基数值 pfmerge 合并多个key 127.0.0.1:6379\u0026gt; pfadd pf1 1 3 1 4 1 5 (integer) 1 127.0.0.1:6379\u0026gt; pfcount pf1 (integer) 4 127.0.0.1:6379\u0026gt; pfadd pf2 3 4 5 6 7 (integer) 1 127.0.0.1:6379\u0026gt; pfcount pf2 (integer) 5 127.0.0.1:6379\u0026gt; pfmerge pf3 pf1 pf2 OK 127.0.0.1:6379\u0026gt; pfcount pf3 (integer) 6 应用场景说明：\n 基数不大，数据量不大就用不上，会有点大材小用浪费空间 有局限性，就是只能统计基数数量，而没办法去知道具体的内容是什么 和bitmap相比，属于两种特定统计情况，简单来说，HyperLogLog 去重比 bitmap 方便很多 一般可以bitmap和hyperloglog配合使用，bitmap标识哪些用户活跃，hyperloglog计数  一般使用：\n 统计注册 IP 数 统计每日访问 IP 数 统计页面实时 UV 数 统计在线用户数 统计用户每天搜索不同词条的个数  3.GEO geoadd/geohash/geopos/geodist/georadius/georadiusbymember Redis的GEO特性在 Redis3.2版本中推出，这个功能可以将用户给定的地理位置(经 度和纬度)信息储存起来，并对这些信息进行操作。\nRedis Lua   类比openrestry = nginx + lua jit\n  类似于数据库的存储过程，mongodb的js脚本\n  redis内存操作的单线程 使得一段lua脚本之心具有：原子性，操作不会被打断，保证了事务\n  直接执行 eval \u0026ldquo;return\u0026rsquo;hello java'\u0026rdquo; 0 eval \u0026ldquo;redis.call(\u0026lsquo;set\u0026rsquo;,KEYS[1],ARGV[1])\u0026rdquo; 1 lua-key lua-value\n127.0.0.1:6379\u0026gt; eval \u0026quot;return'hello java'\u0026quot; 0 \u0026quot;hello java\u0026quot; 127.0.0.1:6379\u0026gt; eval \u0026quot;redis.call('set',KEYS[1],ARGV[1])\u0026quot; 1 ff zz (nil) 127.0.0.1:6379\u0026gt; get ff \u0026quot;zz\u0026quot;   预编译 script load script脚本片段 返回一个SHA-1签名 shastring evalsha shastring keynum [key1 key2 key3 \u0026hellip;] [param1 param2 param3 \u0026hellip;]\n127.0.0.1:6379\u0026gt; script load \u0026quot;redis.call('set',KEYS[1],ARGV[1])\u0026quot; \u0026quot;7cfb4342127e7ab3d63ac05e0d3615fd50b45b06\u0026quot; 127.0.0.1:6379\u0026gt; evalsha 7cfb4342127e7ab3d63ac05e0d3615fd50b45b06 1 fff zzz (nil) 127.0.0.1:6379\u0026gt; get fff \u0026quot;zzz\u0026quot;     redis pipeline 使用管道一次执行多个命令，多个命令的结果一起返回，减少每个命令来回建立链接，来回响应的时间。\nhttps://redis.io/topics/pipelining\nredis事务   开启事务:multi\n  命令入队\n  执行事务:exec\n  撤销事务:discard\n  Watch 监控事务： watch 一个key，发生变化则事务失败\n  unwatch 取消监听\n  127.0.0.1:6379\u0026gt; multi OK 127.0.0.1:6379\u0026gt; set ff ff QUEUED 127.0.0.1:6379\u0026gt; set fff fff QUEUED 127.0.0.1:6379\u0026gt; exec 1) OK 2) OK redis管道 redis备份恢复机制 RDB方式   快照恢复，类似mysql的frm.等数据：备份当前瞬间 Redis 在内存中的数据记录\n  save后在数据目录生成dump.rdb\n  bgsave 异步执行备份\n  恢复：将备份文件 (dump.rdb) 移动到 redis 数据目录并启动服务即可\n  redis.conf中\n#Redis默认配置文件中提供了三个备份条件： ##指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 save 900 1 save 300 10 save 60 10000 AOF方式  追加文件（Append-Only File，AOF） 类比mysql的binlog 配置为 always，其含义为当 Redis 执行 命令的时候，则同时同步到 AOF 文件，这样会使得 Redis 同步刷新 AOF 文件，造成 缓慢。而采用 evarysec 则代表每秒同步一次命令到 AOF 文件  appendfilename \u0026quot;appendonly.aof\u0026quot; # appendfsync always appendfsync everysec # appendfsync no......  恢复：自动加载  redis性能优化 slowlog get 10 几个缓存问题 缓存穿透  现象：key对应的数据在数据源并不存在，每次针对此key的请求从缓存获取不到，请求都会到数据源，从而可能压垮数据源。比如用一个不存在的用户id获取用户信息，不论缓存还是数据库都没有，若黑客利用此漏洞进行攻击可能压垮数据库 解决：  采用布隆过滤器：将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力，详见https://www.cnblogs.com/rinack/p/9712477.html 直接缓存空结果：如果一个查询返回的数据为空，仍然把这个空结果进行缓存。    缓存击穿  现象：key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 解决：使用互斥分布式锁进行从DB加载数据，其他的请求继续重试缓存。使用锁可能会死锁、线程池阻塞等问题，针对高热点key，最好是在并发量最小的时候，写定时器更新key的过期时间  缓存雪崩  现象：当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，也会给后端系统(比如DB)带来很大压力。与缓存击穿的区别在于这里针对很多key缓存，前者则是某一个key。 解决：  最简单尽量将过期时间分散开来：可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机， 设置过期标志更新缓存:  1，新加一个缓存key的标记。缓存数据key的value时，同时缓存key_sign, key_sign的过期时间小于key的过期时间。 2 在查询缓存时，先判断key_sign是否过期，a，如果过期，先直接返回value，再启用异步线程去更新key_sign以及加载db到key的value,重新设置两个的过期时间。b. 没有过期，直接返回value      ","date":"2020-12-11T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/redis%E5%9F%BA%E7%A1%80/","title":"redis基础"},{"content":"gateway整体逻辑 1.流程图 2.几个关键的类  org.springframework.web.reactive.DispatcherHandler: 请求分发处理器；    Spring WebFlux 的访问入口； 类似于spring mvc DispatcherServlet,可以类比spring mvc 接收到请求; DispatcherHandler匹配 HandlerMapping此处会匹配到scg的RoutePredicateHandlerMapping     org.springframework.cloud.gateway.handler.RoutePredicateHandlerMapping：HandlerMapping的实现；\n  通过RouteLocator匹配 Route: getHandlerInternal方法调用lookupRoute()方法，通过routeLocator获取所有配置的route,通过里面的Predicate配置来遍历判断找出符合的Route getHandlerInternal中返回FilteringWebHandler     org.springframework.cloud.gateway.handler.FilteringWebHandler: WebHandler的实现；\n  FilteringWebHandler被RoutePredicateHandlerMapping返回后，在DispatcherHandler中被SimpleHandlerAdapter执行handle方法； 责任链模式：获取Route的GatewayFilter数组，创建DefaultGatewayFilterChain的过滤链；链式调用GatewayFilter     3.项目结构 核心module为spring-cloud-gateway-server\n actuate: 实现springboot actuator的端点，暴露route filter predicate等信息 config: 使用springboot的配置注解的各类配置类 discover：通过注册中心获取路由Route的核心功能配置类及实现类 event：实现ApplicationEvent的事件类，例如路由刷新事件RefreshRoutesEvent filter: 包含特定路由的GatewayFilterFactory，GatewayFiler以及全局的GlobalFilter handler: 包含匹配route的断言工厂AbstractRoutePredicateFactory的所有默认实现，以及核心类FilteringWebHandler及RoutePredicateHandlerMapping route：路由的定义类，及路由定位类CachingRouteLocator的所有实现，及路由定义定位类CompositeRouteDefinitionLocator的所有实现，路由存储接口RouteDefinitionRepository及其所有实现 support：工具类；如HTTP协议处理，组件名处理，日期转换等  ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/01.gateway%E6%95%B4%E4%BD%93%E9%80%BB%E8%BE%91/","title":"01.gateway整体逻辑"},{"content":"reactor 使用 ![image (1)](https://gitee.com/fengzhenbing/picgo/raw/master/image (1).png)\nWebflux 模块的名称是 spring-webflux，名称中的 Flux 来源于 Reactor 中的类 Flux。 Reactor 两个核心概念做一些澄清，一个是Mono，另一个是Flux\n Flux ：表示的是包含 0 到 N 个元素的异步序列。包含三个类型    正常的包含元素的消息 序列结束的消息 序列出错的消息    Mono： 表示的是包含 0 或者 1 个元素的异步序列。该序列中同样可以包含与 Flux 相同的三种类型的消息通知。 示例代码：    https://github.com/fengzhenbing/spring-cloud-gateway-demo/blob/master/demo-gateway/src/main/java/org/fzb/demo/gateway/RectorController.java   ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/02.reactor%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0/","title":"02.reactor响应式编程学习"},{"content":"scg NettyWebServer启动过程 spring cloud gateway(下面简称scg)依赖spring webflux, 而spring webflux依赖于reactor-netty,也就是scg启动过程中最终会启动netty做为服务器。 springboot中定义一下几种服务器：\n1 启动ReactiveWebServerApplicationContext 从springboot启动开始分析\nSpringApplication.run(GatewayApplication.class, args); 设置webApplicationType的值：REACTIVE还是servlet的。\npublic SpringApplication(ResourceLoader resourceLoader, Class\u0026lt;?\u0026gt;... primarySources) { this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \u0026#34;PrimarySources must not be null\u0026#34;); this.primarySources = new LinkedHashSet\u0026lt;\u0026gt;(Arrays.asList(primarySources)); this.webApplicationType = WebApplicationType.deduceFromClasspath();//fzb 通过类路径中类，推测web应用类型：REACTIVE还是servlet的。 \tsetInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass(); } 再看deduceFromClasspath方法：判断DispatcherHandler存在还是DispatcherServlet存在\nstatic WebApplicationType deduceFromClasspath() {//fzb 判断DispatcherHandler存在还是DispatcherServlet存在 \tif (ClassUtils.isPresent(WEBFLUX_INDICATOR_CLASS, null) \u0026amp;\u0026amp; !ClassUtils.isPresent(WEBMVC_INDICATOR_CLASS, null) \u0026amp;\u0026amp; !ClassUtils.isPresent(JERSEY_INDICATOR_CLASS, null)) { return WebApplicationType.REACTIVE; } for (String className : SERVLET_INDICATOR_CLASSES) { if (!ClassUtils.isPresent(className, null)) { return WebApplicationType.NONE; } } return WebApplicationType.SERVLET; } 在此scg项目有DispatcherHandler类，所以是REACTIVE，响应式的。 下面接着创建相应的响应式容器\nprotected ConfigurableApplicationContext createApplicationContext() { Class\u0026lt;?\u0026gt; contextClass = this.applicationContextClass; if (contextClass == null) { try { switch (this.webApplicationType) {//fzb 根据webApplicationType类型创建不同的context容器 \tcase SERVLET: contextClass = Class.forName(DEFAULT_SERVLET_WEB_CONTEXT_CLASS); break; case REACTIVE: contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS);//fzb scg创建的容器 \tbreak; default: contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); } } catch (ClassNotFoundException ex) { throw new IllegalStateException( \u0026#34;Unable create a default ApplicationContext, please specify an ApplicationContextClass\u0026#34;, ex); } } return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass); } 各类容器选择总结\n2 ReactiveWebServerApplicationContext刷新 先创建了 org.springframework.boot.autoconfigure.web.reactive.ReactiveWebServerFactoryAutoConfiguration中配置了EmbeddedNetty，初始化了NettyReactiveWebServerFactory\nReactiveWebServerApplicationContext容器刷新时通过ReactiveWebServerFactory创建WebServerManager\nprotected void onRefresh() { super.onRefresh(); try { createWebServer();//fzb 创建WebServerManager \t} catch (Throwable ex) { throw new ApplicationContextException(\u0026#34;Unable to start reactive web server\u0026#34;, ex); } } private void createWebServer() {//fzb 通过ReactiveWebServerFactory创建WebServerManager \tWebServerManager serverManager = this.serverManager; if (serverManager == null) { String webServerFactoryBeanName = getWebServerFactoryBeanName(); ReactiveWebServerFactory webServerFactory = getWebServerFactory(webServerFactoryBeanName);//fzb 获取EmbeddedNetty配置的ReactiveWebServerFactory \tboolean lazyInit = getBeanFactory().getBeanDefinition(webServerFactoryBeanName).isLazyInit(); this.serverManager = new WebServerManager(this, webServerFactory, this::getHttpHandler, lazyInit);//fzb 创建 \tgetBeanFactory().registerSingleton(\u0026#34;webServerGracefulShutdown\u0026#34;, new WebServerGracefulShutdownLifecycle(this.serverManager)); getBeanFactory().registerSingleton(\u0026#34;webServerStartStop\u0026#34;, new WebServerStartStopLifecycle(this.serverManager)); } initPropertySources(); } 再看WebServerManager初始化时，再通过ReactiveWebServerFactory 初始化创建NettyWebServer\nWebServerManager(ReactiveWebServerApplicationContext applicationContext, ReactiveWebServerFactory factory, Supplier\u0026lt;HttpHandler\u0026gt; handlerSupplier, boolean lazyInit) { this.applicationContext = applicationContext; Assert.notNull(factory, \u0026#34;Factory must not be null\u0026#34;); this.handler = new DelayedInitializationHttpHandler(handlerSupplier, lazyInit); this.webServer = factory.getWebServer(this.handler);//fzb scg中创建NettyWebServer \t} 下面是ReactiveWebServerFactory初始化创建NettyWebServer的过程，实际是创建reactor.netty 的HttpServer，通过适配器模式ReactorHttpHandlerAdapter，适配为NettyWebServer 返回\npublic WebServer getWebServer(HttpHandler httpHandler) { HttpServer httpServer = createHttpServer();//fzb 创建reactor.netty 的HttpServer \tReactorHttpHandlerAdapter handlerAdapter = new ReactorHttpHandlerAdapter(httpHandler);//通过ReactorHttpHandlerAdapter适配器模式适配 \tNettyWebServer webServer = new NettyWebServer(httpServer, handlerAdapter, this.lifecycleTimeout, getShutdown());//HttpServer 适配为NettyWebServer \twebServer.setRouteProviders(this.routeProviders); return webServer; } 3 Netty的ServerBootstrap启动 继续跟进createHttpServer()\nprivate HttpServer createHttpServer() { HttpServer server = HttpServer.create();//fzb 创建httpServer  .... } 跟进HttpServer.create()，如下为\npublic static HttpServer create() { return HttpServerBind.INSTANCE; } 查看HttpServerBind初始化过程，实际最终创建的是TcpServer\nstatic final TcpServer DEFAULT_TCP_SERVER = TcpServer.create();// 创建TcpServer  HttpServerBind() { this(DEFAULT_TCP_SERVER); } 再跟进TcpServer\npublic static TcpServer create() { return TcpServerBind.INSTANCE; } 千呼万唤始出来 TcpServerBind,\nTcpServerBind() { this.serverBootstrap = createServerBootstrap();//创建启动Netty的服务端serverBootstrap \tBootstrapHandlers.channelOperationFactory(this.serverBootstrap, TcpUtils.TCP_OPS); } 至此找到了scg启动时最终启动的netty server.\nHttpServer为reactor netty项目中的 参考 Reactor Netty参考指南 https://projectreactor.io/docs/netty/release/reference/index.html\n","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/03.scg-nettywebserver%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B/","title":"03.scg NettyWebServer启动过程"},{"content":"一次请求的执行过程 ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/04.scg-%E4%B8%80%E6%AC%A1%E8%AF%B7%E6%B1%82%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/","title":"04.scg 一次请求的执行过程"},{"content":"route路由的配置加载  主要在sprng-cloud-gateway-server的route包定义路由相关的定义，构建和加载\n![路由](https://gitee.com/fengzhenbing/picgo/raw/master/image (2).png)\n0.相关配置  通过springboot spi方式，springboot会启动spring.factories中配置的 org.springframework.cloud.gateway.discovery.GatewayDiscoveryClientAutoConfiguration  org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ org.springframework.cloud.gateway.config.GatewayAutoConfiguration,\\ org.springframework.cloud.gateway.discovery.GatewayDiscoveryClientAutoConfiguration,\\ ... 1.路由定义  路由定义RouteDefinition  public class RouteDefinition { private String id; @NotEmpty @Valid//fzb 断言定义数组  private List\u0026lt;PredicateDefinition\u0026gt; predicates = new ArrayList\u0026lt;\u0026gt;(); @Valid//fzb 过滤器定义数组  private List\u0026lt;FilterDefinition\u0026gt; filters = new ArrayList\u0026lt;\u0026gt;(); @NotNull//fzb 路由路径  private URI uri; ... }  路由定义定位器  获取路由的定义，负责读取上述路由定义配置 RouteDefinition，最终会通过路由定义生成路由\npublic interface RouteDefinitionLocator { //fzb 获取路由定义对象  Flux\u0026lt;RouteDefinition\u0026gt; getRouteDefinitions(); } 有以下实现：\n![image (3)](https://gitee.com/fengzhenbing/picgo/raw/master/image (3).png)\n  CachingRouteDefinitionLocator: 它包装了CompositeRouteDefinitionLocator,缓存路由定义RouteDefinition列表   public class CachingRouteDefinitionLocator implements RouteDefinitionLocator, ApplicationListener{//fzb 事件监听RefreshRoutesEvent\nprivate static final String CACHE_KEY = \u0026quot;routeDefs\u0026quot;; private final RouteDefinitionLocator delegate; //fzb 路由定义flux private final Flux\u0026lt;RouteDefinition\u0026gt; routeDefinitions; //fzb 内存缓存RouteDefinition private final Map\u0026lt;String, List\u0026gt; cache = new ConcurrentHashMap\u0026lt;\u0026gt;(); ... //fzb 监听到路由刷新事件，重新获取路由并缓存到cache @Override public void onApplicationEvent(RefreshRoutesEvent event) { fetch().materialize().collect(Collectors.toList()) .doOnNext(routes -\u0026gt; cache.put(CACHE_KEY, routes)).subscribe(); }  \u0026hellip; }\n \u0026gt; * CompositeRouteDefinitionLocator: \u0026gt; 遍历执行所有RouteDefinitionLocator的查找路由定义方法,将找到的路由定义合并； 组合多种 RouteDefinitionLocator 的实现，为 routeDefinitions提供统一入口 ​```java public class CompositeRouteDefinitionLocator implements RouteDefinitionLocator { ... //fzb 组合多个路由定义定位器RouteDefinitionLocator private final Flux\u0026lt;RouteDefinitionLocator\u0026gt; delegates; // fzb 路由定义的id 生成器，默认UUID private final IdGenerator idGenerator; ... public CompositeRouteDefinitionLocator(Flux\u0026lt;RouteDefinitionLocator\u0026gt; delegates, IdGenerator idGenerator) { this.delegates = delegates; this.idGenerator = idGenerator;// 路由定义的id 生成器，默认UUID } @Override public Flux\u0026lt;RouteDefinition\u0026gt; getRouteDefinitions() { //遍历执行所有RouteDefinitionLocator的查找路由定义方法,将找到的路由定义合并 return this.delegates .flatMapSequential(RouteDefinitionLocator::getRouteDefinitions) .flatMap(routeDefinition -\u0026gt; { if (routeDefinition.getId() == null) { return randomId().map(id -\u0026gt; { routeDefinition.setId(id);// 设置uuid if (log.isDebugEnabled()) { log.debug( \u0026quot;Id set on route definition: \u0026quot; + routeDefinition); } return routeDefinition; }); } return Mono.just(routeDefinition); }); } ... }   PropertiesRouteDefinitionLocator: 从配置文件(GatewayProperties 例如，YML / Properties 等 ) 读取RouteDefinition   //fzb 从GatewayProperties读取路由定义  @Override public Flux\u0026lt;RouteDefinition\u0026gt; getRouteDefinitions() { return Flux.fromIterable(this.properties.getRoutes()); }   DiscoveryClientRouteDefinitionLocator 从注册中心如：Netflix Eureka， Consul 或 Zookeeper读取RouteDefinition   public DiscoveryClientRouteDefinitionLocator(ReactiveDiscoveryClient discoveryClient, DiscoveryLocatorProperties properties) { this(discoveryClient.getClass().getSimpleName(), properties); //fzb 通过服务发现客户端 ，比如eureka客户端从注册中心拿到所有的服务实例  serviceInstances = discoveryClient.getServices() .flatMap(service -\u0026gt; discoveryClient.getInstances(service).collectList()); } 将服务实例serviceInstances转为路由定义\n@Override public Flux\u0026lt;RouteDefinition\u0026gt; getRouteDefinitions() { SpelExpressionParser parser = new SpelExpressionParser(); Expression includeExpr = parser .parseExpression(properties.getIncludeExpression());//fzb 扩展点，可以通过properties 配置其他spel  Expression urlExpr = parser.parseExpression(properties.getUrlExpression()); ... }   RouteDefinitionRepository 从存储器(内存 / Redis / MySQL 等 )读取RouteDefinition，实现RouteDefinitionWriter接口； 提供删除和保存RouteDefinition的方法； 默认实现有：InMemoryRouteDefinitionRepository,没有RouteDefinitionRepository的实例，则默认用InMemoryRouteDefinitionRepository   public class InMemoryRouteDefinitionRepository implements RouteDefinitionRepository { private final Map\u0026lt;String, RouteDefinition\u0026gt; routes = synchronizedMap( new LinkedHashMap\u0026lt;String, RouteDefinition\u0026gt;());//fzb 内存中用 同步map 存储  @Override// fzb 保存路由到内存中 \tpublic Mono\u0026lt;Void\u0026gt; save(Mono\u0026lt;RouteDefinition\u0026gt; route) { return route.flatMap(r -\u0026gt; { if (StringUtils.isEmpty(r.getId())) { return Mono.error(new IllegalArgumentException(\u0026#34;id may not be empty\u0026#34;)); } routes.put(r.getId(), r);//保存 \treturn Mono.empty(); }); } @Override// fzb 从内存中删除路由 \tpublic Mono\u0026lt;Void\u0026gt; delete(Mono\u0026lt;String\u0026gt; routeId) { ... } @Override// fzb 获取路由定义 \tpublic Flux\u0026lt;RouteDefinition\u0026gt; getRouteDefinitions() { return Flux.fromIterable(routes.values()); } }   自定义路由存储器,redis存储：RedisRouteDefinitionRepository https://github.com/fengzhenbing/spring-cloud-gateway-demo/blob/master/demo-gateway/src/main/java/org/fzb/demo/gateway/route/RedisRouteDefinitionRepository.java   2.路由  路由Route  public class Route implements Ordered { private final String id; //fzb 路由地址  private final URI uri; //fzb 路由的优先级  private final int order; private final AsyncPredicate\u0026lt;ServerWebExchange\u0026gt; predicate; //fzb gatewayFilter列表  private final List\u0026lt;GatewayFilter\u0026gt; gatewayFilters; //fzb 元数据  private final Map\u0026lt;String, Object\u0026gt; metadata; ... }  路由定位器 RouteLocator 获取路由对象  public interface RouteLocator { //fzb 获取路由对象 \tFlux\u0026lt;Route\u0026gt; getRoutes(); } 类似RouteDefinitionLocator三个实现：\n  缓存：CachingRouteLocator： 缓存路由，查找时使用CompositeRouteLocator去查找 组合： CompositeRouteLocator：使用多个RouteLocator查找路由并合并 单个的 RouteDefinitionRouteLocator：最终使用RouteDefinitionLocator查找路由定义并转为路由对象Route 单个路由的 filters = GlobalFilter + 默认GatewayFilter + 本路由配置的GatewayFilter   public class RouteDefinitionRouteLocator implements RouteLocator, BeanFactoryAware, ApplicationEventPublisherAware { ... public RouteDefinitionRouteLocator(RouteDefinitionLocator routeDefinitionLocator, List\u0026lt;RoutePredicateFactory\u0026gt; predicates, List\u0026lt;GatewayFilterFactory\u0026gt; gatewayFilterFactories, GatewayProperties gatewayProperties, ConfigurationService configurationService) { this.routeDefinitionLocator = routeDefinitionLocator;//fzb //设置路由定义定位器 \tthis.configurationService = configurationService; initFactories(predicates);//fzb //初始化路由断言工厂 \tgatewayFilterFactories.forEach( factory -\u0026gt; this.gatewayFilterFactories.put(factory.name(), factory));//fzb //初始化网关过滤器 \tthis.gatewayProperties = gatewayProperties; } ... //fzb 获取路由对象： 先获取RouteDefinition，再转为 Route \t@Override public Flux\u0026lt;Route\u0026gt; getRoutes() { Flux\u0026lt;Route\u0026gt; routes = this.routeDefinitionLocator.getRouteDefinitions() .map(this::convertToRoute);//fzb RouteDefinition转为 Route  ... } //fzb 路由定义转为 路由 \tprivate Route convertToRoute(RouteDefinition routeDefinition) { //fzb 1,将本路由定义中各个断言 与运算 合并为一个 AsyncPredicate \tAsyncPredicate\u0026lt;ServerWebExchange\u0026gt; predicate = combinePredicates(routeDefinition); //fzb 2, 获取所有的默认的过滤器和本路由定义的过滤器 \tList\u0026lt;GatewayFilter\u0026gt; gatewayFilters = getFilters(routeDefinition); // 组装Route \treturn Route.async(routeDefinition).asyncPredicate(predicate) .replaceFilters(gatewayFilters).build(); } //fzb 过滤器定义filterDefinitions加载为过滤器GatewayFilter \t@SuppressWarnings(\u0026#34;unchecked\u0026#34;) List\u0026lt;GatewayFilter\u0026gt; loadGatewayFilters(String id, List\u0026lt;FilterDefinition\u0026gt; filterDefinitions) { ArrayList\u0026lt;GatewayFilter\u0026gt; ordered = new ArrayList\u0026lt;\u0026gt;(filterDefinitions.size()); for (int i = 0; i \u0026lt; filterDefinitions.size(); i++) { FilterDefinition definition = filterDefinitions.get(i); GatewayFilterFactory factory = this.gatewayFilterFactories .get(definition.getName()); if (factory == null) { throw new IllegalArgumentException( \u0026#34;Unable to find GatewayFilterFactory with name \u0026#34; + definition.getName()); } if (logger.isDebugEnabled()) { logger.debug(\u0026#34;RouteDefinition \u0026#34; + id + \u0026#34; applying filter \u0026#34; + definition.getArgs() + \u0026#34; to \u0026#34; + definition.getName()); } // @formatter:off \tObject configuration = this.configurationService.with(factory) .name(definition.getName()) .properties(definition.getArgs()) .eventFunction((bound, properties) -\u0026gt; new FilterArgsEvent( // TODO: why explicit cast needed or java compile fails \tRouteDefinitionRouteLocator.this, id, (Map\u0026lt;String, Object\u0026gt;) properties)) .bind(); // @formatter:on  // some filters require routeId \t// TODO: is there a better place to apply this? \tif (configuration instanceof HasRouteId) { HasRouteId hasRouteId = (HasRouteId) configuration; hasRouteId.setRouteId(id); } GatewayFilter gatewayFilter = factory.apply(configuration); if (gatewayFilter instanceof Ordered) { ordered.add(gatewayFilter); } else { ordered.add(new OrderedGatewayFilter(gatewayFilter, i + 1)); } } return ordered; } private List\u0026lt;GatewayFilter\u0026gt; getFilters(RouteDefinition routeDefinition) { List\u0026lt;GatewayFilter\u0026gt; filters = new ArrayList\u0026lt;\u0026gt;(); //fzb 1,加上默认的过滤器 \t// TODO: support option to apply defaults after route specific filters? \tif (!this.gatewayProperties.getDefaultFilters().isEmpty()) { filters.addAll(loadGatewayFilters(DEFAULT_FILTERS, new ArrayList\u0026lt;\u0026gt;(this.gatewayProperties.getDefaultFilters()))); } //fzb 2,加上本路由配置的过滤器 \tif (!routeDefinition.getFilters().isEmpty()) { filters.addAll(loadGatewayFilters(routeDefinition.getId(), new ArrayList\u0026lt;\u0026gt;(routeDefinition.getFilters()))); } //fzb 排序 按实现的Ordered接口 \tAnnotationAwareOrderComparator.sort(filters); return filters; } //fzb combinePredicates主要是对找出来的predicate进行and操作 \tprivate AsyncPredicate\u0026lt;ServerWebExchange\u0026gt; combinePredicates( RouteDefinition routeDefinition) { List\u0026lt;PredicateDefinition\u0026gt; predicates = routeDefinition.getPredicates(); if (predicates == null || predicates.isEmpty()) { // this is a very rare case, but possible, just match all \treturn AsyncPredicate.from(exchange -\u0026gt; true); } AsyncPredicate\u0026lt;ServerWebExchange\u0026gt; predicate = lookup(routeDefinition, predicates.get(0)); for (PredicateDefinition andPredicate : predicates.subList(1, predicates.size())) { AsyncPredicate\u0026lt;ServerWebExchange\u0026gt; found = lookup(routeDefinition, andPredicate); predicate = predicate.and(found);//fzb and 各个断言 与 合并为一个 AsyncPredicate \t} return predicate; } ... } 类比  Route -\u0026gt; RouteDefinition -\u0026gt;RouteDefinitionLocator -\u0026gt; xxxRouteDefinitionRepository Bean -\u0026gt; BeanDefinition -\u0026gt; BeanDefinitionRegistry -\u0026gt; DefaultListableBeanFactory#Map\u0026lt;String, BeanDefinition\u0026gt; beanDefinitionMap = new ConcurrentHashMap\u0026lt;\u0026gt;(256);  ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/05.route%E8%B7%AF%E7%94%B1%E7%9A%84%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/","title":"05.route路由的配置加载"},{"content":"route通过注册中心Eureka自动加载配置 配置  gateway及后端微服务引入注册中心客户端eureka-client  \u0026lt;!-- 引入 Spring Cloud Netflix Eureka Client 相关依赖，将 Eureka 作为注册中心的客户端，并实现对其的自动配置 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  eureka-client starter的引入，会同时引入ribbon,作为后续请求，负载均衡的实现  spring.cloud.gateway.discovery.locator.enabled设为true,启用服务发现的DiscoveryClientRouteDefinitionLocator\nspring:cloud:# Spring Cloud Gateway 配置项，对应 GatewayProperties 类gateway:discovery:locator:enabled:true# default fase，设为true开启@Configuration(proxyBeanMethods = false) @ConditionalOnProperty(value = \u0026#34;spring.cloud.discovery.reactive.enabled\u0026#34;,//fzb 默认使用响应式的方式 \tmatchIfMissing = true) public static class ReactiveDiscoveryClientRouteDefinitionLocatorConfiguration { @Bean//fzb spring.cloud.gateway.discovery.locator.enabled配为true时，才开启DiscoveryClientRouteDefinitionLocator \t@ConditionalOnProperty(name = \u0026#34;spring.cloud.gateway.discovery.locator.enabled\u0026#34;) public DiscoveryClientRouteDefinitionLocator discoveryClientRouteDefinitionLocator( ReactiveDiscoveryClient discoveryClient,//响应式的客服端 Eureka就是 EurekaReactiveDiscoveryClient \tDiscoveryLocatorProperties properties) { return new DiscoveryClientRouteDefinitionLocator(discoveryClient, properties); } }  eureka-client的引入，会开启TimedSupervisorTask执行HeartbeatThread的心跳任务， 默认每隔30s一次  RouteRefreshListener 每隔30s接收到HeartBeatEvent事件，同时会发送RefreshRoutes事件\nCachingRouteLocator 收到RefreshRoutesEvent事件，重新获取路由时会发现多了一个DiscoveryClientRouteDefinitionLocator （负责从注册中心获取新路由配置）\n验证 http://localhost:7070/ORDER/order/get 前边ORDER为eureka-client从eureka-server获取的服务名，\nLoadBalancerClientFilter在NettyRoutingFilter之前通过ribbon执行负载均衡策略，选择一个服务实例\n总结  springgateway 接收注册中心心跳事件，发送路由刷新事件， CachingRouteLocator 最终调用DiscoveryClientRouteDefinitionLocator#getRouteDefinitions 获取注册中心最新的路由  代码实现:https://github.com/fengzhenbing/spring-cloud-gateway-demo.git\n","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/06.route%E9%80%9A%E8%BF%87%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/","title":"06.route通过注册中心自动配置加载"},{"content":"precidate选择路由 ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/07.precidate%E7%9A%84%E5%AF%B9%E8%B7%AF%E7%94%B1%E8%BF%9B%E8%A1%8C%E9%80%89%E6%8B%A9/","title":"07.precidate的对路由进行选择"},{"content":"","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/08.filter%E7%9A%84%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD%E5%8F%8A%E5%90%88%E5%B9%B6/","title":"08.filter的配置加载及合并"},{"content":"soul介绍 官网\n 高性能，多协议，易扩展，响应式的API Gateway     丰富的协议 支持 dubbo ，tars， springcloud grpc。     插件化 插件化设计思想，插件热插拔，易扩展。   流控 灵活的流量筛选，能满足各种流量控制。   内置插件 内置丰富的插件支持，鉴权，限流，熔断，防火墙等。   高性能 流量配置动态化，性能极高，网关消耗在 1~2ms。   集群部署 支持集群部署，支持 A/B Test，蓝绿发布。    soul项目结构   soul-admin\nsoul网关管理端，配合soul-dashbord\n  Soul-bootstrap\n网关启动工程： 实际引入soul-spring-boot-starter-gateway(soul-web)\n  Soul-client\n为下游服务提供者提供各类服务接入网关soul的客户端\n Soul-client-common Soul-client-dubbo Soul-client-grpc Soul-client-http Soul-client-sofa Soul-client-tars    Soul-common\n  Soul-dashbord\n  Soul-dist\n  Soul-example\n  Soul-metrics\n  Soul-plugin\n  Soul-register-center\n  Soul-spi\n  Soul-spring-boot-starter\n  Soul-sync-data-center\n Soul-sync-data-api Soul-sync-data-http Soul-sync-data-nacos Soul-sync-data-websocket Soul-sync-data-zookeeper    Soul-web\n  ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/soul%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84/","title":"soul整体结构"},{"content":"下载 https://redis.io/download\n编译 wget https://download.redis.io/releases/redis-6.0.10.tar.gz tar xzf redis-6.0.10.tar.gz cd redis-6.0.10 sudo make 运行 #复制配置文件及命令 mkdir ./bin mkdir ./conf sudo cp ./src/mkreleasehdr.sh ./bin sudo cp ./src/redis-benchmark ./bin sudo cp ./src/redis-check-rdb ./bin sudo cp ./src/redis-cli ./bin sudo cp ./src/redis-server ./bin sudo cp ./redis.conf ./conf #运行 ./bin/redis-server ./conf/redis.conf 配置 redis.conf\n#修改为守护模式 daemonize yes #设置进程锁文件 pidfile /usr/local/redis-4.0.11/redis.pid #端口 port 6379 #客户端超时时间 timeout 300 #日志级别 loglevel debug #日志文件位置 logfile /usr/local/redis-4.0.11/log-redis.log #设置数据库的数量，默认数据库为0，可以使用SELECT \u0026lt;dbid\u0026gt;命令在连接上指定数据库id databases 16 ##指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 #save \u0026lt;seconds\u0026gt; \u0026lt;changes\u0026gt; #Redis默认配置文件中提供了三个条件： save 900 1 save 300 10 save 60 10000 #指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间， #可以关闭该#选项，但会导致数据库文件变的巨大 rdbcompression yes #指定本地数据库文件名 dbfilename dump.rdb #指定本地数据库路径 dir /usr/local/redis-4.0.11/db/ #指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能 #会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有 #的数据会在一段时间内只存在于内存中 appendonly no #指定更新日志条件，共有3个可选值： #no：表示等操作系统进行数据缓存同步到磁盘（快） #always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全） #everysec：表示每秒同步一次（折衷，默认值） appendfsync everysec Docker #最新镜像 docker pull redis #运行 docker run -itd --name redis-test -p 6379:6379 redis docker image inspect redis:latest|grep -i version #运行 docker exec -it redis-test /bin/bash $ redis-cli \u0026gt; info   指定配置文件运行\ndocker run -p 6379:6379 --name redis-test -v /etc/redis/redis.conf:/etc/redis/redis.conf -v /etc/redis/data:/data -d redis redis-server /etc/redis/redis.conf --appendonly yes   停止\ndocker stop redis-test   ","date":"2020-12-10T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/%E5%AE%89%E8%A3%85redis/","title":"安装redis"},{"content":"通过Hugo搭建静态博客网站，再通过github pages部署运行\nHugo介绍  Hugo是一种用Go语言编写的快速，现代的静态网站生成器，旨在让网站创建再次变得有趣。 性能高，安全性和易用性是主要特点 拥有超快的速度，强大的内容管理和强大的模板语言，使其非常适合各种静态网站。  Hugo安装 # mac上安装 brew install hugo # windows可通过Chocolatey上安装 choco install hugo -confirm # 版本验证 hugo version hugo主题  查找你喜欢的主题 在此我选择的主题为toha 详情  初始化网站模板 # 首先在github下创建xxx.github.io的仓库 git clone https://github.com/fengzhenbing/fengzhenbing.github.io.git cd ./fengzhenbing.github.io # 初始化模板 hugo new site ./ -f=yaml --force #添加hugo-toha主题 git submodule add https://github.com/hugo-toha/toha.git themes/toha #本地运行 hugo server -t toha -w 修改配置 参考themes/toha/exampleSite，配置网站根目录下的config.yml文件，配置网站各个模块\nbaseURL:https://fengzhenbing.github.io/languageCode:en-usdefaultContentLanguage:cntitle:\u0026#34;Feng Zhenbing\u0026#39;s Blog\u0026#34;theme:\u0026#34;toha\u0026#34;# Manage languages# For any more details, you can check the official documentation: https://gohugo.io/content-management/multilingual/languages:cn:languageName:中文weight:1# en:# languageName: English# weight: 2# Control TOC depthmarkup:tableOfContents:startLevel:2endLevel:6ordered:false# Enable global emoji supportenableEmoji:true# Site parametersparams:# GitHub repo URL of your sitegitRepo:https://github.com/fengzhenbing/fengzhenbing.github.iogitBranch:master# specify whether you want to write some blog posts or notenableBlogPost:true# specify whether you want to show Table of Contents in reading pageenableTOC:true# Provide newsletter configuration. This feature hasn\u0026#39;t been implemented yet.# Currently, you can just hide it from the footer.newsletter:enable:true 至此浏览器中查看http://localhost:1313/ 可看到大致的博客网站, 更多细节查看https://toha-guides.netlify.app/posts/getting-started/prepare-site/  Github Pages 中部署  创建分支  #github 创建部署文件的分支 git branch -b gh-pages git push   github中setting中将部署分支设为gh-pages分支  img.png \n  开启github actions  img_1.png \n  编写github actions的部署文件\n  # 进入网站根目录，创建./github/workflows目录mkdir ./github/workflowscd ./github/workflowsvim deploy-site.yamldeploy-site.yaml\nname:Deploy to Github Pages# run when a commit or pr is pushed to \u0026#34;master\u0026#34; branchon:pull_request:push:branches:- masterjobs:deploy:runs-on:ubuntu-18.04steps:# checkout to the commit that has been pushed- uses:actions/checkout@v2with:submodules:true# Fetch Hugo themes (true OR recursive)fetch-depth:0# Fetch all history for .GitInfo and .Lastmod# install Hugo- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:\u0026#39;0.77.0\u0026#39;extended:true# build website- name:Buildrun:hugo --minify# push the generated content into the `main` (former `master`) branch.- name:Deployuses:peaceiris/actions-gh-pages@v3if:github.event_name == \u0026#39;push\u0026#39; \u0026amp;\u0026amp; github.ref == \u0026#39;refs/heads/master\u0026#39;with:github_token:${{ secrets.GITHUB_TOKEN }}publish_branch:gh-pages# if your main branch is `master` use that here.publish_dir:./public提交后，每次改动master分支，github action会运行以上任务，自动打包文件到gh-pages分支，并部署\n","date":"2020-12-08T08:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/hugo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/","title":"Hugo搭建博客"},{"content":"配置中心刷新原理 nacos和spring cloud config两种配置中心动态刷新的范围都是以下两种：\n @ConfigurationProperties 注解的配置类 @RefreshScope 注解的bean  手动刷新 post请求config客户端的/refresh端点\n自动刷新   WebHooks动态触发刷新\n  spring-cloud-bus动态刷新\n  这时Spring Cloud Bus做配置更新步骤如下:\n 提交代码触发post给Server端发送bus/refresh Server端接收到请求并发送给Spring Cloud Bus Spring Cloud bus接到消息并通知给其它客户端 其它客户端接收到通知，请求Server端获取最新配置 全部客户端均获取到最新的配置  这样的话我们在server端的代码做一些改动，来支持/actuator/bus-refresh\nSpring Cloud Bus Spring Cloud Bus 使用轻量级的消息代理来连接微服务架构中的各个服务，可以将其用于广播状态更改（例如配置中心配置更改）或其他管理指令\n目前 Spring Cloud Bus 支持两种消息代理：RabbitMQ 和 Kafka。\n参考\nhttps://blog.csdn.net/woshilijiuyi/article/details/88293782\nhttps://www.cnblogs.com/babycomeon/p/11141160.html\n","date":"2020-12-08T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%88%B7%E6%96%B0/","title":"配置中心刷新"},{"content":"小程序原理  使用两个线程：保证平台安全性，不能让开发者控制 render 线程，控制 render 线程将会造成小程序平台方管控困难 worker线程： 用户控制  响应 render 线程的事件，并执行小程序业务逻辑。 准备好数据，通过 setData 传到 page 中，由 page 进行渲染。   render线程：接收数据渲染到页面  TARO   官网\nhttps://taro-ui.jd.com/\n  特点\n  编译时转换\n  React vue \u0026hellip;\n  一套组件可以在 微信小程序，支付宝小程序，百度小程序，H5 多端适配运行\n    创建项目\n# 使用 npm 安装 CLI $ npm install -g @tarojs/cli # OR 使用 yarn 安装 CLI $ yarn global add @tarojs/cli # OR 安装了 cnpm，使用 cnpm 安装 CLI $ cnpm install -g @tarojs/cli   Remax   官网\n  特点\n  运行时转换:worker 线程维护一棵 vdom tree，然后同步到 render 线程通过 w|axml 来进行渲染。\n  react开发\n  多平台支持：支持阿里程序、微信小程序(QQ小程序)、头条小程序以及 Web 应用的开发。\n    创建项目\nnpx create-remax-app my-app cd my-app \u0026amp;\u0026amp; npm install npm run dev \u0026lt;platform\u0026gt; # 跨平台，如：要在阿里小程序环境运行，则 npm run dev ali  案例https://github.com/remaxjs/awesome-remax    WebPY   官网\n  特点\n 类似Vue开发 腾讯出品：小程序最早的框架之一    创建项目\n$ npm install @wepy/cli -g # 全局安装 WePY CLI 工具 $ wepy init standard myproj # 使用 standard 模板初始化项目 $ cd myproj # 进入到项目目录 $ npm install # 安装项目依赖包 $ npm run dev # 监听并且编译项目  案例https://github.com/aben1188/awesome-wepy    Kbone  特点  运行时转换:worker 线程维护一棵 vdom tree，然后同步到 render 线程通过 w|axml 来进行渲染。    uniapp 参考：小程序多平台同构方案分析-kbone 与 remax\n","date":"2020-11-23T00:00:00Z","permalink":"https://fengzhenbing.github.io/p/%E5%B0%8F%E7%A8%8B%E5%BA%8F%E6%A1%86%E6%9E%B6/","title":"小程序框架"},{"content":"kafka相关概念 二代mq, scala开发\n broker topic patition producer customer Customer group leader follower rebalance  服务端partition数量扩大 消费者组中某个消费者down掉    Topic特性  通过partition增加可扩展性：线上改partion数，rebalance ，会照成性能抖动。 partition有序达到高吞吐 partition多副本增加容错性  kafka单机   安装 http://kafka.apache.org/downloads\n  修改配置\ncd kafka_2.13-2.7.0 # 打开 listeners=PLAINTEXT://localhost:9092 vim config/server.properties # 启动zookeeper bin/zookeeper-server-start.sh config/zookeeper.properties # 启动kafaka bin/kafka-server-start.sh config/server.properties   命令测试\n# 创建topic mokernetdeMac-mini:kafka_2.13-2.7.0 mokernet$ bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic testf --partitions 4 --replication-factor 1 Created topic testf. # 查看 bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic testf # 消费者从头开始消费 bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic testf # 生产者生产 bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic testf # 生产者性能测试 100万条数据 每条1000byte 限流100万条 bin/kafka-producer-perf-test.sh --topic testf --num-records 1000000 --record-size 1000 --throughput 1000000 --producer-props bootstrap.servers=localhost:9092 # 消费者性能测试 消费100万条数据 一个线程 bin/kafka-consumer-perf-test.sh --bootstrap-server localhost:9092 --topic testf --fetch-size 1048576 --messages 1000000 --threads 1   kafka集群   修改各个节点的三个属性配置如下：\n# 复制新的配置文件 cp config/server.properties config/server-1.properties cp config/server.properties config/server-2.properties # 修改 id 端口 数据文件目录 # config/server-1.properties: broker.id=1 listeners=PLAINTEXT://:9093 log.dir=/tmp/kafka-logs-1 # 修改 id 端口 数据文件目录 # config/server-2.properties: broker.id=2 listeners=PLAINTEXT://:9094 log.dir=/tmp/kafka-logs-2   启动\nbin/kafka-server-start.sh config/server.properties \u0026amp; bin/kafka-server-start.sh config/server-1.properties \u0026amp; bin/kafka-server-start.sh config/server-2.properties \u0026amp;   测试\n# 创建topic test42 4个patition 2个副本 bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic test42 --partitions 4 --replication-factor 2 # 查看 mokernetdeMac-mini:kafka_2.13-2.7.0 mokernet$ bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic test42 Topic: test42 PartitionCount: 4 ReplicationFactor: 2 Configs: Topic: test42 Partition: 0 Leader: 1 Replicas: 1,2 Isr: 1,2 Topic: test42 Partition: 1 Leader: 2 Replicas: 2,0 Isr: 2,0 Topic: test42 Partition: 2 Leader: 0 Replicas: 0,1 Isr: 0,1 Topic: test42 Partition: 3 Leader: 1 Replicas: 1,0 Isr: 1,0 # 消费者从头开始消费 bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic test42 # 生产者生产 bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test42 # 容错性测试 ，关闭id=1的broker ps aux | grep server-1.properties \u0026gt; mokernet 30302 0.0 1.9 7323756 405020 s006 S+ 10:31PM 0:16.87 /usr/bin/java -Xmx kill -9 30302 #查看 如下，rebalance mokernetdeMac-mini:kafka_2.13-2.7.0 mokernet$ bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic test42 Topic: test42 PartitionCount: 4 ReplicationFactor: 2 Configs: Topic: test42 Partition: 0 Leader: 2 Replicas: 1,2 Isr: 2 Topic: test42 Partition: 1 Leader: 2 Replicas: 2,0 Isr: 2,0 Topic: test42 Partition: 2 Leader: 0 Replicas: 0,1 Isr: 0 Topic: test42 Partition: 3 Leader: 0 Replicas: 1,0 Isr: 0   Kafka connect  通过kafka导入导出数据  # 写入数据到输入文件 test.txt echo -e \u0026#34;testtest\u0026#34; \u0026gt; test.txt # 启动connect # 三个配置文件：  # 1. Kafka Connect的配置文件，包含常用的配置，如Kafka brokers连接方式和数据的序列化格式。 # 2. 源连接器配置，用于从输入文件读取行，并将其输入到 Kafka topic # 3. 接收器连接器配置，它从Kafka topic中读取消息，并在输出文件中生成一行。 bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties # 查看输出文件， 可以看到不断地往 test.txt写入数据时，test.sink.txt 会持续产生数据 tail -f test.sink.txt kafak stream        Kafka 特点  高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒； 可扩展性：kafka集群支持热扩展； 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止丢失； 容错性：允许集群中的节点失败(若分区副本数量为n,则允许n-1个节点失败)； 高并发：单机可支持数千个客户端同时读写；  使用场景   消息系统\n  日志聚合\n  度量监控\n  流式处理\n  跟踪网站浏览记录\n  相关资料 参考 https://kafka.apachecn.org/\n大白话 kafka 架构原理\n","date":"2020-10-20T14:06:25+06:00","permalink":"https://fengzhenbing.github.io/p/kafka%E5%9F%BA%E7%A1%80/","title":"kafka基础"},{"content":"消息队列作用  异步通信：异步通信，减少线程等待，特别是处理批量等大事务、耗时操作。 系统解耦:系统不直接调用，降低依赖，特别是不在线也能保持通信最终完成。 削峰填谷:压力大的时候，缓冲部分请求消息，类似于背压处理。 可靠通信:提供多种消息模式、服务质量、顺序保障等。  消息处理模式  点对点： PTP =\u0026gt; queue 发布订阅： PubSub =\u0026gt; Topic  消息语义 QOS  At most once At least once Exactly once  ","date":"2020-10-11T09:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/%E6%B6%88%E6%81%AF%E5%9F%BA%E7%A1%80/","title":"消息基础"},{"content":"两段式提交 （2 Phase Commit，2PC）\n准备阶段 重操作\n提交阶段 轻操作\n三段式提交 （3 Phase Commit，3PC）\n  在事务需要回滚的场景中：三段式的性能通常是要比两段式好很多的。\n  但在事务能够正常提交的场景中：两者的性能都依然很差，甚至三段式因为多了一次询问，还要稍微更差一些。\n  CanCommit 轻操作\nPreCommit 重操作\nCanCommit 轻操作\n","date":"2020-08-13T14:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/%E5%85%A8%E5%B1%80%E4%BA%8B%E7%89%A9/","title":"全局事物"},{"content":"1 介绍 本地事务（局部事务）在程序代码层面，最多只能对事务接口做一层标准化的包装（如 JDBC 接口），并不能深入参与到事务的运作过程当中，事务的开启、终止、提交、回滚、嵌套、设置隔离级别，乃至与应用代码贴近的事务传播方式，全部都要依赖底层数据源的支持才能工作。\n2 原子性（A）和持久性（D） 崩溃 （Crash）：数据库、操作系统一侧的崩溃，甚至是机器突然断电宕机等意外情况。\n崩溃恢复（Crash Recovery，也有资料称作 Failure Recovery 或 Transaction Recovery）:为了保证原子性和持久性，就只能在崩溃后采取恢复的补救措施\nCommit Logging 为了能够顺利地完成崩溃恢复，在磁盘中写入数据就不能像程序修改内存中变量值那样，直接改变某表某行某列的某个值，而是必须将修改数据这个操作所需的全部信息，包括修改什么数据、数据物理上位于哪个内存页和磁盘块中、从什么值改成什么值，等等，以日志的形式——即仅进行顺序追加的文件写入的形式（这是最高效的写入方式）先记录到磁盘中。只有在日志记录全部都安全落盘，数据库在日志中看到代表事务成功提交的“提交记录”（Commit Record）后，才会根据日志上的信息对真正的数据进行修改，修改完成后，再在日志中加入一条“结束记录”（End Record）表示事务已完成持久化.\n阿里的OceanBase 采用Commit Logging 机制来实现事务\n缺点：所有对数据的真实修改都必须发生在事务提交以后，即日志写入了 Commit Record 之后。在此之前，即使磁盘 I/O 有足够空闲、即使某个事务修改的数据量非常庞大，占用了大量的内存缓冲区，无论有何种理由，都决不允许在事务提交之前就修改磁盘上的数据。\nWrite-Ahead Logging 按照事务提交时点为界，划分为 FORCE 和 STEAL 两类情况。\n FORCE：当事务提交后，要求变动数据必须同时完成写入则称为 FORCE，如果不强制变动数据必须同时完成写入则称为 NO-FORCE。现实中绝大多数数据库采用的都是 NO-FORCE 策略，因为只要有了日志，变动数据随时可以持久化，从优化磁盘 I/O 性能考虑，没有必要强制数据写入立即进行。 STEAL：在事务提交前，允许变动数据提前写入则称为 STEAL，不允许则称为 NO-STEAL。从优化磁盘 I/O 性能考虑，允许数据提前写入，有利于利用空闲 I/O 资源，也有利于节省数据库缓存区的内存。  Write-Ahead Logging 在崩溃恢复时会执行以下三个阶段的操作：\n 分析阶段（Analysis）：该阶段从最后一次检查点（Checkpoint，可理解为在这个点之前所有应该持久化的变动都已安全落盘）开始扫描日志，找出所有没有 End Record 的事务，组成待恢复的事务集合，这个集合至少会包括 Transaction Table 和 Dirty Page Table 两个组成部分。 重做阶段（Redo）：该阶段依据分析阶段中产生的待恢复的事务集合来重演历史（Repeat History），具体操作为：找出所有包含 Commit Record 的日志，将这些日志修改的数据写入磁盘，写入完成后在日志中增加一条 End Record，然后移除出待恢复事务集合。 回滚阶段（Undo）：该阶段处理经过分析、重做阶段后剩余的恢复事务集合，此时剩下的都是需要回滚的事务，它们被称为 Loser，根据 Undo Log 中的信息，将已经提前写入磁盘的信息重新改写回去，以达到回滚这些 Loser 事务的目的。  Shadow Paging 副本方式\n对数据的变动会写到硬盘的数据中，但并不是直接就地修改原先的数据，而是先将数据复制一份副本，保留原数据，修改副本数据。\n事务完成修改数据引用指针 （修改指针保证原子性）\n3 隔离性 锁 写锁 （Write Lock，也叫作排他锁，eXclusive Lock，简写为 X-Lock）：如果数据有加写锁，就只有持有写锁的事务才能对数据进行写入操作，数据加持着写锁时，其他事务不能写入数据，也不能施加读锁。\n读锁 （Read Lock，也叫作共享锁，Shared Lock，简写为 S-Lock）：多个事务可以对同一个数据添加多个读锁，数据被加上读锁后就不能再被加上写锁，所以其他事务不能对该数据进行写入，但仍然可以读取。对于持有读锁的事务，如果该数据只有它自己一个事务加了读锁，允许直接将其升级为写锁，然后写入数据。\n范围锁 对于某个范围直接加排他锁，在这个范围内的数据不能被写入\nSELECT*FROMbooksWHEREprice\u0026lt;200FORUPDATE;以下四种隔离级别属于数据库理论的基础知识， 其实不同隔离级别以及幻读、不可重复读、脏读等问题都只是表面现象，是各种锁在不同加锁时间上组合应用所产生的结果，以锁为手段来实现隔离性才是数据库表现出不同隔离级别的根本原因。\n完全不加锁。。。 读未提交 缺点：出现赃读\nSELECT*FROMbooksWHEREid=1;/* 时间顺序：1，事务： T1 *//* 注意没有COMMIT */UPDATEbooksSETprice=90WHEREid=1;/* 时间顺序：2，事务： T2 *//* 这条SELECT模拟购书的操作的逻辑 */SELECT*FROMbooksWHEREid=1;/* 时间顺序：3，事务： T1 */ROLLBACK;/* 时间顺序：4，事务： T2 */实现方式：不加读锁 ，还是有写锁的，不会出现赃写\n读已提交 缺点：不可重复读，只能读到提交的数据。 缺乏贯穿整个事务周期的读锁，无法禁止读取过的数据发生变化，\nSELECT*FROMbooksWHEREid=1;/* 时间顺序：1，事务： T1 */UPDATEbooksSETprice=110WHEREid=1;COMMIT;/* 时间顺序：2，事务： T2 */SELECT*FROMbooksWHEREid=1;COMMIT;/* 时间顺序：3，事务： T1 */实现方式：对事务涉及的数据加的写锁会一直持续到事务结束，但加的读锁在查询操作完成后就马上会释放。\n可重复读 相对于读已提交，读锁存在时间为整个事物周期\n缺点：可能出现幻读\nSELECTcount(1)FROMbooksWHEREprice\u0026lt;100/* 时间顺序：1，事务： T1 */INSERTINTObooks(name,price)VALUES(\u0026#39;深入理解Java虚拟机\u0026#39;,90)/* 时间顺序：2，事务： T2 */SELECTcount(1)FROMbooksWHEREprice\u0026lt;100/* 时间顺序：3，事务： T1 */实现方式：对事务所涉及的数据加读锁和写锁，且一直持有至事务结束，但不再加范围锁。\n可串行化 缺点：顺序执行，吞吐量低，性能低\n实现方式： 对事务所有读、写的数据全都加上读锁、写锁和范围锁；实际还是很复杂的，要分成 Expanding 和 Shrinking 两阶段去处理读锁、写锁与数据间的关系，称为Two-Phase Lock，2PL\n4 多版本并发控制 参考 （https://icyfenix.cn/architect-perspective/general-architecture/transaction/local.html）\n","date":"2020-08-12T14:16:25+06:00","permalink":"https://fengzhenbing.github.io/p/%E6%9C%AC%E5%9C%B0%E4%BA%8B%E5%8A%A1/","title":"本地事务"}]